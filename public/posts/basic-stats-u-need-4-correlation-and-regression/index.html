<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.60.1" />

    
    
    

<title>Basic Stats U Need #4: Correlation &amp; Regression • Nathaniel Woodward</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Basic Stats U Need #4: Correlation &amp; Regression"/>
<meta name="twitter:description" content="Correlation demonstration! Regression Why normally distributed errors?  Normal-errors assumption and homoskedasticity Confidence (and prediction) intervals for a given X What’s the relationship between correlation and regression? Multiple regression: multiple predictors Multiple \(R\) and \(R^2\) \(R^2\) in one bang using correlation matricies  Semipartial and Partial Correlations Semipartial correlations: the residuals approach Increments to \(R^2\) when adding predictors Multicollinearity  Regression assumptions Partial correlations   .table { margin-left:auto; margin-right:auto; width: 60%; text-align:center; }  #Correlation People are often interested in how different variables are related to each other."/>

<meta property="og:title" content="Basic Stats U Need #4: Correlation &amp; Regression" />
<meta property="og:description" content="Correlation demonstration! Regression Why normally distributed errors?  Normal-errors assumption and homoskedasticity Confidence (and prediction) intervals for a given X What’s the relationship between correlation and regression? Multiple regression: multiple predictors Multiple \(R\) and \(R^2\) \(R^2\) in one bang using correlation matricies  Semipartial and Partial Correlations Semipartial correlations: the residuals approach Increments to \(R^2\) when adding predictors Multicollinearity  Regression assumptions Partial correlations   .table { margin-left:auto; margin-right:auto; width: 60%; text-align:center; }  #Correlation People are often interested in how different variables are related to each other." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/basic-stats-u-need-4-correlation-and-regression/" />
<meta property="article:published_time" content="2018-05-09T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-05-09T00:00:00+00:00" /><meta property="og:site_name" content=" " />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.43109490338b28cdd7a74845262efe59bf66fdc3530f5e818a66c9a579a79080.css" integrity="sha256-QxCUkDOLKM3Xp0hFJi7&#43;Wb9m/cNTD16BimbJpXmnkIA=">


<link rel="stylesheet" href="/scss/print.8c61428a4ae8b36c028e9198876312a2de5a2b3c80bbbdcceb98d738691c4f6b.css" integrity="sha256-jGFCikros2wCjpGYh2MSot5aKzyAu73M65jXOGkcT2s=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    <script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script type="text/javascript" src="/js/bigfoot.min.js"></script>
<script type="text/javascript">
    $.bigfoot();
</script>

</head>


    <body class="theme-base-09 ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
    
      
        
        
        
        
        <div class="author-image">
           <a href="/"><img src="/img/headshot_cropped_f2019.png" alt="Nathaniel Woodward" class="img--circle img--headshot element--center"></a>
        </div>
        
      
      <p class="site__description">
      <a href="mailto:nathanielraley@gmail.com"> Nathaniel Woodward 
      </a></p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Nathaniel Woodward</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Portfolio</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/teaching/">
						<span>Teaching</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/resume.html">
						<span>Vitae</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <br>
        <section class="social">
	
	<a href="https://twitter.com/Distributino" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/nathanielwoodward" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/nathanielraley" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://last.fm/user/ramimba" rel="me"><i class="fab fa-lastfm fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2022  
  
    <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  
</div>



  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Basic Stats U Need #4: Correlation &amp; Regression</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> May 9, 2018
    
    
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/linear-regression">linear regression</a>
           
      
          <a class="badge badge-tag" href="/tags/regression">regression</a>
           
      
          <a class="badge badge-tag" href="/tags/correlation">correlation</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 53 min read
</div>


  </header>
  
  
  <div class="post">
    

<div id="TOC">
<ul>
<li><a href="#correlation-demonstration">Correlation demonstration!</a></li>
<li><a href="#regression">Regression</a><ul>
<li><a href="#why-normally-distributed-errors">Why normally distributed errors?</a></li>
</ul></li>
<li><a href="#normal-errors-assumption-and-homoskedasticity">Normal-errors assumption and homoskedasticity</a><ul>
<li><a href="#confidence-and-prediction-intervals-for-a-given-x">Confidence (and prediction) intervals for a given X</a></li>
<li><a href="#whats-the-relationship-between-correlation-and-regression">What’s the relationship between correlation and regression?</a></li>
<li><a href="#multiple-regression-multiple-predictors">Multiple regression: multiple predictors</a></li>
<li><a href="#multiple-r-and-r2">Multiple <span class="math inline">\(R\)</span> and <span class="math inline">\(R^2\)</span></a></li>
<li><a href="#r2-in-one-bang-using-correlation-matricies"><span class="math inline">\(R^2\)</span> in one bang using correlation matricies</a></li>
</ul></li>
<li><a href="#semipartial-and-partial-correlations">Semipartial and Partial Correlations</a><ul>
<li><a href="#semipartial-correlations-the-residuals-approach">Semipartial correlations: the residuals approach</a></li>
<li><a href="#increments-to-r2-when-adding-predictors">Increments to <span class="math inline">\(R^2\)</span> when adding predictors</a></li>
<li><a href="#multicollinearity">Multicollinearity</a></li>
</ul></li>
<li><a href="#regression-assumptions">Regression assumptions</a></li>
<li><a href="#partial-correlations">Partial correlations</a></li>
</ul>
</div>

<style type="text/css">
.table {
    margin-left:auto; 
    margin-right:auto;
    width: 60%;
    text-align:center;
}
</style>
<p>#Correlation
People are often interested in how different variables are related to each other. In general, a correlation is just a relationship or association between two variables. If one variable gives you information about the other, then they are correlated (dependent, associated). More commonly, we talk about two variables having a linear relationship. To understand linear relationships, it helps to have a good grasp of variance and covariance.</p>
<p>Variance is a number that quantifies how spread out a variable’s values are from their mean on average. We subtract the mean <span class="math inline">\(\bar{x}\)</span> from each each observation <span class="math inline">\(x_i\)</span>, then we square each of these devations, and then average them up, to get the sample’s variance. (We divide by <span class="math inline">\(n-1\)</span> rather than <span class="math inline">\(n\)</span> because we are assuming we have a sample of data from a larger population: see <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>). Also, the standard deviation is just the square root of the variance: if the variance is the average squared deviation, the standard deviation represents the average deviation (though there are other, less common measures of this like <a href="https://en.wikipedia.org/wiki/Average_absolute_deviation">average absolute deviation</a>).</p>
<p><span class="math display">\[
var(X)=\sum_i \frac{(x_i-\bar{x})^2}{n-1}\\
sd_x=\sqrt{var(X)}= \sqrt{\sum_i \frac{(x_i-\bar{x})^2}{n-1}}
\]</span></p>
<p>Covariance is a measure of association between two variables: how much the larger values of one variable tend to “go with” larger values of the other variable (and how much smaller values go with with smaller values). When one value of one variable is above its mean, does the corresponding value for the other variable tend to be above its mean too? If so, you have positive covariance. In contrast, if one variable tends to have values above its mean when the other tends to have values <em>below</em> its mean, you have negative covariance. If there is no relationship between the deviations of each of the two variables from their respective means, the covariance will be zero.</p>
<p>For both variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, we calculate the deviation of each observation from it’s mean, <span class="math inline">\(x_{i}-\bar{x}\)</span> and <span class="math inline">\(y_{i}-\bar{y}\)</span>. Then we multiply those deviations together, <span class="math inline">\((x_{i}-\bar{x})(y_{i}-\bar{y})\)</span>, and we average these products across all observations.</p>
<p><span class="math display">\[
cov(X,Y)=\sum_i \frac{(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1}
\]</span></p>
<p>The numerator is where the magic happens. If, for a point on your scatterplot (<span class="math inline">\(x_i, y_i\)</span>), your <span class="math inline">\(x_i\)</span> is less than its mean (so <span class="math inline">\(x_{i}-\bar{x}\)</span> is negative), but your <span class="math inline">\(y_i\)</span> is greater than its mean (so <span class="math inline">\(y_{i}-\bar{y}\)</span> is positive), then the product of the two deviations in the summation will be negative.</p>
<p>For example, let’s say we observe these two data points:</p>
<p><span class="math display">\[\begin{matrix}
x&amp;y\\ \hline
1 &amp;4\\
3&amp; 6 \\ \hline
\bar x=2 &amp;\bar y=5
\end{matrix}
\]</span></p>
<p>Then we have</p>
<p><span class="math display">\[
\frac{\sum(x_i-\bar x)(y_i-\bar y)}{n-1}=\frac{(1-2)(4-5)+(3-2)(6-5)}{2-1}=\frac{(-1)(-1)+(1)(1)}{1}=2
\]</span></p>
<p>These variables have a sample covariance of 2.</p>
<p>While the sign of the covariance will always tell you whether a relationship between to variables is positive or negative (i.e, relationship <em>direction</em>), the magnitude of the covariance can be misleading as a measure of relationship <em>strength</em> because it is influenced by the magnitude of each of the variables.</p>
<p>Let’s work with this a bit. Below we plot X and Y, which have a perfectly linear relationship; also, the slope is one (when X goes up by 12.5, Y goes up by 12.5). Along the margin, we show the distribution of each variable with tick marks. Focusing on one variable at a time, notice they are both uniformly distributed. We have also drawn through the mean of each variable (<span class="math inline">\(M_{X}=50; M_{Y}=200\)</span>). Notice also that both X and Y have the same number of data points (9), and that they are both evenly spaced throughout. Thus, they have the same standard deviation (<span class="math inline">\(SD=34.23\)</span>). Now the sample covariance of X and Y is 1171.88, which is positive.</p>
<pre class="r"><code>library(ggplot2)
library(dplyr)
#library(tidyr)
library(MASS)
#samples = 10
#r = 0.5
#data = as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE))

data&lt;-data.frame(X=seq(0,100,length.out = 9),Y=seq(150,250,length.out = 9))
knitr::kable(round(data,3),align=&#39;cc&#39;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">X</th>
<th align="center">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.0</td>
<td align="center">150.0</td>
</tr>
<tr class="even">
<td align="center">12.5</td>
<td align="center">162.5</td>
</tr>
<tr class="odd">
<td align="center">25.0</td>
<td align="center">175.0</td>
</tr>
<tr class="even">
<td align="center">37.5</td>
<td align="center">187.5</td>
</tr>
<tr class="odd">
<td align="center">50.0</td>
<td align="center">200.0</td>
</tr>
<tr class="even">
<td align="center">62.5</td>
<td align="center">212.5</td>
</tr>
<tr class="odd">
<td align="center">75.0</td>
<td align="center">225.0</td>
</tr>
<tr class="even">
<td align="center">87.5</td>
<td align="center">237.5</td>
</tr>
<tr class="odd">
<td align="center">100.0</td>
<td align="center">250.0</td>
</tr>
</tbody>
</table>
<pre class="r"><code>ggplot(data,aes(X,Y))+geom_point()+geom_hline(yintercept=mean(data$Y))+geom_vline(xintercept=mean(data$X))+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-1-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>cov(data$X,data$Y)</code></pre>
<pre><code>## [1] 1171.875</code></pre>
<p>But the number 1171.88 itself is rather arbitrary. It is influenced by the scale of each variable. For example, if we multiply both variables by 100, we get:</p>
<pre class="r"><code>round(cov(10*data$X,10*data$Y))</code></pre>
<pre><code>## [1] 117188</code></pre>
<p>We have arbitrarily increased the covariance by a factor of 100.</p>
<p>Here’s an interesting thought: this perfectly linear relationship should represent the maximum strength of association. What would we have to scale by if we wanted this pefect (positive) covariance to equal one?</p>
<p>Well, if we standardize both variables (by subtracting the variable’s mean from each observation and dividing that difference by the variable’s standard deviation), we preserve the relationship among observations but we put them on the same scale (i.e., each variable has a mean of 0 and a standard deviation of 1). That is, we convert each variable to a z-score (if you need a refresher, see <a href="http://www.nathanielwoodward.com/2017/08/08/basic-stats-u-need-z-scores/">my previous post about z-scores</a>).</p>
<p>Plotting them yields an equivalent looking plot, but notice how the scales have changed! The relationship between X and Y is completely preserved. Also, the covariance of X and Y is now 1.</p>
<pre class="r"><code>data&lt;-data.frame(scale(data))
ggplot(data,aes(X,Y))+geom_point()+geom_hline(yintercept=mean(data$Y))+geom_vline(xintercept=mean(data$X))+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-3-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>This is interesting! The covariance of two standardized variables that have a perfectly linear relationship is equal to 1. This is a desirable property: The linear relationship between X and Y simply cannot get any stronger!</p>
<p>Equivalently, since the calcualtion of the covariance (above) already subtracts the mean from each variable, instead of standardizing <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we could just divide the original <span class="math inline">\(cov(X,Y)\)</span> by the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Either way is fine!</p>
<p><span class="math display">\[
r=\sum_i\frac{Z_{X}Z_{Y}}{n-1}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{sd_Xsd_Y(n-1)}=\frac{cov(X,Y)}{\sqrt{var(X)}\sqrt{var(Y)}}=\frac{1171.88}{34.23 \times 34.23}=1
\]</span></p>
<p>We have hit upon the formula for Pearson’s correlation coefficient (<span class="math inline">\(r\)</span>) for measuring the strength of a linear association between two variables. Taking the covariance of two standardized variables will always yield <span class="math inline">\(r\)</span>. In this case, <span class="math inline">\(r=1\)</span> indicating a perfect linear direct relationship between the variables. This is the maximum value of <span class="math inline">\(r\)</span> (the minimum value is -1, indicating a perfectly linear inverse relationship). Also, <span class="math inline">\(r=0\)</span> means no relationship (i.e., random noise). Observe the scatterplots, and their respective correlations, below:</p>
<pre class="r"><code>library(cowplot)

samples = 10; r = -1
d1 = as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE));r=-.5
d2= as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE));r=0
d3= as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE));r=.5
d4= as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE));r=1
d5= as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE))

names(d1)&lt;-names(d2)&lt;-names(d3)&lt;-names(d4)&lt;-names(d5)&lt;-c(&quot;X&quot;,&quot;Y&quot;)

p1&lt;-ggplot(d1,aes(X,Y))+geom_point()+theme(axis.text.x=element_blank(),axis.text.y=element_blank(),
      axis.ticks=element_blank(),axis.title.x=element_blank(),axis.title.y=element_blank())+ggtitle(&quot;r = -1&quot;)
p2&lt;-ggplot(d2,aes(X,Y))+geom_point()+theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks=element_blank(),axis.title.x=element_blank(),axis.title.y=element_blank())+ggtitle(&quot;r = -0.5&quot;)
p3&lt;-ggplot(d3,aes(X,Y))+geom_point()+theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks=element_blank(),axis.title.x=element_blank(),
      axis.title.y=element_blank())+ggtitle(&quot;r = 0&quot;)
p4&lt;-ggplot(d4,aes(X,Y))+geom_point()+theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks=element_blank(),axis.title.x=element_blank(),axis.title.y=element_blank())+ggtitle(&quot;r = 0.5&quot;)
p5&lt;-ggplot(d5,aes(X,Y))+geom_point()+theme(axis.text.x=element_blank(),axis.text.y=element_blank(),axis.ticks=element_blank(),axis.title.x=element_blank(),axis.title.y=element_blank())+ggtitle(&quot;r = 1&quot;)

plot_grid(p1,p2,p3,p4,p5,nrow=1)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-4-1.png" width="480" style="display: block; margin: auto;" /></p>
<pre class="r"><code>theme_set(theme_grey())</code></pre>
<div id="correlation-demonstration" class="section level2">
<h2>Correlation demonstration!</h2>
<p>Let’s approach the correlation <span class="math inline">\(r\)</span> again, but from a slightly different angle. I want to demonstrate a helpful way to visualize the calculation.</p>
<p>As before, divide the scatterplot into quadrants (whose axes intersect at the mean of each variable, in this case 0). Now, for each point (i.e., for each pair of variables) find how far it is away from the x-axis and the y-axis.</p>
<pre class="r"><code>data&lt;-data.frame(scale(data))
knitr::kable(round(data,3),align=&#39;cc&#39;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">X</th>
<th align="center">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-1.461</td>
<td align="center">-1.461</td>
</tr>
<tr class="even">
<td align="center">-1.095</td>
<td align="center">-1.095</td>
</tr>
<tr class="odd">
<td align="center">-0.730</td>
<td align="center">-0.730</td>
</tr>
<tr class="even">
<td align="center">-0.365</td>
<td align="center">-0.365</td>
</tr>
<tr class="odd">
<td align="center">0.000</td>
<td align="center">0.000</td>
</tr>
<tr class="even">
<td align="center">0.365</td>
<td align="center">0.365</td>
</tr>
<tr class="odd">
<td align="center">0.730</td>
<td align="center">0.730</td>
</tr>
<tr class="even">
<td align="center">1.095</td>
<td align="center">1.095</td>
</tr>
<tr class="odd">
<td align="center">1.461</td>
<td align="center">1.461</td>
</tr>
</tbody>
</table>
<pre class="r"><code>p&lt;-ggplot(data,aes(X,Y))+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-2.5,2.5))+scale_y_continuous(limits = c(-2.5,2.5))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)
p+geom_segment(aes(x=data[9,1],xend=data[9,1],y=0,yend=data[9,2]),lty=3)+geom_segment(aes(x=0,xend=data[9,1],y=data[9,2],yend=data[9,2]),lty=3)+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-5-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p+geom_segment(aes(x=data[9,1],xend=data[9,1],y=0,yend=data[9,2]),lty=3)+geom_segment(aes(x=0,xend=data[9,1],y=data[9,2],yend=data[9,2]),lty=3)+annotate(&quot;text&quot;, x = data[9,1], y=data[9,2], label = paste(&quot;(&quot;,round(data[9,1],2),&quot;,&quot;,round(data[9,2],2),&quot;)&quot;,sep=&quot;&quot;),hjust=0,vjust=0,size=3)+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-6-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Now let’s go ahead and do this for every point:</p>
<pre class="r"><code>p+geom_text(aes(label=paste(&quot;(&quot;,round(data$X,2),&quot;,&quot;,round(data$Y,2),&quot;)&quot;,sep=&quot;&quot;),hjust=0, vjust=0),size=3)+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-7-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Since the mean of Zx is <span class="math inline">\(\bar{x}=0\)</span> and the mean of Zy is <span class="math inline">\(\bar{y}=0\)</span>, for each point <span class="math inline">\((x_i,y_i)\)</span> we just found the deviations of each value from the mean <span class="math inline">\((x_i-\bar{x},y_i-\bar{y})\)</span>!</p>
<p>Now then, for each pair of values, multiply the two numbers together. Notice that this is the deviation of <span class="math inline">\(x_i\)</span> from its mean times the deviation of <span class="math inline">\(y_i\)</span> from its mean for all of the <span class="math inline">\(i\)</span> pairs in our sample.</p>
<pre class="r"><code>p+geom_text(aes(label=paste(&quot;(&quot;,round(data$X*data$Y,2),&quot;)&quot;,sep=&quot;&quot;),hjust=0, vjust=0),size=3)+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-8-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Let’s add them up in each quadrant and put the total in the middle:</p>
<pre class="r"><code>p1&lt;-ggplot(data,aes(X,Y))+geom_abline(slope = 1,intercept=0,color=&#39;gray50&#39;,lty=2)+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data[data$X&gt;0 &amp; data$Y&gt;0,]$X*data[data$X&gt;0 &amp; data$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data[data$X&lt;0 &amp; data$Y&lt;0,]$X*data[data$X&lt;0 &amp; data$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data[data$X&lt;0 &amp; data$Y&gt;0,]$X*data[data$X&lt;0 &amp; data$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data[data$X&gt;0 &amp; data$Y&lt;0,]$X*data[data$X&gt;0 &amp; data$Y&lt;0,]$Y),2))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+geom_rug(color=&quot;gray50&quot;)
p1</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-9-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Now let’s use these totals to calculate the correlation coefficient: add up the total in each quadrant and divide by <span class="math inline">\(n-1\)</span>.</p>
<p><span class="math display">\[
\frac{\sum ZxZy}{n-1}=\frac{4+4+0+0}{8}=1
\]</span></p>
<p>Perfect correlation (<span class="math inline">\(r=1\)</span>)! Also, notice that the slope of the (dashed) line that the points lie on is <em>also</em> exactly 1…</p>
<p>What if the relationship were flipped?</p>
<pre class="r"><code>data1&lt;-data
data1$X&lt;-data$X*-1
ggplot(data1,aes(X,Y))+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$Y),2))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-10-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>The sum of the deviations are the same, but the sign has changed. Thus, the correlation is the same strength, but different direction (r=-1) We have a perfect negative correlation: <span class="math inline">\(r=\sum\frac{ZxZy}{n-1}=\frac{-8}{8}=-1\)</span>. Notice that the mean and standard deviation of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are still the same (we just rotated it <span class="math inline">\(90^o\)</span>)</p>
<p>Now let’s see what would happen if data points were no longer perfectly linear. Let’s take the second data point from the bottom and scoot it up, making the y-value equal 0.</p>
<pre class="r"><code>data1&lt;-data
data1[2,2]&lt;-0
ggplot(data1,aes(X,Y))+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;y&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$Y),2))+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-11-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data1,aes(X,Y))+geom_abline(slope = .92,intercept=0,color=&#39;gray50&#39;,lty=3)+
  geom_abline(slope = 1,intercept=0,color=&#39;gray50&#39;,lty=2)+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$Y),2))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-12-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Uh oh, but since we changed a y-value, the mean of the <span class="math inline">\(y\)</span>s no longer equals 1, and the standard deviation no longer equals 0. They are now <span class="math inline">\(M_y=.12\)</span> and <span class="math inline">\(SD_y=.91\)</span>; however, <span class="math inline">\(X\)</span> still remains standardized because nothing changed (see distribution along the bottom).</p>
<p>If we ignore this fact and try to calculate the “correlation” we get: <span class="math inline">\(r_{fake}=\sum\frac{Zx Y}{n-1}=\frac{6.8}{8}=.85\)</span>. However, if we go to the data and calculate the actual correlation, we get this:</p>
<pre class="r"><code>knitr::kable(round(data1,3),align=&#39;cc&#39;)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">X</th>
<th align="center">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">-1.461</td>
<td align="center">-1.461</td>
</tr>
<tr class="even">
<td align="center">-1.095</td>
<td align="center">0.000</td>
</tr>
<tr class="odd">
<td align="center">-0.730</td>
<td align="center">-0.730</td>
</tr>
<tr class="even">
<td align="center">-0.365</td>
<td align="center">-0.365</td>
</tr>
<tr class="odd">
<td align="center">0.000</td>
<td align="center">0.000</td>
</tr>
<tr class="even">
<td align="center">0.365</td>
<td align="center">0.365</td>
</tr>
<tr class="odd">
<td align="center">0.730</td>
<td align="center">0.730</td>
</tr>
<tr class="even">
<td align="center">1.095</td>
<td align="center">1.095</td>
</tr>
<tr class="odd">
<td align="center">1.461</td>
<td align="center">1.461</td>
</tr>
</tbody>
</table>
<pre class="r"><code>print(paste(&quot;Correlation of X and Y: &quot;,with(data1,cor(X,Y))))</code></pre>
<pre><code>## [1] &quot;Correlation of X and Y:  0.931128347758783&quot;</code></pre>
<p>So the actual <span class="math inline">\(r=.93\)</span>. Moving one data point up reduced our actual correlation from 1 to .93. But what is the relationship between the “fake” correlation we got by not standardizing Y (<span class="math inline">\(r_{fake}=.85\)</span>), and the actual correlation (.93)?</p>
<p><span class="math display">\[ \begin{aligned}
r_{fake}&amp;=r\times x\\
.85&amp;=.93x\\
x&amp;=\frac{.93}{.85}=.91=sd_Y
\end{aligned}
\]</span></p>
<p>Our “fake” correlation is just the covariance of X and Y! Remember, <span class="math inline">\(r=\frac{cov(X,Y)}{sd_Xsd_Y}\)</span></p>
<p>Let’s standardize Y again and proceed:</p>
<pre class="r"><code>data1$Y&lt;-scale(data1$Y)
ggplot(data1,aes(X,Y))+geom_abline(slope = .9311,intercept=0,color=&#39;gray50&#39;,lty=3)+
  geom_abline(slope = 1,intercept=0,color=&#39;gray50&#39;,lty=2)+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$Y),2))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-14-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Again, you can see that the best-fitting line (dotted) has a slope somewhat less steep than before (compare to dashed line). Indeed, the slope is now <span class="math inline">\(r\frac{\sum Z_xZ_y}{n-1}=\frac{3.55+3.89}{8}=\frac{7.44}{8}=0.93\)</span>. Because the y-intercept is zero, the equation for the dotted line is <span class="math inline">\(Zy=.93\times Zx\)</span>. This will always be the case: Wen we have standardized variables, the slope of the line of best fit is equal to the correlation coefficient <span class="math inline">\(r\)</span>.</p>
<p>OK, so what happens if our data are not linear? (We will go ahead and standardize our variables to avoid issues this time.)</p>
<pre class="r"><code>data1&lt;-data
data1$Y&lt;--(data$Y^2)+1
data1&lt;-data.frame(scale(data1))
ggplot(data1,aes(X,Y))+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$Y),2))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-15-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#mean(data1$Y)
#sd(data1$Y)</code></pre>
<p>We have <span class="math inline">\(r=\sum\frac{ZxZy}{n-1}=\frac{.65+(-.65)+2.61+(-2.61)}{8}=\frac{0}{8}=0\)</span>, zero correlation! This is because every point in a positive quadrant is cancelled out by a point in a negative quadrant!</p>
<p>However, there is still clearly a relationship between <span class="math inline">\(Zy\)</span> and <span class="math inline">\(Zx\)</span>! (Specifically, <span class="math inline">\(Zy=-Zx^2+1\)</span>). So calculating the correlation coefficient is only appropriate when you are interested in a linear relationship. As we will see, the same applies to linear regression.</p>
<p>Finally, let’s see what more realistic data might look like:</p>
<pre class="r"><code>library(ggplot2)
library(MASS)

samples = 9
r = 0.5

data1 = as.data.frame(mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE))
names(data1)&lt;-c(&quot;X&quot;,&quot;Y&quot;)

ggplot(data1,aes(X,Y))+geom_point()+geom_hline(yintercept=0)+geom_vline(xintercept=0)+scale_x_continuous(limits=c(-3,3))+scale_y_continuous(limits = c(-3,3))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+annotate(&quot;text&quot;,2,2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,-2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&lt;0,]$Y),2))+
  annotate(&quot;text&quot;,-2,2,label=round(sum(data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$X*data1[data1$X&lt;0 &amp; data1$Y&gt;0,]$Y),2))+
  annotate(&quot;text&quot;,2,-2,label=round(sum(data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$X*data1[data1$X&gt;0 &amp; data1$Y&lt;0,]$Y),2))+xlab(&quot;Zx&quot;)+ylab(&quot;Zy&quot;)+geom_rug(color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-16-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Here, we find that <span class="math inline">\(r=\sum \frac{ZxZy}{n-1}=\frac{(-0.18)+3.1+(-1.01)+2.09}{8}=\frac{4}{8}=0.5\)</span></p>
<p>A positive correlation, but not nearly as strong as before.</p>
<p>To recap,</p>
<ul>
<li><p>r = strength of linear relationship between two variables</p></li>
<li><p>for variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, measures how close points <span class="math inline">\((x,y)\)</span> are to a straight line</p></li>
<li><p>linear transformations (add/subtract a constant, multiply/divide by a constant) do not affect the value of r.</p></li>
</ul>
<p>Now then, in the above case where the line fit perfectly (<span class="math inline">\(r=1\)</span>), we could write our prediction for y (denoted <span class="math inline">\(\hat{y}\)</span>) like so:</p>
<p><span class="math display">\[
Zy=Zx \longrightarrow \frac{\hat{y}-\bar{y}}{sd_y}=\frac{x-\bar{x}}{sd_x}
\]</span></p>
<p>In this case, it was a perfect prediction! But when it is not perfect prediction, we need to calculate a slope coefficient for Zx</p>
</div>
<div id="regression" class="section level1">
<h1>Regression</h1>
<p>Correlation and regression are fundamental to modern statistics.</p>
<p>Regression techniques are useful when you want to model an <strong>outcome variable</strong> <span class="math inline">\(y\)</span> as a function of (or in relationship to) <strong>predictor variables</strong> <span class="math inline">\(x_1, x_2,...\)</span>. Specifically, regression is useful when you want</p>
<p><span class="math display">\[
y=f(x_1,x_2,...)
\]</span></p>
<p>Because this goal is so common, and because the tool is so general, regression models are ubiquitous in research today.</p>
<p>We are going to be talking here about situations where we model <span class="math inline">\(y\)</span> as a linear function of <span class="math inline">\(x1, x2,...\)</span></p>
<p><span class="math display">\[
y=b_0+b_1x_1+b_2x_2+...+\epsilon
\]</span></p>
<p>We are interested in modeling the mean of our response variable <span class="math inline">\(y\)</span> given some predictors <span class="math inline">\(x_1, x_2,...\)</span>. Right now, think of regression as a function which gives us the expected value of <span class="math inline">\(y\)</span> (<span class="math inline">\(\hat y\)</span>) for a given value of <span class="math inline">\(x\)</span>. The value <span class="math inline">\(\hat y\)</span> is the predicted value, which will lie on the straight line that runs right through the middle of our scatterplot. Notice that some of the data will be above the predicted line and some will be below it: our <span class="math inline">\(\hat y\)</span>s will not perfectly match our <span class="math inline">\(y\)</span>. The distances between the predicted (or “fitted”) values <span class="math inline">\(\hat y\)</span> and the actual values <span class="math inline">\(y\)</span> are called residuals (<span class="math inline">\(\epsilon=y-\hat y\)</span>). The residuals end up being very important conceptually: they represent the variability in <span class="math inline">\(y\)</span> that is left after accounting for the best-fitting linear combination of your predictor(s) <span class="math inline">\(x\)</span> (i.e., <span class="math inline">\(\hat y\)</span>).</p>
<p>##Deriving regression coefficients!</p>
<p>Let’s keep it basic, because doing so will allow us to do something amazing using pretty basic math.</p>
<p>Let’s look at a special regression with one dependent variable <span class="math inline">\(y\)</span>, one predictor <span class="math inline">\(x\)</span>, and no intercept. Like we said above, the <span class="math inline">\(x\)</span>s and <span class="math inline">\(y\)</span>s we observed aren’t going to have a perfectly linear relationship: we need a residual term (<span class="math inline">\(\epsilon\)</span>) that capture deviations of our data from the predicted line <span class="math inline">\(\hat y=b x\)</span> (“error”). Here’s the model:</p>
<p><span class="math display">\[
\begin{aligned}
y&amp;=b x+\epsilon \\
\hat y&amp;=b x\\
&amp;\text{subtract bottom from top},\\
y-\hat y&amp;=\epsilon\\
\end{aligned}
\]</span></p>
<p>Now, when we estimate <span class="math inline">\(b\)</span>, our goal is to miminize the errors (the residuals <span class="math inline">\(\epsilon= y-\hat y\)</span>). Minimizing the errors/residuals will give us the best fitting line! But <span class="math inline">\(\epsilon_i\)</span> can be positive or negative, so we square them. Specifically, we want to choose <span class="math inline">\(b\)</span> to minimize the sum of these squared residuals. Let’s do it! From above, we know that <span class="math inline">\(\epsilon=y-b x\)</span>, so <span class="math inline">\(\sum_i\epsilon_i^2=\sum_i(y_i-b x_i)^2\)</span>. Let’s do a tiny bit of algebra (<span class="math inline">\(min_b\)</span> just means our goal is to find the value of <span class="math inline">\(b\)</span> that minimizes the function:</p>
<p><span class="math display">\[
\begin{aligned}
\min_b \sum_i\epsilon_i^2&amp;=\sum_i(y_i-\hat y_i)^2\\
\min_b \sum_i\epsilon_i^2&amp;=\sum_i(y_i-b x_i)^2\\
&amp;=\sum_i(y_i-b x_i)(y_i-b x_i)\\
&amp;=\sum_i y_i^2-2b x_iy_i+b^2x_i^2
\end{aligned}
\]</span></p>
<p>Recall that <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are fixed. We want to know the value of <span class="math inline">\(b\)</span> that will result in the smallest total squared error. We can find this value by taking the derivative of the error function (above) with respect to <span class="math inline">\(b\)</span>: this results in a new function that tells you the slope of a line tangent to the original function for each value of <span class="math inline">\(b\)</span>. Since the original function <span class="math inline">\(\sum_i(y_i-b x_i)^2\)</span> is a positive quadratic, we know that it has its minimum value at the bottom of the bend where the slope of the tangent line is zero. This is the minimum value of the error function! We just need to solve for <span class="math inline">\(b\)</span>.</p>
<p><span class="math display">\[
\begin{aligned} &amp;\text{dropping subscripts for readability}\\
\min_b \sum\epsilon^2 &amp;=\sum (y^2-2b xy+b^2x^2)\\
&amp;\text{distribute sums, pull out constants, apply derivative operator}\\
\frac{d}{db} \sum\epsilon^2 &amp;= \frac{d}{db} \sum y^2-2b\sum xy +b^2\sum x^2\\
&amp;=0-2\sum xy+2b\sum x^2 \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text{set derivative equal to zero, solve for }b\\
-2\sum xy+2b\sum x^2&amp;=0 \\
2b\sum x^2&amp;=2\sum xy \\
b&amp;=\frac{\sum xy}{\sum x^2} \\
\end{aligned}
\]</span></p>
<p>So the “least squares estimator” <span class="math inline">\(\hat{b}\)</span> is <span class="math inline">\(\frac{\sum xy}{\sum x^2}\)</span>.</p>
<p>This means that our best prediction of <span class="math inline">\(\hat y\)</span> given <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\hat y &amp;= \hat b x \\
&amp;= \left(\frac{\sum_i x_iy_i}{\sum_i x_i^2}\right)x\\
&amp;= \sum_i y_i\frac{ x_i}{\sum_i x_i^2}x\\
\end{aligned}
\]</span></p>
<p>So our prediction for <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> is a weighted average of all the observed <span class="math inline">\(y_i\)</span>s.</p>
<p>This can be hard to see, so let’s say we observe the following data: <span class="math inline">\(y=\{-1,0,1\}, x=\{-1,1,0\}\)</span>
Then given that <span class="math inline">\(x=1\)</span>, our estimate <span class="math inline">\(\hat y\)</span> is
<span class="math display">\[
\sum_i y_i\frac{ x_i}{\sum_i x_i^2}x=-1\frac{-1}{2}1+0\frac{1}{2}1+1\frac{0}{2}1=.5+0+0=.5
\]</span></p>
<pre class="r"><code>ggplot(data.frame(x=c(-1,1,0),y=c(-1,0,1)),aes(x,y))+geom_point()+geom_smooth(method=lm, se=F)+annotate(geom=&quot;point&quot;,x=1,y=.5,color=&quot;red&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-17-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>Look again at <span class="math inline">\(\hat b =\frac{\sum xy}{\sum x^2}\)</span> for a moment. The top part of the estimator looks kind of like covariance, and the bottom looks kind of like a variance. Indeed,</p>
<p><span class="math display">\[\frac{cov(X,Y)}{var(X)}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})/n-1}{\sum(x_i-\bar{x})^2/n-1}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\]</span></p>
<p>The only difference is that <span class="math inline">\(\frac{cov(X,Y)}{var(X)}\)</span> scales the variables first, by subtracting the mean from each! Remember that in our simplified equation <span class="math inline">\(y_i=b x_i+\epsilon_i\)</span>, we are implicitly assuming that the y-intercept is zero (as it is when both variables are standardized). When it is, you are fine to use <span class="math inline">\(\hat b=\frac{\sum x_iy_i}{\sum x_i^2}\)</span> However, when you do have a y-intercept you want to find (which is usually the case), then the best estimator of the true slope <span class="math inline">\(b_1\)</span> is <span class="math inline">\(\hat b_1=\frac{cov(X,Y)}{var(X)}=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\)</span>. Let me show you the difference with some play data.</p>
<pre class="r"><code>x1&lt;-1:20
y1&lt;-rnorm(20)+.5*x1+5
data&lt;-data.frame(x1,y1)
b_noint=sum(x1*y1)/sum(x1^2)
b_int=cov(x1,y1)/var(x1)

ggplot(data,aes(x1,y1))+geom_point()+scale_x_continuous(limits=c(0,20))+scale_y_continuous(limits=c(0,15))+geom_abline(slope=b_noint,intercept=0)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-18-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>y2&lt;-scale(y1)
x2&lt;-scale(x1)
ggplot(data,aes(x2,y2))+geom_point()+geom_abline(slope=b_noint,intercept=0)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-18-2.png" width="288" style="display: block; margin: auto;" /></p>
<p>I wanted to show a derivation in the simplest case, but it works just as well with more variables. Here is a run-through of the same, but with vectors and matricies. let <span class="math inline">\(y=Xb+e\)</span> where <span class="math inline">\(y\)</span> is a vector, <span class="math inline">\(X\)</span> is a matrix of all predictors, <span class="math inline">\(b\)</span> is a vector of parameters to be estimated, and <span class="math inline">\(e\)</span> is a vector of errors.</p>
<p><span class="math display">\[
\begin{bmatrix}{}
y_1\\
y_2\\
\vdots \\
\end{bmatrix}=
\begin{bmatrix}{}
x_{1,1} &amp; x_{1,2}\\
x_{2,1} &amp; x_{2,2}\\
\vdots &amp;\vdots \\
\end{bmatrix}
\begin{bmatrix}
b_1\\
b_2\\
\vdots 
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{e&#39;e} &amp;=(\mathbf{y-Xb})&#39;(\mathbf{y-Xb})\\
\mathbf{e&#39;e} &amp;=\mathbf{y&#39;y-2b&#39;X&#39;y+b&#39;X&#39;Xb}\\
\frac{d}{db} \mathbf{e&#39;e}&amp;=-2\mathbf{Xy}+2\mathbf{b&#39;X&#39;X}\\
&amp;\text{set equal to zero and solve for b}\\
0 &amp;=-2\mathbf{X&#39;y}+2\mathbf{b&#39;X&#39;X}\\
2\mathbf{X&#39;y}&amp;=2\mathbf{b&#39;X&#39;X}\\
\mathbf{b}&amp;=\mathbf{(X&#39;X)^{-1}X&#39;y}
\end{aligned}
\]</span></p>
<p>Even if you are relatively unfamiliar with matrix algebra, you can see the resemblance that <span class="math inline">\(\mathbf{(X&#39;X)^{-1}X&#39;y}\)</span> bears to <span class="math inline">\(\frac{1}{\sum x^2}\sum xy\)</span>.</p>
<p>###Testing <span class="math inline">\((X&#39;X)^{-1}X&#39;Y\)</span> on some data</p>
<p>Let’s say our data are <span class="math inline">\(Y=10,13,14\)</span> and <span class="math inline">\(X=2,5,12\)</span>. Let’s derive the regression coefficient estimates <span class="math inline">\(\mathbf{\hat b} = b_0, b_1\)</span> using the estimator <span class="math inline">\(\mathbf{X&#39;X^{-1}X&#39;y}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{y} \phantom{xx}&amp;= \phantom{xxxxxx}\mathbf{Xb} \phantom{xxx} + \phantom{xx} \mathbf{e}\\
\begin{bmatrix}{}
10\\
13\\
14 \\
\end{bmatrix}&amp;=
\begin{bmatrix}{}
1 &amp; 2\\
1 &amp; 5\\
1 &amp; 12 \\
\end{bmatrix}
\begin{bmatrix}
b_0\\
b_1\\
\end{bmatrix}+
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\epsilon_3\\
\end{bmatrix}
\end{aligned}
\]</span></p>
<p><span class="math display">\[ 
\text{Now }\hat b = (X&#39;X)^{-1}X&#39;Y \text{, so...} \\
\]</span>
Click button to expand the calculation: <button onclick="myFunction()">Show/Hide</button></p>
<div id="myDIV" class="toshow" style="display:none">
<p><span class="math display">\[
X&#39;X=\begin{bmatrix}{}
1 &amp; 1 &amp;1\\
2 &amp; 5 &amp; 12\\
\end{bmatrix}\begin{bmatrix}{}
1 &amp; 2\\
1 &amp; 5\\
1 &amp; 12 \\
\end{bmatrix}=
\begin{bmatrix}{}
1(1)+1(1)+1(1)&amp;1(2)+1(5)+1(12)\\
2(1)+5(1)+5(12)&amp;2(2)+5(5)+12(12)
\end{bmatrix}=
\begin{bmatrix}
3&amp;19\\
19&amp;173\\
\end{bmatrix}\\
(X&#39;X)^{-1}=\begin{bmatrix}{}
3 &amp; 19 &amp;| &amp; 1 &amp; 0\\
19&amp;173&amp; | &amp; 0 &amp; 1
\end{bmatrix}
=\begin{bmatrix}{}
19 &amp; 19(\frac{19}{3}) &amp;| &amp; \frac{19}{3} &amp; 0\\
19&amp;173&amp; | &amp; 0 &amp; 1
\end{bmatrix}=
\begin{bmatrix}{}
19 &amp; 19(\frac{19}{3}) &amp;| &amp; \frac{19}{3} &amp; 0\\
0&amp;\frac{158}{3}&amp; | &amp; -\frac{19}{3} &amp; 1
\end{bmatrix}\\
=\begin{bmatrix}{}
1 &amp; \frac{19}{3} &amp;| &amp; \frac{1}{3} &amp; 0\\
0&amp;\frac{158}{3}&amp; | &amp; -\frac{19}{3} &amp; 1
\end{bmatrix}=
\begin{bmatrix}{}
1 &amp; 0 &amp;| &amp; \frac{1}{3}+\frac{19}{3}(\frac{19}{158}) &amp; -\frac{19}{158}\\
0&amp;\frac{158}{3}(\frac{19}{158})&amp; | &amp; -\frac{19}{3}(\frac{19}{158}) &amp; \frac{19}{158}
\end{bmatrix}\\=
\begin{bmatrix}{}
1 &amp; 0 &amp;| &amp; \frac{158}{474}+\frac{361}{474} &amp; -\frac{19}{158}\\
0&amp;1&amp; | &amp; -\frac{19}{158} &amp; \frac{3}{158}
\end{bmatrix}=
\begin{bmatrix}{}
\frac{173}{158} &amp; -\frac{19}{158}\\
-\frac{19}{158} &amp; \frac{3}{158}
\end{bmatrix}
\\
\]</span></p>
<p>We can double-check our gaussian elimiation using this formula for the inverse of a square matrix</p>
<p><span class="math display">\[
inv(\begin{bmatrix}
a &amp; b \\
c &amp; d \\
\end{bmatrix})=
\begin{bmatrix}
\frac{d}{ad-bc}&amp;-\frac{b}{ad-bc}\\
-\frac{c}{ad-bc}&amp;\frac{a}{ad-bc}
\end{bmatrix}\\
\begin{bmatrix}
3 &amp;19\\
19&amp;173\\
\end{bmatrix}=
\begin{bmatrix}
\frac{173}{3(173)-19(19)}&amp;-\frac{19}{3(173)-19(19)}\\
-\frac{19}{3(173)-19(19)}&amp;\frac{3}{3(173)-19(19)}
\end{bmatrix}=
\begin{bmatrix}
\frac{173}{158}&amp;-\frac{19}{158}\\
-\frac{19}{158}&amp;\frac{3}{158}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
X&#39;Y=\begin{bmatrix}
1 &amp; 1 &amp;1\\
2 &amp; 5 &amp; 12\\
\end{bmatrix}
\begin{bmatrix}
10\\
13\\
14\\
\end{bmatrix}=
\begin{bmatrix}
1(10)+1(13)+1(14)\\
2(10)+5(13)+12(14)\\
\end{bmatrix}=
\begin{bmatrix}
37\\
253\\
\end{bmatrix}
\\
\hat b =(X&#39;X)^{-1}X&#39;Y=\begin{bmatrix}{}
\frac{173}{158} &amp; -\frac{19}{158}\\
-\frac{19}{158} &amp; \frac{3}{158}
\end{bmatrix}
\begin{bmatrix}
37\\
253\\
\end{bmatrix}=
\begin{bmatrix}
\frac{6401}{158}-\frac{4807}{158}\\
-\frac{703}{158}+\frac{759}{158}
\end{bmatrix}=
\begin{bmatrix}
10.0886\\
0.3544\\
\end{bmatrix}
\]</span></p>
</div>
<p>So we found that the intercept estimate is 10.09 and the slope estimate is 0.35.
Let’s check this result by conducting a regression on the same data:</p>
<pre class="r"><code>lm(c(10,13,14)~c(2,5,12))</code></pre>
<pre><code>## 
## Call:
## lm(formula = c(10, 13, 14) ~ c(2, 5, 12))
## 
## Coefficients:
## (Intercept)  c(2, 5, 12)  
##     10.0886       0.3544</code></pre>
<pre class="r"><code>dat&lt;-data.frame(x=c(2,5,12),y=c(10,13,14))

lm_eqn &lt;- function(df){
    m &lt;- lm(y ~ x, df);
    eq &lt;- substitute(hat(italic(y)) == a + b %.% italic(x), 
         list(a = format(coef(m)[1], digits = 2), 
              b = format(coef(m)[2], digits = 2)))
    as.character(as.expression(eq));                 
}

ggplot(dat,aes(x,y))+geom_point()+geom_smooth(method=lm,se=F)+geom_text(x = 5, y = 14, label = lm_eqn(dat), parse = TRUE)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-19-1.png" width="288" style="display: block; margin: auto;" /></p>
<div id="why-normally-distributed-errors" class="section level2">
<h2>Why normally distributed errors?</h2>
<p>Notice that we have derived regression coefficient estimates without making any assumptions about how the errors are distributed. But wait, you may say, if you already know something about regression: one assumption of regression (which is required for statistical inference) is that the errors are independent and normally distributed! That is,</p>
<p><span class="math display">\[
y_i=b_0 +b_1 x_i+\epsilon_i,  \phantom{xxxxx} \epsilon_i\sim N(0,\sigma^2)
\]</span></p>
<p>What does this say? Imagine there is a <em>true</em> linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the real world that actually takes the form <span class="math inline">\(y=b_0+b_1x\)</span>. When we estimate <span class="math inline">\(\hat b_0\)</span> and <span class="math inline">\(\hat b_1\)</span>, we are estimating these <em>true</em> population coefficients, but we are doing so impercisely based on observed data. The data we collect is always noisy and imperfect, and in statistics, we handle these imperfections by modeling them as random processes. Thus, for now we assume that the deviations of <span class="math inline">\(y\)</span> from our predicted <span class="math inline">\(\hat y\)</span> are random and follow a normal distribution. Equivalently, we can say that for a given <span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span> is normally distributed, with a mean of <span class="math inline">\(\hat y_i\)</span> and a standard deviation of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Let’s visualize this function and use it to create some data, add some noise to it, and run a regression to see if we can estimate the true parameters. But what are those parameters going to be? How about <span class="math inline">\(b_0=1\)</span> and <span class="math inline">\(b_1=2\)</span>, so we have <span class="math inline">\(y_i=1+2x_i+\epsilon_i\)</span>. Our predicted values of <span class="math inline">\(y\)</span> (<span class="math inline">\(\hat{y}\)</span>) are just <span class="math inline">\(\hat{y}=2x_i\)</span>.</p>
<pre class="r"><code>x&lt;-seq(-5,4.99,.01)
y&lt;-1+2*x
qplot(x,y)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-20-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#ggplot()+geom_abline(slope=2,intercept=1)+
#  scale_y_continuous(limits=c(-4,4))+scale_x_continuous(limits=c(-4,4))</code></pre>
<p>Now let’s give a little random error to each estimate by adding a draw from a standard normal distribution, <span class="math inline">\(\epsilon_i\sim N(\mu=0,\sigma^2=1)\)</span>.</p>
<pre class="r"><code>#x&lt;-seq(-4,4,.1)
y&lt;-y+rnorm(1000,mean=0,sd=1)
qplot(x,y)#+geom_abline(slope=2,intercept=1,color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-21-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#  scale_y_continuous(limits=c(-4,4))+scale_x_continuous(limits=c(-4,4))</code></pre>
<p>Since we set the standard deviation of the errors equal to one (i.e., <span class="math inline">\(\sigma=1\)</span>), ~95% of the observations will lie within <span class="math inline">\(2\sigma\)</span> of the predicted values <span class="math inline">\(\hat y\)</span>.</p>
<p>Now let’s estimate the best-fitting line, given only a sample of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (say, <span class="math inline">\(n=50\)</span> from the population). We will plot it (gray) along with the line we know to be true, namely <span class="math inline">\(y=1+2x\)</span> (white).</p>
<pre class="r"><code>dat&lt;-data.frame(x,y)
dat0&lt;-data.frame(x,y)
sum1&lt;-summary(lm(data=dat[sample(nrow(dat),size = 50,replace = F),],y~x))
sum1</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = dat[sample(nrow(dat), size = 50, replace = F), 
##     ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9826 -0.6407  0.1432  0.5365  2.1388 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.13011    0.12946   8.729 1.78e-11 ***
## x            1.96966    0.04761  41.368  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9149 on 48 degrees of freedom
## Multiple R-squared:  0.9727, Adjusted R-squared:  0.9721 
## F-statistic:  1711 on 1 and 48 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>beta&lt;-coef(sum1)[1:2,1]

qplot(x,y)+geom_abline(intercept=1,slope=2,color=&quot;white&quot;)+geom_abline(intercept=beta[1],slope=beta[2],color=&quot;gray50&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" />
We generated the data to have <span class="math inline">\(\epsilon\sim N(\mu=0,\sigma^2=1)\)</span>. Notice that the coefficient estimate for <span class="math inline">\(x\)</span>, (<span class="math inline">\(\hat b_1=\)</span> 1.97) is very close to the actual value (<span class="math inline">\(b_1=2\)</span>), and the coefficient estimate for the intercept (<span class="math inline">\(\hat b_0=\)</span> 1.13) is close to it’s actual value (<span class="math inline">\(b_0=1\)</span>). Notice also that the <em>residual standard error</em> is also close to 1. This is the standard deviation of the residuals (<span class="math inline">\(\epsilon=y-\hat y\)</span>) and thus it is an estimate of <span class="math inline">\(\sigma\)</span> above.</p>
<p>If we repeat this 1000 times (i.e., based on new random samples of 50), we get 1000 different estimates of the slope <span class="math inline">\(\hat b_1\)</span></p>
<pre class="r"><code>#betas&lt;-NULL
#sigma2&lt;-NULL
betas&lt;-vector()
sigma2&lt;-vector()
for(i in 1:1000){
sum1&lt;-summary(lm(data=dat[sample(nrow(dat),size = 50,replace = F),],y~x))
sum1
betas&lt;-rbind(betas,coef(sum1)[1:2,1])
sigma2&lt;-rbind(sigma2,sum1$sigma^2)
}

qplot(x,y)+geom_abline(intercept=betas[,1],slope=betas[,2],color=&quot;gray50&quot;)+geom_abline(intercept=1,slope=2,color=&quot;white&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>sample1&lt;-data.frame(x,y)[sample(1:1000,size=50),]
#ggplot(sample1,aes(x,y))+geom_point()+geom_smooth(method=&#39;lm&#39;,se = T,level=.9999,color=&#39;gray50&#39;)
#ggplot(sample1,aes(x,y))+geom_point()+geom_abline(intercept=betas[,1],slope=betas[,2],color=&quot;gray50&quot;)+geom_abline(intercept=1,slope=2,color=&quot;white&quot;)</code></pre>
<p>Each sample (i.e, your observed data) has error. Thus, each linear model also has error (in the intercept and in the slope). Notice that the prediced values for each sample (gray lines) tend to be better closer to the point (<span class="math inline">\(\bar x, \bar y\)</span>) and worse (farther from the white line) at the extremes.</p>
<p>When taking 1000 samples of 50 data points from our population, the estimates we get for the intercept and slope (i.e., <span class="math inline">\(\hat b_0\)</span> and <span class="math inline">\(\hat b_1\)</span>) range from and from respectively.</p>
<p>As we saw above, we can also estimate <span class="math inline">\(\sigma^2\)</span> from the sample variance of the residuals for each regression (<span class="math inline">\(\frac{\sum(y_i-\hat y)^2}{n-2}\)</span>). (We divide by <span class="math inline">\(n-2\)</span> here because <span class="math inline">\(\hat y\)</span> requires estimating both beta parameters, which uses up 2 degrees of freedom.) Let’s plot the distribution of these estimates:</p>
<pre class="r"><code>library(tidyr)
ests&lt;-betas
colnames(ests)&lt;-c(&quot;Est Intercept&quot;, &quot;Est Slope&quot;)
colnames(sigma2)&lt;-&quot;Est Var&quot;
ests&lt;-gather(data.frame(ests,sigma2))
ggplot(ests,aes(x=value))+geom_histogram(bins=30)+facet_grid(~key,scales=&quot;free_x&quot;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice how the mean of each distribution is equal to the population value (<span class="math inline">\(b_0=1, b_1=2, \sigma=1\)</span>).</p>
</div>
</div>
<div id="normal-errors-assumption-and-homoskedasticity" class="section level1">
<h1>Normal-errors assumption and homoskedasticity</h1>
<p>But let’s not get ahead of ourselves! Back to this for a moment:</p>
<p><span class="math display">\[
y_i=b_0 +b_1 x_i+\epsilon_i,  \phantom{xxxxx} \epsilon_i\sim N(0,\sigma^2)\\
\]</span></p>
<p>For a given <span class="math inline">\(y_i\)</span>, <span class="math inline">\(b_0\)</span>, <span class="math inline">\(b_1\)</span>, and <span class="math inline">\(x_i\)</span>, the quantity <span class="math inline">\(b_0 +b_1 x_i\)</span> gets added to the <span class="math inline">\(\epsilon_i\sim N(0,\sigma^2)\)</span>, thus shifting the mean from 0, but leaving the variance alone. Thus, we can write</p>
<p><span class="math display">\[
y_i \sim N(b_0 +b_1 x_i,\sigma^2)\\
\]</span></p>
<p>This means that we are modeling <span class="math inline">\(y\)</span> as a random variable that is normally distributed, with a mean that depends on <span class="math inline">\(x\)</span>. Thus, given <span class="math inline">\(x\)</span>, y has a distribution with a mean of <span class="math inline">\(b_0+b_1 x\)</span> and a standard deviation of <span class="math inline">\(\sigma\)</span>. Our predicted value <span class="math inline">\(\hat y\)</span> is the conditional mean of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x\)</span>.</p>
<p>We can see this below. For a given (range of) <span class="math inline">\(x\)</span>, the <span class="math inline">\(y\)</span> should be normally distributed, <span class="math inline">\(N(\mu=b_0+b_1x,\sigma^2=1)\)</span>. We plot the observed density (red) and the theoretical density (blue).</p>
<p>Plot of density of y given x; plot includes entire “population” of 1000:</p>
<pre class="r"><code>dat&lt;-data.frame(x,y)

breaks &lt;- seq(min(dat$x), max(dat$x), len=8)
dat$section &lt;- cut(dat$x, breaks)

## Get the residuals
dat$res &lt;- residuals(lm(y ~ x, data=dat))

## Compute densities for each section, and flip the axes, and add means of sections
## Note: the densities need to be scaled in relation to the section size (2000 here)
dens &lt;- do.call(rbind, lapply(split(dat, dat$section), function(x) {
    res &lt;- data.frame(y=mean(x$y)+seq(-3,3,length.out=50),x=min(x$x) +2*dnorm(seq(-3,3,length.out=50)))
    ## Get some data for normal lines as well
    xs &lt;- seq(min(x$res)-2, max(x$res)+2, len=50)
    res &lt;- rbind(res, data.frame(y=xs + mean(x$y),
                                 x=min(x$x) + 2*dnorm(xs, 0, sd(x$res))))
    res$type &lt;- rep(c(&quot;theoretical&quot;, &quot;empirical&quot;), each=50)
    res
}))
dens$section &lt;- rep(levels(dat$section), each=100)

## Plot both empirical and theoretical
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, fill=NA, lwd=2) +
  geom_path(data=dens, aes(x, y, group=interaction(section,type), color=type), lwd=1)+
  theme_bw() + theme(legend.position=&quot;none&quot;)+
  geom_vline(xintercept=breaks, lty=2)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Plot of density of y given x for a sample of 50 data points:</p>
<pre class="r"><code>dat&lt;-data.frame(x,y)
dat&lt;-dat[sample(size=50,nrow(dat),replace=F),]

breaks &lt;- seq(min(dat$x), max(dat$x), len=8)
dat$section &lt;- cut(dat$x, breaks)

## Get the residuals
dat$res &lt;- residuals(lm(y ~ x, data=dat))

## Compute densities for each section, and flip the axes, and add means of sections
## Note: the densities need to be scaled in relation to the section size (2000 here)
dens &lt;- do.call(rbind, lapply(split(dat, dat$section), function(x) {
    res &lt;- data.frame(y=mean(x$y)+seq(-3,3,length.out=50),x=min(x$x) +2*dnorm(seq(-3,3,length.out=50)))
    ## Get some data for normal lines as well
    xs &lt;- seq(min(x$res)-2, max(x$res)+2, len=50)
    res &lt;- rbind(res, data.frame(y=xs + mean(x$y),
                                 x=min(x$x) + 2*dnorm(xs, 0, sd(x$res))))
    res$type &lt;- rep(c(&quot;theoretical&quot;, &quot;empirical&quot;), each=50)
    res
}))
dens$section &lt;- rep(levels(dat$section), each=100)

## Plot both empirical and theoretical
ggplot(dat, aes(x, y)) +
  geom_point() +
  geom_smooth(method=&quot;lm&quot;, fill=NA, lwd=2) +
  geom_path(data=dens, aes(x, y, group=interaction(section,type), color=type), lwd=1)+
  theme_bw() + theme(legend.position=&quot;none&quot;)+
  geom_vline(xintercept=breaks, lty=2)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that in the sample, the empirical densities are not exactly normal with constant variance (we wouldn’t expect them to be; this is just a small sample). However, we can example a plot of residuals vs. fitted-values (or vs. <span class="math inline">\(x\)</span>) to see if the scatter looks random. We can also formally test for heteroskedasticity</p>
<pre class="r"><code>samp.lm&lt;-lm(data=dat,y~x)

dat$fvs&lt;-samp.lm$fitted.values
dat$res&lt;-samp.lm$residuals

ggplot(dat, aes(fvs,res))+geom_point()+geom_hline(yintercept=0)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-27-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(lmtest)
lmtest::bptest(samp.lm)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  samp.lm
## BP = 0.026574, df = 1, p-value = 0.8705</code></pre>
<p>In the Breusch-Pagan test, the null hypothesis is that variance is constant. Thus, if <span class="math inline">\(p&lt;.05\)</span> we have reason to suspect that the heteroskedasticity assumption might be violated. Also, if our residual plot shows any thickening/widening, or really any observable pattern, this is another way of diagnosing issues with nonconstant variance.</p>
<p>To see if our residuals are normally distributed, we can compare quantiles of their distribution to expanded quantiles under a normal distribution. If we plot them against each-other, their match would be indicated by all points falling on a straight line (with deviations from the line indicating non-normality). There are also hypothesis tests, such as the Shapiro-Wilk test, that assess the extent of deviation from normality. Here again, the null hypothesis is that the observed distribution matches a normal distribution.</p>
<pre class="r"><code>plot(samp.lm, which=c(2))</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-28-1.png" width="288" style="display: block; margin: auto;" /></p>
<pre class="r"><code>shapiro.test(dat$res)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  dat$res
## W = 0.9822, p-value = 0.6476</code></pre>
<p>This normal-errors assumption implies that at each <span class="math inline">\(x\)</span> the scatter of <span class="math inline">\(y\)</span> around the regression line is Normal. The equal-variances assumption implies that the variance of <span class="math inline">\(y\)</span> is constant for all <span class="math inline">\(x\)</span> (i.e., “homoskedasticity”). As we can see above (comparing empirical to theoretical distributions of <span class="math inline">\(y|x\)</span>), they will never hold <em>exactly</em> for any finite sample, but we can test them.</p>
<p>##Sampling distributions of coefficient estimates</p>
<p>The normal-errors assumption allows us to construct normal-theory confidence intervals and to conduct hypothesis tests for each parameter estimate.</p>
<p>Because <span class="math inline">\(\hat b_0\)</span> and <span class="math inline">\(\hat b_1\)</span> are statistics, they have a sampling distribution! In fact, we have already seen them (above), when we repeatedly estimated the coefficients based on samples of 50.</p>
<p>We can get empirical 95% confidence intervals by finding the quantiles that cut off the bottom 2.5% and the top 2.5% of each distribution:</p>
<pre class="r"><code>library(tidyr)
cutoff95&lt;-apply(data.frame(betas,sigma2),2,function(x)quantile(x,probs=c(.025,.975)))

colnames(cutoff95)&lt;-c(&quot;Est.Intercept&quot;,&quot;Est.Slope&quot;,&quot;Est.Var&quot;)
cutoff95</code></pre>
<pre><code>##       Est.Intercept Est.Slope   Est.Var
## 2.5%      0.7187877  1.889069 0.6385057
## 97.5%     1.2616610  2.086461 1.4419042</code></pre>
<pre class="r"><code>betas95&lt;-data.frame(est.int=betas[!(is.na(cut(betas[,1],breaks=cutoff95[,1]))),1],est.slope=betas[!(is.na(cut(betas[,2],breaks=cutoff95[,2]))),2],est.sigma2=sigma2[!(is.na(cut(sigma2,breaks=cutoff95[,3])))])

cutoff95&lt;-gather(data.frame(cutoff95))
ggplot(ests,aes(x=value))+geom_histogram(bins=30)+facet_grid(~key,scales=&quot;free_x&quot;)+geom_vline(data=cutoff95,aes(xintercept=value),color=&quot;red&quot;,lty=3)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>These are empirical confidence intervals (the middle 95% of the sampling distribution of each parameter).</p>
<p>But how can we get this from just a sample of the observed data?</p>
<p>We know that the (least squares) best estimate of <span class="math inline">\(b_1\)</span> is <span class="math inline">\(\hat b_1=\frac{\sum(x_i-\bar x_i)(y_i-\bar y_i)}{\sum(x_i-\bar x_i)^2}\)</span>.</p>
<p>Because the regression line always passes through the point (<span class="math inline">\(\bar x, \bar y\)</span>), we can plug those values in rearrange the equation to find that <span class="math inline">\(\hat b_0=\bar y-\hat b\bar x\)</span>.</p>
<p>But what about the variance of these estimates? Let’s find it!</p>
<p><span class="math display">\[
\begin{aligned}
Var[\hat b_1]&amp;=Var[\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum(x_i-\bar x)^2}]=Var[\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}y_i]\\ &amp;=Var[\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}(b_0+b_1x_i+\epsilon_i)]\\
\text{because only }\epsilon_i \text{ is a random variable, } \epsilon \sim N(0,\sigma^2),\\
&amp;=\frac{\sum(x_i-\bar x_i)^2}{(\sum(x_i-\bar x)^2)^2}Var[\epsilon_i]\\
&amp;=\frac{1}{\sum(x_i-\bar x)}\sigma^2
\end{aligned}
\]</span></p>
<p>We could do something similar to find the variance of <span class="math inline">\(\hat b_0\)</span>, but in practice the intercept is much less interesting than the slope.</p>
<p>Typically, we are interested in whether changing <span class="math inline">\(x\)</span> is associated with changes in <span class="math inline">\(y\)</span>, which is the same as testing the null hypothesis that <span class="math inline">\(b_1\)</span> is zero, or <span class="math inline">\(H_0:b_1=0\)</span></p>
<p>We also showed earlier that <span class="math inline">\(\hat b_1=\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}y_i\)</span>. Thus, <span class="math inline">\(\hat b_1\)</span> is a linear combination of the <span class="math inline">\(y_i\)</span>s.</p>
<p>But wait! We know that <span class="math inline">\(y_i\sim N(b_0+b_1x_i,\sigma^2)\)</span></p>
<p>We also know that a linear combination of normally distributed variables is itself normally distributed! (Indeed, a linear combination of any independent random variable converges to a Normal distribution with as <span class="math inline">\(n\rightarrow \infty\)</span>)</p>
<p>Since <span class="math inline">\(\hat b_1\)</span> is a linear combination of the <span class="math inline">\(y_i\)</span>s, <span class="math inline">\(\hat b_1\)</span> is normally distributed as well.</p>
<p>This means <span class="math inline">\(\hat b_1 \sim N(E[\hat b_1],Var[\hat b_1])\)</span>.</p>
<p>We already found the variance of the distribution of <span class="math inline">\(\hat b_1\)</span>, <span class="math inline">\(Var[\hat b_1]=\frac{\sigma^2}{\sum (x_i-\bar x)^2}\)</span></p>
<p>We have already seen that <span class="math inline">\(E[\hat b_1]= b_1\)</span>, but proving it isn’t too bad either:</p>
<p><span class="math display">\[
\begin{aligned}
E[\hat b_1]&amp;=E[\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}y_i]=E[\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}(b_0+b_1x_i+\epsilon_i)]\\
&amp;=\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}E[b_0+b_1x_i+\epsilon_i]\\
&amp;=\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}(b_0+b_1x_i)\\
&amp;=b_0\left(\frac{\sum(x_i-\bar x)}{\sum(x_i-\bar x)^2}\right)+b_1\left(\frac{\sum(x_i-\bar x)x_i}{\sum(x_i-\bar x)^2}\right)\\
&amp;=b_0(0)+b_1=(1)\\
&amp;=b_1
\end{aligned}
\]</span></p>
<p>Thus, we can now say that <span class="math inline">\(\hat b_1\sim N(b_1,\frac{\sigma^2}{\sum (x_i-\bar x)^2})\)</span></p>
<p>We don’t know <span class="math inline">\(\sigma^2\)</span> but we do have an unbiased estimator of it: <span class="math inline">\(s^2=\frac{\sum(y_i-\hat y)^2}{n-2}\)</span></p>
<p><a href="http://www.nathanielwoodward.com/2017/08/23/basic-stats-u-need-t-test/">In a previous post</a>, we showed that for sample of size <span class="math inline">\(n\)</span> from <span class="math inline">\(x \sim N(\mu,\sigma^2)\)</span>, and a sample variance of <span class="math inline">\(s^2=\frac{\sum (x-\bar x)^2}{n-1}\)</span>, then
<span class="math display">\[
\frac{\bar{x}-\mu}{\sqrt{s^2/n}}\sim t_{(n-1)}
\]</span></p>
<p>Since <span class="math inline">\(\hat b_1\sim N(\mu=b_1,\sigma^2/\sum(x-\bar x)^2)\)</span>, we have</p>
<p><span class="math display">\[
\frac{\hat b_1 - b_1}{\sqrt{s^2/\sum(x-\bar x)^2}}\sim t_{(n-2)}
\]</span></p>
<p>We can check to see if this makes sense with our data above. Recall that we repeatedly sampled 50 data points <span class="math inline">\((x,y)\)</span>, ran a regression, and found <span class="math inline">\(\hat b_1\)</span>. We thus created a sampling distribution of those values for <span class="math inline">\(n=50\)</span></p>
<pre class="r"><code>b1&lt;-ests[ests$key==&quot;Est.Slope&quot;,]
ggplot(b1,aes(x=value))+geom_histogram(bins=30)+geom_text(aes(x=1.9, y=75),label=paste(&quot;mean=&quot;,round(value),2)))+  geom_text(aes(x=1.9, y=60),label=paste(&quot;sd=&quot;,round(sd(value),3)))
#mean(b1$value)
#sd(b1$value)</code></pre>
<pre><code>## Error: &lt;text&gt;:2:112: unexpected &#39;)&#39;
## 1: b1&lt;-ests[ests$key==&quot;Est.Slope&quot;,]
## 2: ggplot(b1,aes(x=value))+geom_histogram(bins=30)+geom_text(aes(x=1.9, y=75),label=paste(&quot;mean=&quot;,round(value),2)))
##                                                                                                                   ^</code></pre>
<p>We know the true <span class="math inline">\(b_1\)</span> is 2.0 and the variance is <span class="math inline">\(\sigma^2/\sum(x-\bar x)^2\)</span>. The sampling distribution (for samples of size 50) will converge to these values.</p>
<p>Notice that the estimate of the variance depends on the actual sample values. Interestingly, we get pretty close to the empirical value based on one sample of size 50:</p>
<pre class="r"><code>x_samp=sample(x,size=50)
sqrt(1/sum((x_samp-mean(x_samp))^2))</code></pre>
<pre><code>## [1] 0.0447268</code></pre>
<p>Pretend all we observe are 50 observations. We can construct normal theory confidence intervals and conduct t-tests of the null hypothesis that <span class="math inline">\(\hat b_1\)</span> is zero. First we estimate <span class="math inline">\(\hat b_1\)</span> and <span class="math inline">\(s^2/\sum(x-\bar x)^2\)</span>. This means we have to estimate <span class="math inline">\(s^2=\frac{\sum(y_i-\hat y_i)^2}{n-2}\)</span>. Finally, we will conduct a t-test (<span class="math inline">\(df=n-2\)</span>) and also construct a 95% CI for our estimate,
<span class="math display">\[
\hat b_1 \pm t^*_{(\alpha=.975,\ df=48)} \times \frac{s^2}{\sum(x-\bar x)^2}
\]</span></p>
<p>Here are our coefficient estimates:</p>
<pre class="r"><code>samp=dat[sample(nrow(dat),size = 50,replace = F),]

#coefficients
b1&lt;-with(samp, sum((x-mean(x))*(y-mean(y)))/sum((x-mean(x))^2))
b0&lt;-with(samp, mean(y)-b1*mean(x))

paste(&quot;b0:&quot;,round(b0,4))</code></pre>
<pre><code>## [1] &quot;b0: 0.8434&quot;</code></pre>
<pre class="r"><code>paste(&quot;b1:&quot;,round(b1,4))</code></pre>
<pre><code>## [1] &quot;b1: 2.0241&quot;</code></pre>
<p>Here is our estimated variance <span class="math inline">\(s^2\)</span>:</p>
<pre class="r"><code>#residuals
e&lt;-samp$y-(b0+b1*samp$x)

#MSE
s2&lt;-sum(e^2)/(50-2)

s2</code></pre>
<pre><code>## [1] 1.008651</code></pre>
<p>Here is our standard error (standard deviation of the sampling distribution for <span class="math inline">\(\hat b_1\)</span>):</p>
<pre class="r"><code>#standard deviation of the sampling distribution (standard error)
se_b1&lt;-with(samp, sqrt(s2/sum((x-mean(x))^2)))
se_b1</code></pre>
<pre><code>## [1] 0.04713645</code></pre>
<pre class="r"><code>#for the slope too: see below
se_b0&lt;-with(samp,sqrt(s2*((1/50)+(mean(x)^2/sum((x-mean(x))^2)))))</code></pre>
<p>Let’s combine these to form a 95% confidence interval. We will use a critical <span class="math inline">\(t^*_{(48)}=2.0106\)</span> value that cuts off 2.5% in either tail.</p>
<pre class="r"><code>#95% CI:
b1+c(qt(.025,48)*se_b1,qt(.975,48)*se_b1)</code></pre>
<pre><code>## [1] 1.929284 2.118833</code></pre>
<p>Thus, there’s a 95% chance that the true <span class="math inline">\(b_1\)</span> is captured by the interval [1.929, 2.119]</p>
<p>Equivalently, we could test the null hypothesis of no effect <span class="math inline">\(H_0: b_1=0\)</span> versus the alternative hypothesis <span class="math inline">\(H_A: b_1\ne 0\)</span>:</p>
<p>First we find the test statistic and the p-value (below):</p>
<pre class="r"><code># t test (H0: b1=0)
t=(b1-0)/se_b1
t</code></pre>
<pre><code>## [1] 42.9404</code></pre>
<pre class="r"><code># pvalue
2*(1-pt(t,df = 48))</code></pre>
<pre><code>## [1] 0</code></pre>
<p>So <span class="math inline">\(t=\)</span> 42.940405, <span class="math inline">\(p=\)</span> 0. Let’s confirm with R.</p>
<pre class="r"><code>#confirm with R
samp.lm&lt;-lm(y~x, data=samp)
summary(samp.lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = samp)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.75221 -0.54629  0.05986  0.60153  2.45200 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.84340    0.14343    5.88 3.82e-07 ***
## x            2.02406    0.04714   42.94  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.004 on 48 degrees of freedom
## Multiple R-squared:  0.9746, Adjusted R-squared:  0.9741 
## F-statistic:  1844 on 1 and 48 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In much the same fashion, you can also construct 95% confidence intervals and conduct hypotheses tests for the intercept <span class="math inline">\(b_0\)</span>.</p>
<p>However, the standard deviation of the sampling distribution of <span class="math inline">\(b_0\)</span> (i.e., the standard error) is different! We wont walk through the derivation, but it is very similar to that above for <span class="math inline">\(b_1\)</span>. You wind up with this for the variance:</p>
<p><span class="math display">\[
Var[b_0]=\sigma^2\left(\frac{1}{n}+\frac{\bar x^2}{\sum(x_i-\bar x)^2} \right)
\]</span>
Again, since can’t know <span class="math inline">\(\sigma^2\)</span>, we use the estimator <span class="math inline">\(s^2=\frac{\sum (y_i-\hat y)^2}{n-2}\)</span> as before. Thus, the standard error of the sampling distribution of <span class="math inline">\(b_0\)</span> is</p>
<p><span class="math display">\[
s.e.(b_0)=s\sqrt{\frac{1}{n}+\frac{\bar x^2}{\sum(x_i-\bar x)^2}}\\
or, equivalently \\
s.e.(b_0)=\frac{s}{\sqrt{n}}\sqrt{1+\frac{\bar x^2}{\sum(x_i-\bar x)^2/n}}
\]</span>
I included the second formulation of the standard error so that you can see that it is a product of the standard error of a variable’s mean (i.e., <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span>).</p>
<p>So a 95% confidence interval for <span class="math inline">\(b_0\)</span> looks like</p>
<p><span class="math display">\[
\hat b_0 \pm t^*_{(n-2)}*s.e.(b_0)
\]</span>
In our specific case, <span class="math inline">\(\hat b_0\)</span> is 0.8434 and our standard error is 0.1434349, and our 95% confidence interval for the intercept is 0.555003, 1.1317936</p>
<div id="confidence-and-prediction-intervals-for-a-given-x" class="section level2">
<h2>Confidence (and prediction) intervals for a given X</h2>
<p>This focus on individual coefficients is very useful, but it can’t tell us two things: how confident we are in the value of an outcome <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span>, and what value of <span class="math inline">\(y\)</span> we would predict if we observed another <span class="math inline">\(x\)</span>.</p>
<p>Here’s our sample of size 50 with the regression line overlayed.</p>
<pre class="r"><code>ggplot(samp,aes(x,y))+geom_point()+geom_smooth(method=lm,color=&#39;gray50&#39;)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The gray band around the regression line is the 95% confidence interval for <span class="math inline">\(\hat y\)</span> for a given value of <span class="math inline">\(x\)</span>. You can think of it as a range that captures the true regression 95% of the time! It is a band because the interval changes depending on which <span class="math inline">\(x\)</span> you consider. It is narrowest at the point (<span class="math inline">\(\bar x, \bar y\)</span>) and widest at the edges of the data. This confidence interval, for a given value of the predictor <span class="math inline">\(x_k\)</span>, is defined as</p>
<p><span class="math display">\[
\hat y_k\pm t^*_{(n-2)} \sqrt{s^2\left(\frac{1}{n}+\frac{(x_k- \bar x)^2}{\sum(x_i-\bar x)^2} \right)},\\
where\ (as\ before),\\
s^2=\frac{\sum (y_i-\hat y)^2}{n-2}
\]</span></p>
<p>Again, I will not bore you with the derivation of the standard error (it is similar to how we did the slope above). Notice that the standard error is a function of the specific value of <span class="math inline">\(x_k\)</span> that is chosen (the numerator of the second term). Notice that this numerator <span class="math inline">\((x_k-\bar x)^2\)</span> will be zero when we choose <span class="math inline">\(x_k=\bar x\)</span>, and that the further away <span class="math inline">\(x_k\)</span> is from the the mean, the larger the numerator becomes. This fact accounts for the wider confidence interval at the ends of the range of <span class="math inline">\(x\)</span> and the narrowest interval at the mean of x.</p>
<p>Let’s calculate the 95% CI for three values of <span class="math inline">\(x_k: min(x),0, \text{ and } \bar x\)</span>. The mean of the <span class="math inline">\(x\)</span>s in our data was -0.4246 and the smallest <span class="math inline">\(x\)</span> in our data was -4.95</p>
<pre class="r"><code>preds&lt;-predict(samp.lm, newdata=data.frame(x=c(min(samp$x),0,mean(samp$x))),interval=&#39;confidence&#39;)

print(paste(&quot;95% CI for yhat when x=min(x): &quot;,&quot;[&quot;,round(preds[1,2],3),&quot;,&quot;,round(preds[1,3],3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;95% CI for yhat when x=min(x):  [ -9.691 , -8.66 ]&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;95% CI for yhat when x=0:      &quot;,&quot;[&quot;,round(preds[2,2],3),&quot;,&quot;,round(preds[2,3],3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;95% CI for yhat when x=0:       [ 0.555 , 1.132 ]&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;95% CI for yhat when x=mean(x):&quot;,&quot;[&quot;,round(preds[3,2],3),&quot;,&quot;,round(preds[3,3],3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;95% CI for yhat when x=mean(x): [ -0.302 , 0.27 ]&quot;</code></pre>
<p>We can see this better if we plot these (in red) along with the full 95% confidence bands (in gray) on the regression line and zoom in.</p>
<pre class="r"><code>ggplot(samp,aes(x,y))+geom_point()+geom_smooth(method=lm,size=.5,color=&quot;black&quot;,fill=&#39;gray50&#39;)+
#  scale_x_continuous(limits=c(min(x),mean(x)+1))+
#  scale_y_continuous(limits=c(-10,5))+
  geom_segment(aes(x=min(x),xend=min(x),y=preds[1,2],yend=preds[1,3]),color=&quot;red&quot;)+
  geom_segment(aes(x=0,xend=0,y=preds[2,2],yend=preds[2,3]),color=&quot;red&quot;)+
  geom_segment(aes(x=mean(x),xend=mean(x),y=preds[3,2],yend=preds[3,3]),color=&quot;red&quot;)+
   coord_cartesian(ylim=c(min(y),4),xlim=c(min(x),mean(x)+.5))</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-40-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that the 95% CI for <span class="math inline">\(\hat y\)</span> when <span class="math inline">\(x=0\)</span> is the exact same as the 95% CI for <span class="math inline">\(\hat b_0\)</span>! To see this, just plug <span class="math inline">\(x_k=0\)</span> into the formula for the standard error above and compare it with the intercept standard error! They will be identical!</p>
<div id="prediction-interval" class="section level3">
<h3>Prediction interval</h3>
<p>The confidence intervals we just calculated are for <span class="math inline">\(\hat y_k=E[y|x_k]\)</span>. But what if we want to predict what <span class="math inline">\(y_k\)</span> would actually be if we observed a new <span class="math inline">\(x_k\)</span>? These two intervals are not the same: there is a subtle difference! Recall that regression models <em>the distribution of</em> <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>. Thus, even if you knew the true values for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, you still don’t know exactly what <span class="math inline">\(y\)</span> itself would be because of the error <span class="math inline">\(\sigma^2\)</span>. Now, <span class="math inline">\(\hat y_k\)</span> is definitely your best guess (point estimate), but we are talking the confidence intervals! These are built from the standard error, and the standard error of <span class="math inline">\(y_k\)</span> given <span class="math inline">\(x_k\)</span> needs to account for the variability of the observations around the predicted mean <span class="math inline">\(\hat y\)</span> (recall: <span class="math inline">\(y_k\sim N(\hat y_k,\sigma^2)\)</span>). Since we don’t have <span class="math inline">\(\sigma^2\)</span>, we use <span class="math inline">\(s^2\)</span>. Literally, we are just adding this extra variability into the confidence interval (above) to get our prediction interval:</p>
<p><span class="math display">\[
\hat y_k\pm t^*_{(n-2)} \sqrt{s^2\left(1+\frac{1}{n}+\frac{(x_k- \bar x)^2}{\sum(x_i-\bar x)^2} \right)},\\
where\ (as\ before),\\
s^2=\frac{\sum (y_i-\hat y)^2}{n-2}
\]</span></p>
<p>Again, the only different between a 95% confidence interval and a 95% prediction interval is the standard error, which is larger for the prediction interval because of the variability of individual observations around the predicted mean (i.e., <span class="math inline">\(s^2\)</span>).</p>
<p>Here is the prediction interval for the same values of <span class="math inline">\(x_k\)</span> as above:</p>
<pre class="r"><code>preds&lt;-predict(samp.lm, newdata=data.frame(x=c(min(samp$x),0,mean(samp$x))),interval=&#39;prediction&#39;)

print(paste(&quot;95% CI for y (obs) when x=min(x): &quot;,&quot;[&quot;,round(preds[1,2],3),&quot;,&quot;,round(preds[1,3],3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;95% CI for y (obs) when x=min(x):  [ -11.26 , -7.092 ]&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;95% CI for y (obs) when x=0:      &quot;,&quot;[&quot;,round(preds[2,2],3),&quot;,&quot;,round(preds[2,3],3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;95% CI for y (obs) when x=0:       [ -1.196 , 2.883 ]&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;95% CI for y (obs) when x=mean(x):&quot;,&quot;[&quot;,round(preds[3,2],3),&quot;,&quot;,round(preds[3,3],3),&quot;]&quot;))</code></pre>
<pre><code>## [1] &quot;95% CI for y (obs) when x=mean(x): [ -2.055 , 2.023 ]&quot;</code></pre>
<p>Let’s plot both the prediction and the confidence interval for our data:</p>
<pre class="r"><code>pred.int&lt;-data.frame(predict(samp.lm,                       newdata=data.frame(x=samp$x),interval=&#39;prediction&#39;))

conf.int&lt;-data.frame(predict(samp.lm,                       newdata=data.frame(x=samp$x),interval=&#39;confidence&#39;))

ggplot(samp,aes(x,y))+geom_point()+geom_smooth(method=&quot;lm&quot;,size=.5,color=&quot;black&quot;)+
    geom_ribbon(inherit.aes=F,data=conf.int,aes(x=samp$x,ymin=lwr,ymax=upr),fill=&quot;red&quot;,alpha=.3)+
    geom_ribbon(inherit.aes=F,data=pred.int,aes(x=samp$x,ymin=lwr,ymax=upr),fill=&quot;blue&quot;,alpha=.3)</code></pre>
<p><img src="/posts/2018-05-09-basic-stats-u-need-4-correlation-and-regression_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice also that the 95% prediction interval captures all but approximately 2-3 observations, which makes sense because we have <span class="math inline">\(N=50\)</span>, since 5% of the time our 95% CI wont contain <span class="math inline">\(y\)</span>, <span class="math inline">\(.05*50=2.5\)</span>!</p>
</div>
</div>
<div id="whats-the-relationship-between-correlation-and-regression" class="section level2">
<h2>What’s the relationship between correlation and regression?</h2>
<p>Let’s take a step back from all this nitty-gritty and think about things from an ANOVA perspective. (If you’ve made it this far, you probably won’t need it, but if you want a quick review of ANOVA, check out <a href="http://www.nathanielwoodward.com/2017/08/27/basic-stats-u-need-3-anova/">this post</a>).</p>
<p>In ANOVA, the sum of the squared deviations of the <span class="math inline">\(y_i\)</span>s around the mean <span class="math inline">\(\bar y\)</span> is the total sum of squares (SST=<span class="math inline">\(\sum_i(y_i-\bar y)^2\)</span>). Now, these deviations around the grand mean can be separated into two parts: the variability <em>between</em> the <em>k</em> groups (SSB=<span class="math inline">\(\sum_k(\bar y_k-\bar y)^2\)</span>) and the variability <em>within</em> the <em>k</em> groups (SSW=<span class="math inline">\(\sum_k \sum_i (y_{ki}-\bar y_k)^2\)</span>). It turns out that <span class="math inline">\(SST=SSB+SSW\)</span></p>
<p>Analogously, in Regression, the SST=SSR+SSE. For now, instead of groups we have at least one continuous predictor <span class="math inline">\(x\)</span> (we will see soon that ANOVA is a special case of regression). Here, the total sum of squares is the same (SST=<span class="math inline">\(\sum_i(y_i-\bar y)^2\)</span>). The regression sum of squares is the part of variability in <span class="math inline">\(y\)</span> attributable to the regression, or the sum of the deviations of the predicted value from the mean of <span class="math inline">\(y\)</span> (SSR=<span class="math inline">\(\sum_i(\hat y-\bar y)^2\)</span>). What’s left over is the error: the deviations of the observed data from the predicted values that lie on the regression line (SSE=<span class="math inline">\(\sum_i(y_i-\hat y)^2\)</span>). Thus,</p>
<p><span class="math display">\[
\sum_i(y_i-\bar y)^2=\sum_i(\hat y_i-\bar y)^2 + \sum_i(y_i-\hat y_i)^2
\]</span></p>
<p>This says that the total variablity can be partitioned into a component explained by the regression (SSR) and an error component (SSE). A natural question to ask is, what proportion of the variability in <span class="math inline">\(y\)</span> can I account for with my regression <span class="math inline">\(\hat b_0+ \hat b_1x+...\)</span>? To find out, we can literally just look at the ratio <span class="math inline">\(\frac{SSR}{SST}=\frac{\sum_i(\hat y_i-\bar y)^2}{\sum_i(y_i-\bar y)^2}\)</span></p>
<p>Now then, if <span class="math inline">\(\hat y=bx\)</span>, and remembering our formula for variances, we can see that
<span class="math display">\[
\frac{\sum_i(\hat y_i-\bar y)^2}{\sum_i(y_i-\bar y)^2}=\frac{var(\hat y)}{var(y)}=\frac{var(\hat bx)}{var(y)}=\frac{\hat b^2var(x)}{var(y)}
\]</span></p>
<p>From above, we have the <span class="math inline">\(\hat b=\frac{cov(x,y)}{var(x)}\)</span>, so we get
<span class="math display">\[\frac{\hat b^2var(x)}{var(y)}=\frac{cov(x,y)^2var(x)}{var(x)^2var(y)}=\frac{cov(x,y)^2}{var(x)var(y)}=\left(\frac{cov(x,y)}{sd(x)sd(y)}\right)^2=r^2\]</span></p>
<p>So the proportion of variance explained by the regression line is equal to the square of the correlation coefficient <span class="math inline">\(r=cor(x,y)=cov(x,y)/sd(x)sd(y)\)</span>.</p>
<p>Now we are in a position to figure out the relationship between the slope of the regression line <span class="math inline">\(\hat b\)</span> and <span class="math inline">\(r\)</span>.</p>
<p><span class="math display">\[
\hat b = \frac{cov(x,y)}{var(x)}, \
r=\frac{cov(x,y)}{\sqrt{var(x)}\sqrt{var(y)}}
\]</span>
How do we transform <span class="math inline">\(r\)</span> into <span class="math inline">\(\hat b\)</span>? Multiply it by <span class="math inline">\(\sqrt{var(y)}\)</span> and divide by <span class="math inline">\(\sqrt{var(x)}\)</span>!
<span class="math display">\[
r\left(\frac{\sqrt{var(y)}}{\sqrt{var(x)}} \right)=r\left(\frac{sd_y}{sd_x} \right)=\frac{cov(x,y)\sqrt{var(y)}}{\sqrt{var(x)}\sqrt{var(y)}\sqrt{var(x)}}=\frac{cov(x,y)}{var(x)}=\hat b\\ \text{So,}\\
r(\frac{sd_y}{sd_x})=\hat b
\]</span></p>
<p>We have found that all we need to do is multiply the correlation coefficient between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> by the standard deviation of <span class="math inline">\(y\)</span> and divide it by the standard deviation of <span class="math inline">\(x\)</span> to get the regression coefficient (from the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>).</p>
<p>But if regression and correlation are both only able to describe an associative (i.e., non-causal) relationship, why is one different from the other?</p>
<p>Regression is primarily a predictive tool: it tells us what <span class="math inline">\(y\)</span> we can expect in a subset of the observations with given vaules for the predictor variables <span class="math inline">\(x\)</span> (that is, we make a probabilistic prediction by conditioning). To interpret a coefficient estimate causally, the predictor variables need to be assigned in a way that breaks dependencies between them and omitted variables (or noise), either using random assignment or control techniques. Most important from a modeling-flexibility standpoint, regression allows for multiple predictors!</p>
</div>
<div id="multiple-regression-multiple-predictors" class="section level2">
<h2>Multiple regression: multiple predictors</h2>
<p>So far our focus has been the simple linear case,
<span class="math display">\[
y_i=b_0+b_1x_i+\epsilon_i
\]</span></p>
<p>By way of transition to the general case, here’s another helpful way of looking at all of this: regression seeks to find the linear combination of the predictor variables <span class="math inline">\(x_1,x_2,...\)</span> (i.e., <span class="math inline">\(b_0+b_1x_1+b_2x_2,+...\)</span>) that is maximally correlated with <span class="math inline">\(y\)</span>.</p>
<p>Indeed, minimizing the sum of squared errors is the same thing as maximizing the correlation between the observed outcome (<span class="math inline">\(y\)</span>) and the predicted outcome (<span class="math inline">\(\hat y\)</span>).</p>
<p>This way of thinking allows us to generalize from one predictor of interest to multiple predictors.</p>
<p><span class="math display">\[
\hat y_i=b_0+b_1x_1+b_2x_2+...+b_kx_k \\
\hat z_{y}=\beta_1z_{x_1}+\beta_2z_{x_2}+...+\beta_kz_{x_k}
\]</span>
The second equation is the standardized form of the first: they result from estimating the coefficients with standardized variables <span class="math inline">\(z_y=\frac{y-\bar y}{sd_y}, z_{x_1}=\frac{x_1-\bar x_1}{sd_{x_1}}, z_{x_2}=\frac{x_2-\bar x_2}{sd_{x_2}}\)</span> etc. Because the variables are scale-free, the coefficients can be easier to interpret and compare. Sometimes you want the interpretation “for ever standard deviation increase in <span class="math inline">\(x_1\)</span>, <span class="math inline">\(y\)</span> goes up <span class="math inline">\(\beta_1\)</span> standard deviations.” Other times you want to keep <span class="math inline">\(y\)</span> and/or the <span class="math inline">\(x\)</span>s in terms of their original units.</p>
<p>It is easy to convert between <span class="math inline">\(b\)</span> and <span class="math inline">\(\beta\)</span>: you do not need to re-run the regression. As we will see below, <span class="math inline">\(b=\beta \frac{sd_y}{sd_x}\)</span>.</p>
<p>Also, notice that the fully standardized model has no intercept (i.e., <span class="math inline">\(\beta_0=0\)</span>). This will always be the case when all variables are standarized. Just like in simple linear regression, the line (or plane) of best fit passes through the point (y, x_1, …). When all variables are standardized, the mean of each is 0, and so the y-intercept (when <span class="math inline">\(z_y=0\)</span>) will be where <span class="math inline">\(\bar x_1=0\)</span> also!</p>
<p>We already saw the matrix equations for calculating our least squares regression weights: <span class="math inline">\(\hat b = (X&#39;X)^{-1}{X&#39;y}\)</span>. There is another, more direct route to calculate the standardized coefficients <span class="math inline">\(\hat \beta\)</span>. All you need are correlations. You need correlations between all pairs of predictors, as well as the correlation between each predictor and the outcome. Specifically,</p>
<p><span class="math display">\[
\begin{aligned}
\hat \beta&amp;=\mathbf{R}^{-1}\mathbf{r}_{y,x}\\
\begin{bmatrix}
\beta_1\\
\beta_2\\
\beta_3\\
\vdots
\end{bmatrix}&amp;=
\begin{bmatrix}
1 &amp; r_{x_1,x_2} &amp; r_{x_1,x_3} &amp; ...\\
r_{x_1,x_2}&amp; 1 &amp; r_{x_2,x_3} &amp; ...\\
r_{x_1,x_3}&amp;r_{x_2,x_3}&amp;1 &amp;... \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{bmatrix}^{-1}
\begin{bmatrix}
r_{y,x_1}\\
r_{y,x_2}\\
r_{y,x_3}\\
\vdots\\
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>In the above, <span class="math inline">\(\mathbf{R}\)</span> is a correlation matrix containing all pairwise correlations of the predictors. Notice the diagonal consists of 1s because the correlation of anything with itself equals 1.</p>
<p>Here’s some play data</p>
<p><span class="math display">\[
\begin{matrix}
y &amp; x_1 &amp; x_2 \\ \hline
12 &amp; 1 &amp; 6 \\
13 &amp; 5 &amp; 5 \\
14 &amp; 12 &amp; 7 \\ 
17 &amp; 14 &amp; 10 \\\hline
\bar y=14&amp;  \bar x_1=8 &amp; \bar x_2=7\\
\end{matrix}
\]</span>
Let’s get the deviations and sum their squares</p>
<p><span class="math display">\[
\begin{matrix}
(y-\bar y)^2 &amp; (x_1-\bar x_1)^2 &amp; (x_2-\bar x_2)^2 \\ \hline
(-2)^2 &amp; (-7)^2 &amp; (-1)^2 \\
(-1)^2 &amp; (-3)^2 &amp; (-2)^2 \\
0^2 &amp; 4^2 &amp; 0^2 \\
3^2 &amp; 6^2 &amp; 3^2\\\hline
SS_y=14 &amp; SS_{x1}=110 &amp; SS_{x2}=14
\end{matrix}
\]</span>
…and their products:</p>
<p><span class="math display">\[
\begin{matrix}
(y-\bar y)(x_1-\bar x_1) &amp; (y-\bar y)(x_2-\bar x_2) &amp; (x_1-\bar x_1)(x_2-\bar x_2) \\ \hline
(-2)(-7)&amp;(-2)(-1) &amp; (-7)(-1) \\
(-1)(-3)&amp;(-1)(-2) &amp; (-3)(-2) \\
0(4)  &amp;0(0)   &amp; 4(0) \\
3(6)  &amp;3(3)   &amp; 6(3)\\\hline
SP_{y,x1}=35 &amp; SP_{y,x2}=13 &amp; SP_{x1,x2}=31
\end{matrix}
\]</span></p>
<p>We can compute
<span class="math display">\[
\begin{aligned}
r_{x1,x2}&amp;=\frac{\sum(x_1-\bar x_1)(x_2-\bar x_2)}{n-1 sd_{x1}sd_{x2}} \\
&amp;=\frac{\sum(x_1-\bar x_1)(x_2-\bar x_2)}{\sqrt{\sum(x_1-\bar x_1)^2\sum(x_2-\bar x_2)^2}}\\ &amp;=\frac{SP_{x1,x2}}{\sqrt{SS_{x1}SS_{x2}}}\\ &amp;=\frac{31}{\sqrt{110*14}}=0.79
\end{aligned}
\]</span></p>
<p>We can do the same for the correlation of each predictor with the outcome:</p>
<p><span class="math display">\[
\frac{SP_{y,x1}}{\sqrt{SS_{x1}SS_{y}}} =\frac{35}{\sqrt{110*14}}=0.8919\\
\frac{SP_{y,x2}}{\sqrt{SS_{x2}SS_{y}}} =\frac{13}{\sqrt{14*14}}=0.9286
\]</span></p>
<p>Let’s confirm:</p>
<pre class="r"><code>y=matrix(c(12,13,14,17))
x=matrix(c(1,5,12,14,6,5,7,10),nrow=4)
print(&quot;Correlation matrix of predictors:&quot;)</code></pre>
<pre><code>## [1] &quot;Correlation matrix of predictors:&quot;</code></pre>
<pre class="r"><code>cor(x)</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 1.0000000 0.7899531
## [2,] 0.7899531 1.0000000</code></pre>
<pre class="r"><code>print(&quot;Correlation of y with x1:&quot;)</code></pre>
<pre><code>## [1] &quot;Correlation of y with x1:&quot;</code></pre>
<pre class="r"><code>cor(x[,1],y)</code></pre>
<pre><code>##           [,1]
## [1,] 0.8918826</code></pre>
<pre class="r"><code>print(&quot;Correlation of y with x2:&quot;)</code></pre>
<pre><code>## [1] &quot;Correlation of y with x2:&quot;</code></pre>
<pre class="r"><code>cor(x[,2],y)</code></pre>
<pre><code>##           [,1]
## [1,] 0.9285714</code></pre>
<p>Yep! Looks good! Now for the amazing part:</p>
<p><span class="math display">\[\begin{aligned}
\begin{bmatrix}
\beta_1 \\ \beta_2
\end{bmatrix}&amp;=
\begin{bmatrix}
1&amp; .79 \\
.79 &amp; 1 \\
\end{bmatrix}^{-1} 
\begin{bmatrix}
.8919 \\
.9286
\end{bmatrix}\\
&amp;=\frac{1}{1-.79^2}\begin{bmatrix}
1&amp; -.79 \\
-.79 &amp; 1 \\
\end{bmatrix}
\begin{bmatrix}
.8919 \\
.9286
\end{bmatrix}\\
&amp;=\begin{bmatrix}
2.6598&amp; -2.1011 \\
-2.1011 &amp; 2.6598 \\
\end{bmatrix}
\begin{bmatrix}
.8919 \\
.9286
\end{bmatrix}\\
&amp;= \begin{bmatrix}
0.4212\\
0.5959
\end{bmatrix}\\
\end{aligned}
\]</span></p>
<p>Let’s check our calculations with R</p>
<pre class="r"><code>solve(cor(x))%*%cor(x,y)</code></pre>
<pre><code>##           [,1]
## [1,] 0.4211851
## [2,] 0.5958549</code></pre>
<p>Good, it checks out! Now, are these the same as the standardized regression coefficients?</p>
<pre class="r"><code>round(coef(lm(scale(y)~scale(x)))[2:3],4)</code></pre>
<pre><code>## scale(x)1 scale(x)2 
##    0.4212    0.5959</code></pre>
<pre class="r"><code>#summary(lm(scale(y)~scale(x)))</code></pre>
<p>Yep! Neat!</p>
<p>Now that we’ve done the heavy lifting with the matrix equations, I want to show you a really handy (and conceptually important!) way to calculate the standardized regression coefficients <em>using only correlations among <span class="math inline">\(x_1, x_2,\)</span> and <span class="math inline">\(y\)</span></em></p>
<p>Recall that in the simple, one-predictor linear regression (with standardized variables) we had</p>
<p><span class="math display">\[
z_y=r_{y,x_1}z_{x_1}
\]</span></p>
<p>Now, in the two-predictor case, instead of <span class="math inline">\(r_{x,y}\)</span> we have</p>
<p><span class="math display">\[
z_y=\beta_1z_{x_1}+\beta_2z_{x_2}
\]</span></p>
<p>You might ask: How is <span class="math inline">\(r_{y,x_1}\)</span> related to <span class="math inline">\(\beta_1\)</span>? Well,</p>
<p><span class="math display">\[
\beta_1= \frac{r_{y,x_1}-r_{y,x_2}r_{x_1,x_2}}{1-r_{x_1,x_2}^2} \\
\beta_2= \frac{r_{y,x_2}-r_{y,x_1}r_{x_1,x_2}}{1-r_{x_1,x_2}^2} \\
\]</span>
Look at the formula for a minute.</p>
<p>Starting in the numerator, we have <span class="math inline">\(r_{y,x_1}\)</span>, the old slope in the single-predictor case. But we subtract off the other slope <span class="math inline">\(r_{y,x_2}\)</span> to the extent that <span class="math inline">\(x_2\)</span> is correlated with <span class="math inline">\(x_1\)</span>.</p>
<p>In the denominator, we see <span class="math inline">\(r^2_{x1,x2}\)</span>. This is the proportion of variance in <span class="math inline">\(x_1\)</span> shared with <span class="math inline">\(x_2\)</span>: their overlap! Thus, the denominator is adjusted to remove this overlap.</p>
<p>In summary, we can see that the old slopes have been adjusted to account for the correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<p>Let’s show that these actually work. Recall from above that <span class="math inline">\(r_{y,x_1}=.8919\)</span>, <span class="math inline">\(r_{y,x_2}=.9286\)</span>, and <span class="math inline">\(r_{x_1,x_2}=.79\)</span>. Plugging in, we get</p>
<p><span class="math display">\[
\beta_1= \frac{.8919-(.9286)(.79)}{1-.79^2}=.4211 \\
\beta_2= \frac{.9286-(.8919)(.79)}{1-.79^2}=.5959  \\
\]</span></p>
<p>Also, we saw that in the simple, two-variable case, <span class="math inline">\(b=r(\frac{sd_y}{sd_x})\)</span>. But what about now, when we have two predictors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>? How can we get unstandardized coefficients (<span class="math inline">\(\hat b_1, \hat b_2\)</span>) from our standardized ones (<span class="math inline">\(\hat \beta_1, \hat \beta_2\)</span>)? Above, we found that</p>
<p><span class="math display">\[
\hat z_y=0.4212\times z_{x1}+0.5959\times z_{x2}
\]</span></p>
<p>We can just apply the same logic to convert to unstandardized coefficients!</p>
<p><span class="math inline">\(b_1=\beta_1(\frac{sd_y}{sd_{x_1}})\)</span>, <span class="math inline">\(b_2=\beta_2(\frac{sd_y}{sd_{x2}})\)</span></p>
<p>And the intercept can be found using the coefficients by plugging in the mean of each variable and solving for it: <span class="math inline">\(b_0= \bar y-(b_1\bar x_1+ b_2 \bar x_2)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\hat y&amp;=b_0+ \beta_1 \left(\frac{sd_y}{sd_{x1}}\right) x_1 + \beta_2 \left(\frac{sd_y}{sd_{x2}}\right)x_2\\
\hat y&amp;=b_0+0.4212 \left(\frac{sd_y}{sd_{x1}}\right) x_1 + 0.5959 \left(\frac{sd_y}{sd_{x2}}\right)x_2
\end{aligned}\\
\text{Using SS from table above,}\\
\begin{aligned}
\hat y&amp;=b_0+0.4212 \left(\frac{\sqrt{14/3}}{\sqrt{110/3}}\right) x_1 + 0.5959 \left(\frac{\sqrt{14/3}}{\sqrt{14/3}}\right)x_2\\
&amp;=b_0+0.1503  x_1 + 0.5959 x_2
\end{aligned}
\]</span>
OK, but what about the intercept <span class="math inline">\(b_0\)</span>? We know that <span class="math inline">\(\bar y=14\)</span>, <span class="math inline">\(\bar x_1=8\)</span>, and <span class="math inline">\(\bar x_2=7\)</span> (see table above). Since the mean value (<span class="math inline">\(\bar x_1, \bar y\)</span>) will always been on the line (or in this case, plane) of best fit, we can plug them in and solve for <span class="math inline">\(b_0\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\bar y&amp;=b_0+0.1503(\bar x_1) + 0.5959 (\bar x_2)\\
14&amp;=b_0+0.1503(8) + 0.5959 (7)\\
b_0&amp;=14-1.2024-4.1713\\
&amp;=8.6263
\end{aligned}
\]</span></p>
<p>So our final (unscaled) equation becomes</p>
<p><span class="math display">\[
\hat y=8.6263+0.1503(x_1) + 0.5959 (x_2)
\]</span></p>
<p>Let’s see if R gives us the same estimates:</p>
<pre class="r"><code>coef(lm(y~x))</code></pre>
<pre><code>## (Intercept)          x1          x2 
##   8.6269430   0.1502591   0.5958549</code></pre>
<p>Looks good!</p>
</div>
<div id="multiple-r-and-r2" class="section level2">
<h2>Multiple <span class="math inline">\(R\)</span> and <span class="math inline">\(R^2\)</span></h2>
<p>Recall that earlier we saw that the proportion of variance in the outcome (y) explained by the predictor was <span class="math inline">\(R^2=\frac{SS_{regression}}{SS_{total}}\)</span>. We found that this was just the Pearson correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(r_{x,y}\)</span>, squared. But now we have two predictors! So what is <span class="math inline">\(R^2\)</span>?</p>
<p>Just as <span class="math inline">\(r\)</span> represents the degree of association between two variables, <span class="math inline">\(R\)</span> represents the degree of association between an outcome variable and an optimally weighted linear combination of the other variables. Since the fitted values <span class="math inline">\(\hat y\)</span> represent this best-fitting linear combination of the <span class="math inline">\(x\)</span>s, we could just use that (i.e., <span class="math inline">\(R=r_{y,\hat y}\)</span>). However, there is another formula I want to call your attention to:</p>
<p><span class="math display">\[
\begin{aligned}
R=R_{y,12}&amp;=\sqrt{\frac{r^2_{y,x_1}+r^2_{y,x_2}-2r_{y,x_1}r_{x_1,x_2}}{1-r^2_{x_1,x_2}}}\\
&amp;=\sqrt{\beta_1r_{y,x_1}+\beta_2r_{y,x_2}}
\end{aligned}
\]</span>
Let’s see it at work:</p>
<p><span class="math display">\[
R=\sqrt{.4211(.8919)+ .5959(.9286)}=.9638
\]</span></p>
<p>It turns out that the proportion of variance in <span class="math inline">\(y\)</span> shared with the optimal linear combination of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is <span class="math inline">\(R^2=.9638^2=.9289\)</span>.</p>
<p>Above we claimed that the correlation coefficient R is the same as the correlation between the fitted values <span class="math inline">\(\hat y\)</span> and the actual values <span class="math inline">\(y\)</span>. Let’s see for ourselves:</p>
<pre class="r"><code>#R
cor(y,lm(y~x)$fitted.values)</code></pre>
<pre><code>##           [,1]
## [1,] 0.9638161</code></pre>
<pre class="r"><code>#R squared
cor(y,lm(y~x)$fitted.values)^2</code></pre>
<pre><code>##           [,1]
## [1,] 0.9289415</code></pre>
<p>The better the regression line fits, the smaller <span class="math inline">\(SS_{res}\)</span> (<a href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions">This picture from wikipedia illustrates this nicely</a>).</p>
<p>As we discussed above, the sum of squared deviations of <span class="math inline">\(y\)</span> from its mean <span class="math inline">\(\bar y\)</span> (<span class="math inline">\(SS_{tot}\)</span>) can be paritioned into a sum of squared deviations of the fitted/predicted values <span class="math inline">\(\hat y\)</span> from <span class="math inline">\(\bar y\)</span> (the <span class="math inline">\(SS_{regression}\)</span>) and the leftover deviations of <span class="math inline">\(y\)</span> from the fitted values <span class="math inline">\(\hat y\)</span> (the <span class="math inline">\(SS_{residuals}\)</span>, or error).</p>
<p>Because <span class="math inline">\(SS_{reg}\)</span> is the sum of squared residuals after accounting for the regression line, <span class="math inline">\(\frac{SS_{reg}}{SS_{tot}}\)</span> gives the proportion of the error that was saved by using the regression line (i.e., the proportion of variance explained by the regression line).</p>
<p>Is this value (<span class="math inline">\(R^2=0.9289\)</span>) the same as <span class="math inline">\(R^2=\frac{SS_{regression}}{SS_{total}}=1-\frac{SS_{residuals}}{SS_{total}}\)</span>?</p>
<p><span class="math display">\[R^2=\frac{SS_{regression}}{SS_{total}}=\frac{\sum(b_0+b_1x_1+b_2x_2-\bar y)^2}{\sum(y-\bar y)^2}=\frac{\sum(\hat y-\bar y)^2}{\sum(y-\bar y)^2}=1-\frac{\sum(y- \hat y)^2}{\sum(y-\bar y)^2}\]</span></p>
<p><span class="math display">\[
\begin{matrix}
y &amp; \epsilon=y-\hat y&amp; \hat y &amp;= \ \ b_0+b_1x_1+b_2x_2 \\ \hline
12 &amp; -.3523 &amp;12.3523 &amp;=8.6263+0.1503(1) + 0.5959 (6) \\
13 &amp; .6425 &amp;12.3573 &amp;=8.6263+0.1503(5) + 0.5959 (5) \\
14 &amp; -.6010 &amp; 14.6012 &amp;=8.6263+0.1503(12) + 0.5959 (7) \\ 
17 &amp; .3109 &amp;16.6895 &amp;= 8.6263+0.1503(14) + 0.5959 (10) \\\hline
&amp;  \sum \epsilon^2=.995 &amp; \\
\end{matrix}
\]</span></p>
<p>So
<span class="math display">\[R^2=1-\frac{SS_{residuals}}{SS_{total}}=1-\frac{\sum(y-\hat y)}{\sum(y-\bar y)}=1-\frac{.995}{14}=.9289\]</span></p>
<p>Yep! 92.9% of the variability in <span class="math inline">\(y\)</span> can be accounted for by the combination of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</p>
<p>An easier way to think about this calculation is to break up our variance like so:</p>
<p><span class="math display">\[
\overbrace{\text{variance of } y}^{sd_y^2} = \overbrace{\text{variance of fitted values}}^{sd^2_{\hat y}} + \overbrace{\text{variance of residuals}}^{sd^2_{y-\hat y}}
\]</span>
It is mathematically equivalent, but I find the formula rather helpful!</p>
</div>
<div id="r2-in-one-bang-using-correlation-matricies" class="section level2">
<h2><span class="math inline">\(R^2\)</span> in one bang using correlation matricies</h2>
<p>Before we begin our discussion of semipartial and partial correlations, I want to show you a simple matrix calculation of <span class="math inline">\(R^2\)</span> that only requires the correlation matrix of the predictors <span class="math inline">\(\mathbf{R}\)</span> and the vector containing each predictor’s correlation with the outcome, <span class="math inline">\(\mathbf{r_{x,y}}=\begin{bmatrix}r_{y,x_1} &amp; r_{y,x_2} &amp;...\end{bmatrix}^{\text{T}}\)</span></p>
<p>Recall that above, we had this formula for the standardized regression coefficients:</p>
<p><span class="math display">\[
\begin{aligned}
\hat \beta&amp;=\mathbf{R}^{-1}\mathbf{r}_{y,x}\\
\begin{bmatrix}
\beta_1\\
\beta_2\\
\beta_3\\
\vdots
\end{bmatrix}&amp;=
\begin{bmatrix}
1 &amp; r_{x_1,x_2} &amp; r_{x_1,x_3} &amp; ...\\
r_{x_1,x_2}&amp; 1 &amp; r_{x_2,x_3} &amp; ...\\
r_{x_1,x_3}&amp;r_{x_2,x_3}&amp;1 &amp;... \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{bmatrix}^{-1}
\begin{bmatrix}
r_{y,x_1}\\
r_{y,x_2}\\
r_{y,x_3}\\
\vdots\\
\end{bmatrix}
\end{aligned}
\]</span>
If we (matrix) multiply both sides of this equation by <span class="math inline">\(\mathbf{r}_{x,y}^{\text{T}}=\begin{bmatrix}r_{y,x_1} &amp; r_{y,x_2} &amp;...\end{bmatrix}\)</span>, something really neat happens. I’ll shorten the depictions to the two-predictor case, but the formula holds in general:</p>
<p><span class="math display">\[
\begin{aligned}
 \mathbf{r}_{x,y}^{\text{T}} \hat \beta&amp;=\mathbf{r}_{x,y}^{\text{T}}\mathbf{R}^{-1}\mathbf{r}_{y,x}\\
\begin{bmatrix}r_{y,x_1} &amp; r_{y,x_2} &amp;...\end{bmatrix}
\begin{bmatrix}
\beta_1\\
\beta_2\\
\vdots
\end{bmatrix}&amp;=
\begin{bmatrix}r_{y,x_1} &amp; r_{y,x_2} &amp;...\end{bmatrix}
\begin{bmatrix}
1 &amp; r_{x_1,x_2} &amp; ...\\
r_{x_1,x_2}&amp; 1 &amp;  ...\\
\vdots &amp; \vdots &amp;  \ddots \\
\end{bmatrix}^{-1}
\begin{bmatrix}
r_{y,x_1}\\
r_{y,x_2}\\
\vdots\\
\end{bmatrix}\\
r_{y,x_1}\beta_1+r_{y,x_2}\beta_2&amp;=\begin{bmatrix} \frac{r_{y,x_1}-r_{y,x_2}r_{x_1,x_2}}{1-r^2_{x_1,x_2}} &amp;\frac{-r_{y,x_1}r_{x_1,x_2}+r_{y,x_2}}{1-r^2_{x_1,x_2}}\end{bmatrix}
\begin{bmatrix}
r_{y,x_1}\\
r_{y,x_2}\\
\end{bmatrix}\\
&amp;=\frac{r^2_{y,x_1}-r_{y,x_2}r_{x_1,x_2}r_{y,x_1}}{1-r^2_{x_1,x_2}} +\frac{-r_{y,x_1}r_{x_1,x_2}r_{y,x_2}+r^2_{y,x_2}}{1-r^2_{x_1,x_2}}\\
&amp;=\frac{r^2_{y,x_1}+r^2_{y,x_2}-2r_{y,x_2}r_{x_1,x_2}r_{y,x_1}}{1-r^2_{x_1,x_2}}\\
&amp;=R^2
\end{aligned}
\]</span></p>
<p>All of these are equal to <span class="math inline">\(R^2\)</span>! Specifically <span class="math inline">\(R^2\)</span> is equal to the sum of the products of the correlation of <span class="math inline">\(y\)</span> with each predictor times the standardized coefficient for that predcitor (in the two-variable case, <span class="math inline">\(R^2=r_{y,x_1}\beta_1+r_{y,x_2}\beta_2\)</span>). <span class="math inline">\(R^2\)</span> also equals the sum of the univariate <span class="math inline">\(r^2\)</span>s minus twice the product of all three unique correlations (<span class="math inline">\(2r_{y,x_2}r_{x_1,x_2}r_{y,x_1}\)</span>), divided by the variance not shared by the predictors (<span class="math inline">\(1-r_{x_1,x_2}^2\)</span>).</p>
<p>Don’t believe me?</p>
<pre class="r"><code>coryx&lt;-matrix(c(cor(y,x[,1]),cor(y,x[,2])))
corxx&lt;-cor(x)
#as before, the correlation of y with each x
coryx</code></pre>
<pre><code>##           [,1]
## [1,] 0.8918826
## [2,] 0.9285714</code></pre>
<pre class="r"><code>#and the correlation matrix of the xs
corxx</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 1.0000000 0.7899531
## [2,] 0.7899531 1.0000000</code></pre>
<pre class="r"><code>#now, R2 should be equal to 
t(coryx)%*%solve(corxx)%*%coryx</code></pre>
<pre><code>##           [,1]
## [1,] 0.9289415</code></pre>
<pre class="r"><code>#we should also be able to get it from the betas like so
t(coryx)%*%coef(lm(scale(y)~scale(x)))[2:3]</code></pre>
<pre><code>##           [,1]
## [1,] 0.9289415</code></pre>
<pre class="r"><code>#perfect!</code></pre>
</div>
</div>
<div id="semipartial-and-partial-correlations" class="section level1">
<h1>Semipartial and Partial Correlations</h1>
<p>The <span class="math inline">\(R^2\)</span> value is extremely informative, but you might want to know how much of the variability in <span class="math inline">\(y\)</span> is due to <span class="math inline">\(x_1\)</span>, holding <span class="math inline">\(x_2\)</span> constant. This is known as a <strong>semi-partial</strong> (or “part” but <em>not</em> “partial”) correlation. We will represent this as <span class="math inline">\(r_{y(x_1 | x_2)}\)</span>. This is the correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span> with the effect of <span class="math inline">\(x_2\)</span> removed from <span class="math inline">\(x_1\)</span> (but <em>not</em> from <span class="math inline">\(y_1\)</span>).</p>
<p>On the other hand, we could remove the effect of <span class="math inline">\(x_2\)</span> from both <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1\)</span> before we calculate the correlation. This would result in the <strong>partial</strong> correlation of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span>, which we will denote <span class="math inline">\(r_{y,x_1|x_2}\)</span>.</p>
<p>The difference between semi-partial and partial correlations can be difficult to see at first, so let’s look at this helpful venn diagram:</p>
<center>
<img src="variancevenn2.png" />
</center>
<p>Imagine that the variance of each variable is represented by a circle with an area of 1. The overlapping area of <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(y\)</span> (<span class="math inline">\(A+B+C\)</span>) is <span class="math inline">\(R^2\)</span>. Interestingly, the squared correlation of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(r^2_{y,x_1}\)</span> (or equivalently, the <span class="math inline">\(r^2\)</span> of the regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span>) is <span class="math inline">\(A+C\)</span>. Likewise, the <span class="math inline">\(r^2_{y,x_2}=B+C\)</span></p>
<p>The section marked <span class="math inline">\(A\)</span> is the variance of <span class="math inline">\(y\)</span> shared uniquely with <span class="math inline">\(x_1\)</span>, and the section marked <span class="math inline">\(B\)</span> is the variance of <span class="math inline">\(y\)</span> shared uniquely with <span class="math inline">\(x_2\)</span>. These represent the square of what is known as the <strong>semipartial</strong> (or “part”) correlations (that is, <span class="math inline">\(A=r_{y,x_1|x_2}^2\)</span> and <span class="math inline">\(B=r_{y,x_2|x_1}^2\)</span>).</p>
<p>Notice that when we remove the effect of <span class="math inline">\(x_2\)</span>, the squared semipartial correlation of <span class="math inline">\(x_1\)</span> with <span class="math inline">\(y\)</span> is <span class="math inline">\(\frac{A}{A+B+C+D}=\frac{A}{1}=A\)</span>.</p>
<p>Here is a full table of the relationships in the Venn diagram:</p>
<p><span class="math display">\[
\begin{matrix}
\text{correlation }(r) &amp;  \text{semipartial } (sr_1) &amp; \text{partial }pr_1\\
\begin{aligned}
&amp;=r^2_{y,x_1}\\ 
&amp;=\frac{A+C}{A+B+C+D}\\ &amp;\\
&amp;=\frac{F-FB-FC}{A+D}
\end{aligned}
&amp;
\begin{aligned}
&amp;=r^2_{y,x_1|x_2}\\
&amp;=\frac{A}{A+D}\\ &amp;\\
&amp;=\frac{F-FB-GB}{A+D}
\end{aligned}
&amp;
\begin{aligned}
&amp;=r^2_{y|x_2,x_1|x_2}\\
&amp;=\frac{A}{A+B+C+D}\\ &amp;\\
&amp;=A
\end{aligned}
\end{matrix}
\]</span></p>
<p>These squared semipartials are useful: they are equal to the increase in <span class="math inline">\(R^2\)</span> when the second predictor is added to the model. This makes them easy to calculate: to find <span class="math inline">\(A\)</span> above, all we need is <span class="math inline">\(r^2_{y,x_1|x_2}=R^2-r^2_{y,x_1}\)</span>.
To find <span class="math inline">\(B\)</span>, <span class="math inline">\(r^2_{y,x_2|x_1}=R^2-r^2_{y,x_1}\)</span>.</p>
<p>For example, we know from above that <span class="math inline">\(R^2=.9289\)</span> and <span class="math inline">\(r_{y,x_2}=.9286\)</span>, so <span class="math inline">\(A=r^2{y,x_1|x_2}=.9289-.9286^2=.0666\)</span>. Also, since <span class="math inline">\(r_{y,x_1}=.8919\)</span>, <span class="math inline">\(B=r^2{y,x_2|x_1}=.9289-.8919^2=.1334\)</span>. We can say that 6.66% of the variance in <span class="math inline">\(y\)</span> is shared uniquely with <span class="math inline">\(x_1\)</span> and that 13.34% of the variance in <span class="math inline">\(y\)</span> is shared uniquely with <span class="math inline">\(x_2\)</span>.</p>
<p>There is also a useful formula for calculating these semipartial correlations directly with just three values: <span class="math inline">\(r_{y,x_2}=.9286\)</span>, <span class="math inline">\(r_{y,x_1}=.8919\)</span>, and <span class="math inline">\(r_{x_1,x_2}=.79\)</span></p>
<p><span class="math display">\[
\begin{aligned}
sr_1=r_{y,x_1|x_2}&amp;=\frac{r_{y,x_1}-r_{y,x_2}r_{x_1,x_2}}{\sqrt{1-r^2_{x_1,x_2}}}\\
&amp;=\frac{.8919-.9286(.79)}{\sqrt{1-.79^2}}\\
&amp;=\frac{.1583}{.6131}\\
&amp;=.2582\\
(&amp;=\sqrt{.0666})
\end{aligned}
\]</span></p>
<div id="semipartial-correlations-the-residuals-approach" class="section level2">
<h2>Semipartial correlations: the residuals approach</h2>
<p>If you thought that semipartial correlations were confusing, this might help conceptually.</p>
<p>We said at the start that semipartial correlations represented the correlation of <span class="math inline">\(y\)</span> with <span class="math inline">\(x_1\)</span> when the effects of <span class="math inline">\(x_2\)</span> have been removed from <span class="math inline">\(x_1\)</span>. To remove the effects of <span class="math inline">\(x_2\)</span> from <span class="math inline">\(x_1\)</span>, we regress <span class="math inline">\(x_1\)</span> on <span class="math inline">\(x_2\)</span> (that is, treating <span class="math inline">\(x_1\)</span> as the dependent variable), and we save the residuals (the part of <span class="math inline">\(x_2\)</span> unexplained by <span class="math inline">\(x_2\)</span>). Then we correlate these <span class="math inline">\(x_1\)</span> residuals (with the effect of <span class="math inline">\(x_2\)</span> removed) with <span class="math inline">\(y\)</span> to get our semipartial correlation:</p>
<pre class="r"><code>res_x1.x2&lt;-lm(x[,1]~x[,2])$res

cor(res_x1.x2,y)</code></pre>
<pre><code>##           [,1]
## [1,] 0.2582569</code></pre>
<p>Semipartial correlations are called <em>semi-</em>partial because the effect of a controlling variable (e.g., <span class="math inline">\(x_2\)</span>) is removed from the predictor (<span class="math inline">\(x_1\)</span>) but <em>not</em> from the outcome (<span class="math inline">\(y\)</span>). To the get partial correlation, we just remove the effect of the controlling variable from the outcome too! Let’s represent the partial correlation of <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1\)</span> (with the effect of <span class="math inline">\(x_2\)</span> removed from both <span class="math inline">\(y\)</span> <em>and</em> <span class="math inline">\(x_1\)</span>) as <span class="math inline">\(pr=r_{y|x_2,x_1|x_2}\)</span>. When we square partial correlations, we get the proportion of variance in <span class="math inline">\(y\)</span> <em>not</em> associated with <span class="math inline">\(x_2\)</span> that <em>is</em> associated with <span class="math inline">\(x_1\)</span>.</p>
<p>In the Venn diagram above, this quantity is found by</p>
<p><span class="math display">\[
pr_1^2=r^2_{y|x_2,x_1|x_2}=\frac{a}{a+d}=\frac{R^2-r^2_{y,x_2}}{1-r^2_{y,x_2}}
\]</span></p>
<p>Because we are completely removing variance from <span class="math inline">\(y\)</span>, the denominator of partial correlations will be smaller than for semi-partial (which includes all of the variance in <span class="math inline">\(y\)</span>). Thus, partial correlations will always be greater than semipartial correlations. Partial and semipartial correlations <span class="math inline">\(sr_1\)</span> and <span class="math inline">\(pr_1\)</span> will only ever be equal to each other if the other predictor <span class="math inline">\(x_2\)</span> is uncorrelated with <span class="math inline">\(y\)</span> (i.e., if <span class="math inline">\(r_{y,x_2}=0\)</span>). We can see this using this formula for the partial correlation:</p>
<p><span class="math display">\[
\begin{aligned}
pr_1=r_{y|x_2,x_1|x_2}&amp;=\frac{r_{y,x_1}-r_{y,x_2}r_{x_1,x_2}}{\sqrt{1-r^2_{x_1,x_2}}\sqrt{1-r^2_{y,x_2}}}\\
\text{if } r_{y,x_2}=0,\\
&amp;=\frac{r_{y,x_1}}{\sqrt{1-r^2_{x_1,x_2}}}\\
(\text{which equals }sr_1 \text{ when } r_{y,x_2}=0)
\end{aligned}
\]</span></p>
<p>Let’s go ahead and use this formula and our correlations to calculate the partial correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1\)</span>, removing the effect of <span class="math inline">\(x_2\)</span> from each. We will need <span class="math inline">\(r_{y,x_2}=.9286\)</span>, <span class="math inline">\(r_{y,x_1}=.8919\)</span>, and <span class="math inline">\(r_{x_1,x_2}=.79\)</span></p>
<p><span class="math display">\[
\begin{aligned}
pr_1=r_{y|x_2,x_1|x_2}&amp;=\frac{r_{y,x_1}-r_{y,x_2}r_{x_1,x_2}}{\sqrt{1-r^2_{x_1,x_2}}\sqrt{1-r^2_{y,x_2}}}\\
&amp;=\frac{.8919-.9286(.79)}{\sqrt{1-.79^2}\sqrt{1-.9286^2}}\\
&amp;=\frac{.1583}{.6131*.3711}\\
&amp;=.6958
\end{aligned}
\]</span></p>
</div>
<div id="increments-to-r2-when-adding-predictors" class="section level2">
<h2>Increments to <span class="math inline">\(R^2\)</span> when adding predictors</h2>
<p>It is important to realize that you can build <span class="math inline">\(R^2\)</span> from semipartial correlations. You may have noticed this already, but take a look:</p>
<p><span class="math display">\[
R^2=r^2_{y,x_1}+r^2_{y,x_2|x_1}+r^2_{x_3|x_1,x2}+...
\]</span></p>
<p>First, using our running example, we can take squared correlation of <span class="math inline">\(y\)</span> with <span class="math inline">\(x_2\)</span> (<span class="math inline">\(r_{y,x2}=.9286^2=.8623\)</span>) and then add to it the squared semipartial correlation of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span> with the effect of <span class="math inline">\(x_2\)</span> removed, <span class="math inline">\(r^2_{y,x1|x2}=.2582^2=.0666\)</span>.</p>
<p><span class="math display">\[
R^2=.8623+.0666=.9289
\]</span></p>
<p>The take-home here is that you can keep on adding variables in this fashion! When you add a new predictor, if it has a correlation with <span class="math inline">\(y\)</span> when you have removed the effects of the other predictors in the model, then <span class="math inline">\(R^2\)</span> will increase by the square of the semipartial correlation!</p>
<p>Let’s look at what happens when we add a third predictor. Unfortunately, because there are only four data points and we are estimating four parameters (including the intercept), we will be able to predict the data perfectly (serious overfitting, leaving no residuals). So we need another observation on each variable too.</p>
<p><span class="math display">\[
\begin{matrix}
y &amp; x_1 &amp; x_2 &amp; x_3 \\ \hline
12 &amp; 1 &amp; 6 &amp; 2\\
13 &amp; 5 &amp; 5 &amp; 4\\
14 &amp; 12 &amp; 7 &amp; 6 \\ 
17 &amp; 14 &amp; 10 &amp; 8 \\
24 &amp; 18 &amp; 17 &amp; 25 \\ \hline
\bar y=16&amp;  \bar x_1=10 &amp; \bar x_2=9 &amp;\bar x_3=10\\
\end{matrix}
\]</span>
First, let’s find <span class="math inline">\(r^2_{y,x_1}\)</span> by regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(x_1\)</span> (and confirm that this equals the square of <span class="math inline">\(r_{y,x_1}\)</span> and also the square of the correlation between <span class="math inline">\(y\)</span> and the fitted values <span class="math inline">\(\hat y\)</span>):</p>
<pre class="r"><code>x=matrix(cbind(x,c(2,4,6,8)),nrow=4)
x=matrix(rbind(x,c(18,17,25)),nrow=5)
y&lt;-matrix(rbind(y,24),nrow=5)

summary(lm(y~x[,1]))$r.squared</code></pre>
<pre><code>## [1] 0.7404815</code></pre>
<pre class="r"><code>cor(y,x[,1])^2</code></pre>
<pre><code>##           [,1]
## [1,] 0.7404815</code></pre>
<pre class="r"><code>cor(lm(y~x[,1])$fitted.values,y)^2</code></pre>
<pre><code>##           [,1]
## [1,] 0.7404815</code></pre>
<p>Check! <span class="math inline">\(r^2_{y,x_1}=.7405\)</span>. Now we need the semipartial correlation of <span class="math inline">\(y\)</span> and <span class="math inline">\(x_2\)</span> (with the effect of <span class="math inline">\(x_1\)</span> removed from <span class="math inline">\(x_2\)</span>), <span class="math inline">\(r_{y,x_2|x_1}\)</span>. An easy way is to (1) regress <span class="math inline">\(x_2\)</span> on <span class="math inline">\(x_1\)</span> and take the residuals (the part of <span class="math inline">\(x_2\)</span> not associated with <span class="math inline">\(x_1\)</span>), and use them to predict <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>res_x2_x1&lt;-lm(x[,2]~x[,1])$residuals

summary(lm(y~res_x2_x1))$r.squared</code></pre>
<pre><code>## [1] 0.2432009</code></pre>
<pre class="r"><code>#correlation of y with the residuals, quantity squared
cor(y,lm(y~res_x2_x1)$fitted.values)^2</code></pre>
<pre><code>##           [,1]
## [1,] 0.2432009</code></pre>
<p>OK, so <span class="math inline">\(r^2_{y,x_1}=.7405\)</span> and <span class="math inline">\(r^2_{y,x_2|x_1}=.2432\)</span>, and their sum is <span class="math inline">\(.9837\)</span>. Clearly there is not much additional variability for the third predictor to explain.</p>
<p>But how can we find the third predictor’s contribution to <span class="math inline">\(R^2\)</span>, <span class="math inline">\(r_{y,x1|x2,x3}\)</span>?</p>
<p>The same way as before: remove the effects of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> from <span class="math inline">\(x_3\)</span> and see how much variability those residuals share with <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>res_x3_x1x2&lt;-lm(x[,3]~x[,1]+x[,2])$residuals

summary(lm(y~res_x3_x1x2))$r.squared</code></pre>
<pre><code>## [1] 0.008596529</code></pre>
<p>All together,</p>
<p><span class="math display">\[
\begin{aligned}
R^2=R^2_{y.123}&amp;=r^2_{y,x_1}+r^2_{y,x_2|x_1}+r^2_{y,x_3|x_1,x2}\\
&amp;=.7405+.2432+.0086\\
&amp;=.9923
\end{aligned}
\]</span></p>
<p>We can confirm that this is the total <span class="math inline">\(R^2\)</span> with the full regression:</p>
<pre class="r"><code>round(summary(lm(y~x[,1]+x[,2]+x[,3]))$r.squared,4)</code></pre>
<pre><code>## [1] 0.9923</code></pre>
<p>Notice that it is easy to find squared semipartial correlations by subtracting <span class="math inline">\(R^2\)</span> from different models.</p>
<p>For example, let’s find how much variability in <span class="math inline">\(y\)</span> is shared with <span class="math inline">\(x_1\)</span> when you remove the effects of <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_2\)</span> (i.e., <span class="math inline">\(r^2_{y,x_1|x_2,x_3}\)</span>). As we will see, this is equivalent to the increase in <span class="math inline">\(R^2\)</span> when you add <span class="math inline">\(x_1\)</span> to the regression predicting <span class="math inline">\(y\)</span> that already contains <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>. We can use this logic and work backwards: first, we find the <span class="math inline">\(R^2\)</span> when all three predictors are in the model (we can denote this <span class="math inline">\(R^2_{y.123}\)</span>), and subtract from it the <span class="math inline">\(R^2\)</span> when only predictors <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are in the model (<span class="math inline">\(R^2_{y.23}\)</span>). What you are left with will be <span class="math inline">\(r^2_{y,x1|x2,x3}\)</span>. Again,</p>
<p><span class="math display">\[
r^2_{y,x_1|x_2,x_3}=R^2_{y.123}-R^2_{y.23}
\]</span></p>
<p>Don’t take my word for it. Let’s try it!</p>
<p>Here’s the <span class="math inline">\(R^2\)</span> when all three predictors are in the model (<span class="math inline">\(R^2_{y.123}\)</span>):</p>
<pre class="r"><code>summary(lm(y~x[,1]+x[,2]+x[,3]))$r.squared</code></pre>
<pre><code>## [1] 0.992279</code></pre>
<p>And here’s the <span class="math inline">\(R^2\)</span> when just predictors <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are in the model (<span class="math inline">\(R^2_{y.23}\)</span>):</p>
<pre class="r"><code>summary(lm(y~x[,2]+x[,3]))$r.squared</code></pre>
<pre><code>## [1] 0.9876434</code></pre>
<p>Thus,
<span class="math display">\[
\begin{aligned}
r^2_{y,x_1|x_2,x_3}&amp;=R^2_{y.123}-R^2_{y.23}\\
&amp;=.9923-.9876\\
&amp;=.0047
\end{aligned}
\]</span>
This means that when <span class="math inline">\(x_1\)</span> is added to a model that already contains <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>, the increase in <span class="math inline">\(R^2\)</span> is very small: it explains just 0.5% of the additional variability!</p>
<p>But we saw earlier that when <span class="math inline">\(x_1\)</span> is the only predictor in the model, <span class="math inline">\(R^2=r^2_{y,x_1}=.7405\)</span>. With no other predictors in the model, <span class="math inline">\(x_1\)</span> accounts for almost 75% of the variability in <span class="math inline">\(y\)</span>!</p>
<p>This is because the predictors are highly intercorrelated: they share a lot of variance!</p>
</div>
<div id="multicollinearity" class="section level2">
<h2>Multicollinearity</h2>
<p>Multicollinearity is observed when a given predictor can be predicted from the others very accurately. For example, using the data above, the <span class="math inline">\(R^2\)</span> from the model predicting <span class="math inline">\(x_3\)</span> from <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (call it <span class="math inline">\(r^2_{x_3.12}\)</span>) is</p>
<pre class="r"><code>summary(lm(x[,3]~x[,1]+x[,2]))$r.squared</code></pre>
<pre><code>## [1] 0.9473233</code></pre>
<p>Extremely high! This is the reason that <span class="math inline">\(x_3\)</span> adds so little when added to a model containing <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>: it is almost entirely redundant with those variables!</p>
<p>Multicollinearity affects cofficient estimates by making them less precise (large standard errors). This is because collinear predictors contain mostly the same information about the dependent variable <span class="math inline">\(y\)</span> and are thus extremely sensitive to the presence of other predictors. Regression coefficients are interpreted as the impact of one variable (e.g., <span class="math inline">\(x_1\)</span>) with the other variables held constant. But because of multicolinearity, we do not have a set of observations for which changes in <span class="math inline">\(x_1\)</span> are independent of changes in the other variables!</p>
<p>Measuring multicollinearity for a variable (say <span class="math inline">\(x_3\)</span>) is often done using the <em>tolerance</em> (<span class="math inline">\(1-r^2_{x_3.12}\)</span>). Here, <span class="math inline">\(tolerance=.053\)</span>. More commonly, the inverse of the tolerance (known as the variance inflation factor, or VIF) is used.
<span class="math display">\[
VIF=\frac{1}{tolerance}=\frac{1}{.053}=18.87
\]</span>
Here, this VIF for predictor <span class="math inline">\(x_3\)</span> is very high. Common cut-offs for problematic variables are 5 (conservative) or 10 (liberal).</p>
</div>
</div>
<div id="regression-assumptions" class="section level1">
<h1>Regression assumptions</h1>
<p>I have already mentioned all of the assumptions of linear regression in this post, but I will bring them all in once place here!</p>
<ul>
<li>Linear relationship</li>
<li>Independent observations (or equivalently, errors)</li>
<li>Normally distributed errors</li>
<li>Homoskedasticity (constant variance)</li>
<li>No outliers (haven’t discussed)</li>
<li>No severe multicolinearity (e.g., <span class="math inline">\(VIF&gt;5\)</span>)</li>
</ul>
</div>
<div id="partial-correlations" class="section level1">
<h1>Partial correlations</h1>
<p>Finding the partial correlation from residuals is easy: we do just as we did for semipartials, except we also have to remove the effect of <span class="math inline">\(x_2\)</span> from <span class="math inline">\(y\)</span>. The full procedure is as follows: regress <span class="math inline">\(y\)</span> on <span class="math inline">\(x_2\)</span> and save the residuals <span class="math inline">\(\epsilon_{y|x_2}\)</span>; regress <span class="math inline">\(x_1\)</span> on <span class="math inline">\(x_2\)</span> and save the residuals <span class="math inline">\(\epsilon_{x_1|x_2}\)</span>. Finally, compute the correlation between both sets of residuals (i.e., <span class="math inline">\(r_{\epsilon_{y|x_2},\epsilon_{x_1|x_2}}\)</span>).</p>
<pre class="r"><code>res_x1.x2&lt;-lm(x[,1]~x[,2])$res
res_y.x2&lt;-lm(y~x[,2])$res

cor(res_x1.x2,res_y.x2)</code></pre>
<pre><code>## [1] 0.4785116</code></pre>
<p>Thus, we can say that <span class="math inline">\(pr^2_1=.6958^2=48.4\%\)</span> of the variance in <span class="math inline">\(y\)</span> is uniquely associated with <span class="math inline">\(x_1\)</span>.</p>
<p>Let’s say <span class="math inline">\(y\)</span> is a vector of <span class="math inline">\(i\)</span> people’s scores on an survey measuring attitude toward abortion, <span class="math inline">\(x1\)</span> is a vector of political ideology scores, and <span class="math inline">\(x2\)</span> is a vector of religiosity scores. Clearly the outcome would be correlated with each of the variables in isolation, but what if we want to model <span class="math inline">\(y\)</span> as a function of both? Regression allows us to assess the unique relationship between a predictor and an outcome variable conditional on other predictors.</p>
<p>For example, the relationship between political views (x) and abortion attitudes (y) may appear strong, but if removing the effect of religiosity (z) from both leaves little or no variation in common (i.e., the partial correlation <span class="math inline">\(r_{y,x|z}\)</span> is near zero; see below), then the influence of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> is likely spurious, since it no longer exists given <span class="math inline">\(z\)</span>. Another way to think about the relationship of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> “controlling for” <span class="math inline">\(z\)</span> is to look at the relationship of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> only in groups that have the same value for <span class="math inline">\(z\)</span></p>
<script>
function myFunction() {
    var x = document.getElementById('myDIV');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</div>

  </div>
  


<div class="navigation navigation-single">
    
    <a href="/posts/statistical-validity/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Validity in Research</span>
    </a>
    
    
    <a href="/posts/enemy-tank-problem/" class="navigation-next">
      <span class="navigation-title">Enemy Tank Problem</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
