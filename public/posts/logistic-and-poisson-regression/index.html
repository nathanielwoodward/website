<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.60.1" />

    
    
    

<title>Binary and Count Prediction • Nathaniel Woodward</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Binary and Count Prediction"/>
<meta name="twitter:description" content="Let’s enrich our previous discussion of linear regression by considering outcome data that is binary/dichotomous (e.g., success/failure, whether someone recovered or not) and data that is count-based (e.g., number of distinct species per survey site).
In a regular linear regression, we model an outcome \(Y\) as a linear function of explanatory variables \(X_i\) and error \(e\), like this
\[ Y=\beta_0&#43;\beta_1X_1&#43;\dots&#43;\beta_nX_n&#43;e, \phantom{xxxxx} e\sim N(0,\sigma) \] Let’s focus on the case of just a single predictor,"/>

<meta property="og:title" content="Binary and Count Prediction" />
<meta property="og:description" content="Let’s enrich our previous discussion of linear regression by considering outcome data that is binary/dichotomous (e.g., success/failure, whether someone recovered or not) and data that is count-based (e.g., number of distinct species per survey site).
In a regular linear regression, we model an outcome \(Y\) as a linear function of explanatory variables \(X_i\) and error \(e\), like this
\[ Y=\beta_0&#43;\beta_1X_1&#43;\dots&#43;\beta_nX_n&#43;e, \phantom{xxxxx} e\sim N(0,\sigma) \] Let’s focus on the case of just a single predictor," />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/logistic-and-poisson-regression/" />
<meta property="article:published_time" content="2019-07-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-07-24T00:00:00+00:00" /><meta property="og:site_name" content=" " />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.43109490338b28cdd7a74845262efe59bf66fdc3530f5e818a66c9a579a79080.css" integrity="sha256-QxCUkDOLKM3Xp0hFJi7&#43;Wb9m/cNTD16BimbJpXmnkIA=">


<link rel="stylesheet" href="/scss/print.8c61428a4ae8b36c028e9198876312a2de5a2b3c80bbbdcceb98d738691c4f6b.css" integrity="sha256-jGFCikros2wCjpGYh2MSot5aKzyAu73M65jXOGkcT2s=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    <script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script type="text/javascript" src="/js/bigfoot.min.js"></script>
<script type="text/javascript">
    $.bigfoot();
</script>

</head>


    <body class="theme-base-09 ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
    
      
        
        
        
        
        <div class="author-image">
           <a href="/"><img src="/img/headshot_cropped_f2019.png" alt="Nathaniel Woodward" class="img--circle img--headshot element--center"></a>
        </div>
        
      
      <p class="site__description">
      <a href="mailto:nathanielraley@gmail.com"> Nathaniel Woodward 
      </a></p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Nathaniel Woodward</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Portfolio</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/teaching/">
						<span>Teaching</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/resume.html">
						<span>Vitae</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <br>
        <section class="social">
	
	<a href="https://twitter.com/Distributino" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/nathanielwoodward" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/nathanielraley" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://last.fm/user/ramimba" rel="me"><i class="fab fa-lastfm fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2022  
  
    <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  
</div>



  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Binary and Count Prediction</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Jul 24, 2019
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 41 min read
</div>


  </header>
  
  
  <div class="post">
    


<p>Let’s enrich our previous discussion of linear regression by considering outcome data that is binary/dichotomous (e.g., success/failure, whether someone recovered or not) and data that is count-based (e.g., number of distinct species per survey site).</p>
<p>In a regular linear regression, we model an outcome <span class="math inline">\(Y\)</span> as a linear function of explanatory variables <span class="math inline">\(X_i\)</span> and error <span class="math inline">\(e\)</span>, like this</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1X_1+\dots+\beta_nX_n+e, \phantom{xxxxx} e\sim N(0,\sigma)
\]</span> Let’s focus on the case of just a single predictor,</p>
<p><span class="math display">\[
Y=\beta_0+\beta_1X_1+e, \phantom{xxxxx} e\sim N(0,\sigma)
\]</span></p>
<p>Recall that the residuals (deviations from data to prediction) are normally distributed, since you can rearrange the above to <span class="math inline">\(Y-\beta_0+\beta_1X_1=e, \phantom{xxxxx} e\sim N(0,\sigma)\)</span></p>
<p>Also, recall that <span class="math inline">\(\beta_0+\beta_1X_1\)</span> is equal to the mean of <span class="math inline">\(Y\)</span> where <span class="math inline">\(X_1=x_i\)</span>. That is <span class="math inline">\(\mu=E[Y|X]=\beta_0+\beta_1X_1\)</span>, and therefore we are modeling the response variable <span class="math inline">\(Y\sim N(\beta_0+\beta_1X_1,\sigma)\)</span></p>
<p>What happens if we try to run a linear regression on binary response data? Let’s use the <code>biopsy</code> dataset include with the MASS package. This dataset includes nine attributes of tumors and whether they were malignant (1) or benign (0). Let’s predict this outcome from the clump thickness using simple linear regression.</p>
<pre class="r"><code>library(dplyr)
library(MASS)
library(ggplot2)

data&lt;-biopsy%&gt;%transmute(clump_thickness=V1,
                         cell_uniformity=V2,
                         marg_adhesion=V3,
                         bland_chromatin=V7,
                         outcome=class,
                         y=as.numeric(outcome)-1)
head(data)</code></pre>
<pre><code>##   clump_thickness cell_uniformity marg_adhesion bland_chromatin   outcome
## 1               5               1             1               3    benign
## 2               5               4             4               3    benign
## 3               3               1             1               3    benign
## 4               6               8             8               3    benign
## 5               4               1             1               3    benign
## 6               8              10            10               9 malignant
##   y
## 1 0
## 2 0
## 3 0
## 4 0
## 5 0
## 6 1</code></pre>
<pre class="r"><code>fit&lt;-lm(y~clump_thickness,data=data)
summary(fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ clump_thickness, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.77804 -0.17331 -0.01994  0.06859  1.06859 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -0.189535   0.023395  -8.102 2.43e-15 ***
## clump_thickness  0.120947   0.004467  27.078  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3323 on 697 degrees of freedom
## Multiple R-squared:  0.5127, Adjusted R-squared:  0.512 
## F-statistic: 733.2 on 1 and 697 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>ggplot(data, aes(clump_thickness,y))+geom_jitter(width=.3,height=0)+geom_smooth(method=&#39;lm&#39;)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Well, it runs! But as you can see, we are violating many assumptions here (the residuals will certainly not be normal). Also, your outcome data can only be 0/1, but predicted values can be greater than 1 or less than 0, which is not appropriate. For example, someone with a clump thickness of <span class="math inline">\(X=10\)</span> would have a predicted probability of <span class="math inline">\(-.190+.121(10)=1.02\)</span>, or 102%. This is clearly the wrong tool for the job.</p>
<p>Above, we used the structural part of the model (i.e., the part without the error) <span class="math inline">\(\mu=\beta_0+\beta_1X_1\)</span>, where <span class="math inline">\(\mu\)</span> is the mean of <span class="math inline">\(Y\)</span> at a specific value of <span class="math inline">\(X_1\)</span>. This works when the response is continuous and the assumption of normality is met. Instead, however, we could use <span class="math inline">\(g(\mu)=\beta_0+\beta_1X_1\)</span>, where <span class="math inline">\(g()\)</span> is some function that links the response to the predictors in an acceptable, continuous fashion.</p>
<p>In the case of binary outcome data, <span class="math inline">\(g()\)</span> would need to take <span class="math inline">\([0,1]\)</span> data and output continuous data (or vice versa). One idea is to use a cumulative distribution function (CDF). For example, the normal distribution is defined over <span class="math inline">\((-\infty, \infty)\)</span>, but the cumulative probability (the area under the distribution) is defined over <span class="math inline">\([0,1]\)</span>, since <span class="math inline">\(\int_{-\infty}^a \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx \in [0,1]\)</span> for all <span class="math inline">\(a\)</span>. Since the normal CDF <span class="math inline">\(\Phi(z)=P(Z\le z), Z \sim N(0,1)\)</span> maps from all real numbers <span class="math inline">\(z\)</span> to <span class="math inline">\([0,1]\)</span>, it’s inverse <span class="math inline">\(\Phi^{-1}(x)\)</span> will do the opposite.</p>
<pre class="r"><code>#normal CDF
curve(pnorm(x,0,1),-4,4)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#inverse
curve(qnorm(x,0,1),0,1)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-2-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>Let’s regress <span class="math inline">\(y\)</span> using the probit link function</p>
<pre class="r"><code>fit&lt;-glm(y~clump_thickness,data=data,family=binomial(link=&quot;probit&quot;))
summary(fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_thickness, family = binomial(link = &quot;probit&quot;), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1443  -0.4535  -0.1421   0.1450   3.0332  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -2.83936    0.18083  -15.70   &lt;2e-16 ***
## clump_thickness  0.51487    0.03535   14.56   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 466.66  on 697  degrees of freedom
## AIC: 470.66
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>This equation tells us that for every 1 unit increase in clump thickness (X), the z score increases by .514 on average.</p>
<p><span class="math display">\[
\begin{aligned}
\Phi^{-1}(\widehat{P(\text{malignant} | X)})&amp;=-2.839+0.514X\\
\widehat{P(\text{malignant} | X)}&amp;=\Phi(-2.839+0.514X)\\
\end{aligned}
\]</span></p>
<p>If a patient has a clump thickness of 10, our predicted probability of malignancy is <span class="math inline">\(\Phi(-2.839+0.514*10)=P(Z\le2.301)=.989\)</span></p>
<p>We can get everyone’s predicted z-score (and probability) in this fashion</p>
<pre class="r"><code>data$z_pred&lt;-predict(fit)
data$prob_pred&lt;-predict(fit,type = &quot;response&quot;)

head(data)</code></pre>
<pre><code>##   clump_thickness cell_uniformity marg_adhesion bland_chromatin   outcome
## 1               5               1             1               3    benign
## 2               5               4             4               3    benign
## 3               3               1             1               3    benign
## 4               6               8             8               3    benign
## 5               4               1             1               3    benign
## 6               8              10            10               9 malignant
##   y     z_pred  prob_pred
## 1 0 -0.2650294 0.39549341
## 2 0 -0.2650294 0.39549341
## 3 0 -1.2947609 0.09770136
## 4 0  0.2498364 0.59864305
## 5 0 -0.7798952 0.21772629
## 6 1  1.2795679 0.89965143</code></pre>
<pre class="r"><code>ggplot(data, aes(clump_thickness,y))+geom_jitter(width=.3,height=0)+
  stat_smooth(method=&quot;glm&quot;,method.args=list(family=binomial(link=&quot;probit&quot;)),se=F)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="cross-validation-of-probit-model" class="section level2">
<h2>Cross validation of probit model</h2>
<p>Let’s use cross-validation to see how this simple model performs when we train it to part of our full 699 dataset and test how well it predicts the rest. Let’s split the dataset into 10 roughly equal pieces, leave one piece (or “fold”) out (70 observations), fit the model to the remaining 9 folds (629 observations), see how well it classifies the test set by computing false-positive and false-negative rates, and repeat the process so that each fold gets a turn being the one left out (i.e., retrain the model on a new training set, test it on a new test set). Since we get predicted z-scores (or probabilities) from the model, we say the model predicts 1 if <span class="math inline">\(z\ge 0\ (p\ge.5)\)</span>, and 0 otherwise.</p>
<pre class="r"><code>set.seed(1234)

k=10

acc&lt;-fpr&lt;-fnr&lt;-NULL

data1&lt;-data[sample(nrow(data)),]
folds&lt;-cut(seq(1:nrow(data)),breaks=k,labels=F)

for(i in 1:k){
  train&lt;-data1[folds==i,]
  test&lt;-data1[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness,data=train,family=binomial(link=&quot;probit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-mean(truth==pred)
  fpr[i]&lt;-mean(pred[truth==0])
  fnr[i]&lt;-mean(1-pred[truth==1])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.860595</code></pre>
<pre><code>## [1] 0.3299324</code></pre>
<pre><code>## [1] 0.03901361</code></pre>
<p>The probit regression is classifying correctly about 85.49% of the time using just the clump thickness predictor variable. The false negative rate (the percentage of malignancies it identified as benign) was 29.71%; the false positive rate (the percentage of benign tumors it identified as malignant) was 6.45%.</p>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic regression</h2>
<p>There is another, more widely used link function for binary data, and that is the logit (or log odds) function. This approach is based on odds. Recall that the odds are calculated from a probability like so: <span class="math inline">\(odds(p)=\frac{p}{1-p}\)</span>. The nice thing about this is that we can take a variable <span class="math inline">\(p\)</span> from <span class="math inline">\([0,1]\)</span> and transform it. However, as <span class="math inline">\(p\)</span> approaches 0, the odds approach 0, and as <span class="math inline">\(p\)</span> approaches 1, the odds approach <span class="math inline">\(\infty\)</span>.</p>
<pre class="r"><code>curve(x/(1-x),from = 0, 1)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If we take the logarithm of the odds (use the natural log for convenience), you now have variable taking values from <span class="math inline">\([-\infty,\infty]\)</span>. This function, the logarithm of the odds, is called the <em>logit</em> function (it’s inverse is the <em>logistic</em> function): it is where logistic regression gets its name. Below is a graph of <span class="math inline">\(y=log(\frac{p}{1-p})\)</span> shown alongside the dotted normal quantile function (i.e., <span class="math inline">\(\Phi^{-1}(x)\)</span></p>
<pre class="r"><code>curve(log(x/(1-x)),0,1)
curve(qnorm(x,0,1),0,1,add=T,lty=2)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Very similar! Using this link function to convert our 0, 1 values to a continuous scale, we write the logistic regression equation</p>
<p><span class="math display">\[
log(\frac{p}{1-p})=\beta_0+\beta_1X
\]</span> Where <span class="math inline">\(p\)</span> is the <span class="math inline">\(p(Y=1 \mid X=x)\)</span>.</p>
<p>Let’s try using this link function in our regression of malignancy on clump thickness.</p>
<pre class="r"><code>fit&lt;-glm(y~clump_thickness,data=data,family=binomial(link=&quot;logit&quot;))
summary(fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_thickness, family = binomial(link = &quot;logit&quot;), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1986  -0.4261  -0.1704   0.1730   2.9118  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     -5.16017    0.37795  -13.65   &lt;2e-16 ***
## clump_thickness  0.93546    0.07377   12.68   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 464.05  on 697  degrees of freedom
## AIC: 468.05
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code>data$logit_pred&lt;-predict(fit)</code></pre>
<p>What does this tell us? Well first, clump thickness appears to be strongly and positively predictive of malignancy. The equation with the parameter estimates is <span class="math inline">\(log(\frac{p}{1-p})=-5.160+0.935X\)</span>. You can see that for every one-unit increase in X (clump thickness), the log-odds goes up by .935, which means that the odds change by a factor of <span class="math inline">\(e^{.935}=2.55\)</span> (i.e., they go up by 155%)! If you go up two clump-thickesses, we get a change in odds of <span class="math inline">\(e^{2\times.935}=e^{.935}e^{.935}=2.55\times 2.55\approx6.50\)</span>, which is <span class="math inline">\(2.55\times 2.55\)</span>.</p>
<p>How about getting a predicted probability for a person with a clump thickness of <span class="math inline">\(X\)</span>? For a person with <span class="math inline">\(X=10\)</span>, for example, we get a logit score of <span class="math inline">\(4.19\)</span> (by plugging into the equation above). But how do we invert the logit function to get the predicted probability out? Just use algebra: first exponentiate to remove the log, then multiply both sides by the denominator, then distribute, the get all terms with <em>p</em> on the same side, then factor out the <em>p</em>, then solve for it!</p>
<p><span class="math display">\[
\begin{aligned}
log(\frac{p}{1-p})&amp;=\beta_0+\beta_1X\\
\frac{p}{1-p}&amp;=e^{\beta_0+\beta_1X}\\
p&amp;=(1-p)e^{\beta_0+\beta_1X}\\
p&amp;=e^{\beta_0+\beta_1X}-pe^{\beta_0+\beta_1X}\\
p+pe^{\beta_0+\beta_1X}&amp;=e^{\beta_0+\beta_1X}\\
p(1+e^{\beta_0+\beta_1X})&amp;=e^{\beta_0+\beta_1X}\\
p&amp;=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}\\
\end{aligned}
\]</span></p>
<p>In general, <span class="math inline">\(p=\frac{odds}{1+odds}\)</span>. Note that if you multiply top and bottom of the RHS by <span class="math inline">\(e^{-\beta_0+\beta_1X}\)</span> (note the negative), you get the alternative formulation <span class="math inline">\(p=\frac{1}{1+e^{-\beta_0+\beta_1X}}\)</span>.</p>
<p>So we take what the regression evaluates to, raise <span class="math inline">\(e\approx2.71828\)</span> to that power, and divide the result by <span class="math inline">\(1 +\)</span> itself.</p>
<pre class="r"><code>exp(4.19)/(1+exp(4.19))</code></pre>
<pre><code>## [1] 0.9850797</code></pre>
<p>A 98.5% chance! A bit smaller than what the probit model predicted. We can also get this directly from R. Let’s ask R for the probability of malignancy at each level of clump thickness (i.e., from 1 to 10).</p>
<pre class="r"><code>predict(fit,newdata=data.frame(clump_thickness=1:10),type=&quot;response&quot;)</code></pre>
<pre><code>##          1          2          3          4          5          6 
## 0.01441866 0.03594186 0.08676502 0.19492345 0.38157438 0.61125443 
##          7          8          9         10 
## 0.80028036 0.91080524 0.96299397 0.98514461</code></pre>
<pre class="r"><code>data$logit&lt;-predict(fit)

ggplot(data, aes(clump_thickness,y))+geom_jitter(width=.3,height=0)+stat_smooth(method=&quot;glm&quot;,method.args=list(family=&quot;binomial&quot;),se=F)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>data$outcome&lt;-factor(data$outcome,levels=c(&quot;malignant&quot;,&quot;benign&quot;))
ggplot(data,aes(logit, fill=outcome))+geom_density(alpha=.3)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-11-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>Let’s see how it fares under cross-validation. We will start by using the exact same procedure as we did above.</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:k){
  train&lt;-data1[folds==i,]
  test&lt;-data1[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness,data=train,family=binomial(link=&quot;logit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-mean(truth==pred)
  fpr[i]&lt;-mean(pred[truth==0])
  fnr[i]&lt;-mean(1-pred[truth==1])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.860595</code></pre>
<pre><code>## [1] 0.3299324</code></pre>
<pre><code>## [1] 0.03901361</code></pre>
<p>The logistic regression accurately predicted out of sample exactly as accurately as the probit model; the false negative and false positive rates were the same as well!</p>
<p>I’m curious what we will see using another supervised classification approach: linear discriminant analysis (LDA). I won’t go into details here, but just for the sake of comparison let’s run it.</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:k){
  train&lt;-data1[folds==i,]
  test&lt;-data1[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-lda(y~clump_thickness,data=train)
 
  pred&lt;-predict(fit,test)
  tab&lt;-table(pred$class,test$outcome)
  
  acc[i]=sum(diag(tab))/sum(tab)
  fpr[i]&lt;-tab[2,1]/sum(tab[,1])
  fnr[i]&lt;-tab[1,2]/sum(tab[,2])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.860595</code></pre>
<pre><code>## [1] 0.3299324</code></pre>
<pre><code>## [1] 0.03901361</code></pre>
<p>Hmm, ever so slightly worse in the accuracy and false negative department, but slightly lower false positive rate. Still, very close!</p>
<div id="leave-one-out-cv" class="section level4">
<h4>Leave-One-Out CV</h4>
<p>Let’s take this opportunity to introduce another kind of cross-validation technique: Leave-one-out cross-validation (or LOOCV).</p>
<p>Just like it sounds we train the model on <span class="math inline">\(n-1\)</span> datapoints and use it to predict the omitted point. We do this for every observation in the dataset (i.e., each point gets a turn being left out and we try to predicted it from all of the rest). Let’s try it on the logistic regression.</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:length(data)){

  train&lt;-data[-i,]
  test&lt;-data[i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness,data=train,family=binomial(link=&quot;logit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-truth==pred
  if(truth==0 &amp; pred==1) fpr[i]&lt;-1
  if(truth==1 &amp; pred==0) fnr[i]&lt;-1
}

mean(acc)</code></pre>
<pre><code>## [1] 0.9</code></pre>
</div>
<div id="categorical-predictor-variables" class="section level3">
<h3>Categorical predictor variables</h3>
<p>Imagine that instead, clump size was a categorical variable with three levels, say <em>small</em>, <em>medium</em>, and <em>large</em>. I will artificially create this variable using terciles because the dataset doesn’t actually contain any categorical variables for us to play with (and the interpretation of the coefficients is slightly different).</p>
<pre class="r"><code>data$clump_cat&lt;-cut(data$clump_thickness,breaks=quantile(data$clump_thickness,0:3/3),include.lowest = T)
data$clump_cat&lt;-factor(data$clump_cat,labels=c(&quot;S&quot;,&quot;M&quot;,&quot;L&quot;))

fit1&lt;-glm(y~clump_cat, data=data, family=binomial(link=&quot;logit&quot;))
summary(fit1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_cat, family = binomial(link = &quot;logit&quot;), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0886  -0.3599  -0.3599   0.4895   2.3534  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -2.7045     0.2370 -11.413  &lt; 2e-16 ***
## clump_catM    1.7171     0.2833   6.062 1.34e-09 ***
## clump_catL    4.7660     0.3314  14.381  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 518.73  on 696  degrees of freedom
## AIC: 524.73
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>First of all, notice that the residual deviance and the AIC are <em>higher</em> now than the were before we artificially discretized this variable. This serves to illustrate the value of keeping your data numeric rather than using it to create groups: here, we are wasting good information and getting poorer model fit.</p>
<p>Notice that there are two effect estimates in the output: one for the medium category (M) and one for the large category (L). By default, regressions are dummy coded such that the estimates are compared to the reference category (the one not listed; in this case, the small category). Here, the coefficients are log odds ratios; if we exponentiate, we get odds ratios compared to the reference category. For example, <span class="math inline">\(e^{1.717}=5.57\)</span>, so the odds of malignancy are 5.57 times as high for those with medium clump sizes as those with small clump sizes, and this difference is significant. This difference is of course more pronounced for the large clump sizes: compared to those with small clump sizes, the odds of malignancy among those with large clump sizes are <span class="math inline">\(e^{4.766}=117.45\)</span> times as high!</p>
<p>To see that this is true, let’s calculate the odds ratios by hand.</p>
<pre class="r"><code>table(data$y,data$clump_cat)</code></pre>
<pre><code>##    
##       S   M   L
##   0 284 153  21
##   1  19  57 165</code></pre>
<p>The odds of cancer (y=1) if you have small clump thickness is <span class="math inline">\(19/284=.0669\)</span> and the odds for medium and large are <span class="math inline">\(57/153=.3725\)</span> and <span class="math inline">\(7.8571\)</span>. The odds ratio of M-to-S is <span class="math inline">\(\frac{.3725}{.0669}=5.57\)</span> and L-to-S is <span class="math inline">\(\frac{7.8571}{.0669}=117.45\)</span></p>
<p>We can even use these numbers to calculate the standard errors. The estimated standard error of a <span class="math inline">\(log\ OR\)</span> is just <span class="math inline">\(\sqrt{\frac 1a + \frac 1b +\frac 1c + \frac 1d}\)</span> where <span class="math inline">\(a, b, c,\)</span> and <span class="math inline">\(d\)</span> are the counts with/without cancer in each of the two conditions being compared. For example, comparing M-to-S, <span class="math inline">\(SE=\sqrt{\frac{1}{284}+\frac{1}{19}+\frac{1}{153}+\frac{1}{57}}=.2833\)</span>, which matches the output above. You can use this to perform hypothesis tests using the normal distribution.</p>
</div>
<div id="multiple-predictor-variables" class="section level3">
<h3>Multiple predictor variables</h3>
<p>Let’s add more variables to our model to try to improve prediction. Let’s try adding bland chromatin and marginal adhesion. Our model becomes</p>
<pre class="r"><code>fit2&lt;-glm(y~clump_thickness+marg_adhesion+bland_chromatin, data=data, family=&quot;binomial&quot;)
summary(fit2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ clump_thickness + marg_adhesion + bland_chromatin, 
##     family = &quot;binomial&quot;, data = data)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -3.11645  -0.16339  -0.08323   0.03270   2.94043  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -9.0911     0.8262 -11.003  &lt; 2e-16 ***
## clump_thickness   0.6137     0.1069   5.744 9.26e-09 ***
## marg_adhesion     0.8457     0.1257   6.729 1.71e-11 ***
## bland_chromatin   0.7404     0.1267   5.845 5.07e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 900.53  on 698  degrees of freedom
## Residual deviance: 165.23  on 695  degrees of freedom
## AIC: 173.23
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>Notice that you can be lazy with your <code>family=</code> specification for logistic regression. Wow, so all of these variables are significant. Notice that the AIC is 173.23, compared to our previous model’s 468.05! Lower AIC/deviance is better. We could formally test the additional explanatory value of the new variables by performing an analysis of deviance with the null hypothesis that the more basic model is the true model.</p>
<p>Without going into too much detail, the way this works under the hood is that we compute the likelihood of the data given the saturated model (i.e., estimating a parameter for every data point) and compare it to the likelihood of the data under the basic (null) model (i.e., estimating just a single intercept parameter). We take the ratio of these likelihoods (full to null), take the log of these ratios, and multiply them each by <span class="math inline">\(-2\)</span>, resulting in something called the <code>null deviance</code> (the smaller, the better the null model explains the data). We also compare the saturated model to each of our actual model(s) in the same, resulting in <code>residual deviance</code>: the smaller it is, the better the proposed model explains the data. We can compute residual deviances for two models and compare them: For example, the residual deviance of the first model is 464.05 (from the output below), while the deviance for the model with three predictors is 165.23 (smaller: a good thing!). The difference in residual deviances follows a chi-squared distribution with DF equal to the number of additional parameters in the larger model. That is, we compute</p>
<pre class="r"><code>fit&lt;-glm(y~clump_thickness,data=data,family=binomial(link=&quot;logit&quot;))
anova(fit,fit2,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: y ~ clump_thickness
## Model 2: y ~ clump_thickness + marg_adhesion + bland_chromatin
##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
## 1       697     464.05                          
## 2       695     165.23  2   298.82 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Yes, it appears that the full model is better! We can get a goodness-of-fit estimate analogous to <span class="math inline">\(R^2\)</span> by seeing what proportion of the total deviance in the null model can be accounted for by the full model. You just put the difference in deviances in the numerator and the null deviance in the denominator, like this.</p>
<pre class="r"><code>(fit2$null.deviance-fit2$deviance)/fit2$null.deviance</code></pre>
<pre><code>## [1] 0.8165162</code></pre>
<p>Wow, quite high! This quantity is known as the Hosmer and Lemeshow pseudo <span class="math inline">\(R^2\)</span>; it is analogous to traditional <span class="math inline">\(R^2\)</span>, but there are alternatives (e.g., Nagelkerke).</p>
<p>Here is the fitted model with the parameter estimates:</p>
<p><span class="math display">\[
log(\frac{p}{1-p})=-9.09+0.61Clump+0.85MargAd+0.74BlandChrom
\]</span></p>
<p>Holding marginal adhesion and bland chromatin constant, we see an <span class="math inline">\(e^{.61}=1.84\)</span> change in the odds of developing breast cancer for every one-unit increase in clump thickness. Specifically, we see an 84% increase in the odds of cancer for each increase in clump thickness.</p>
<p>How much better does this perform? Let’s rerun our cross-validation. First, 10-fold:</p>
<pre class="r"><code>acc&lt;-fpr&lt;-fnr&lt;-NULL

for(i in 1:k){
  train&lt;-data[folds==i,]
  test&lt;-data[folds!=i,]
  truth&lt;-test$y
  
  fit&lt;-glm(y~clump_thickness+marg_adhesion+bland_chromatin,data=train,family=binomial(link=&quot;logit&quot;))
  
  pred&lt;-ifelse(predict(fit,newdata = test)&gt;0,1,0)
  
  acc[i]&lt;-mean(truth==pred)
  fpr[i]&lt;-mean(pred[truth==0])
  fnr[i]&lt;-mean(1-pred[truth==1])
}

mean(acc); mean(fnr); mean(fpr)</code></pre>
<pre><code>## [1] 0.9434098</code></pre>
<pre><code>## [1] 0.09410461</code></pre>
<pre><code>## [1] 0.03666142</code></pre>
<p>Wow, very good prediction! We are accurately predicting out-of-sample 94.3% of the time, with a 9.4% false negative rate and a 3.7% false positive rate.</p>
</div>
</div>
<div id="sensitivity-specificity-and-roc-curves" class="section level2">
<h2>Sensitivity, Specificity, and ROC Curves</h2>
<p>We could try to visualize this logistic regression, but we would need an axis for each additional predictor variable. We could just plot one predictor at a time on the x-axis. Instead, let’s use PCA to find the best linear combination of our three predictors, resulting in a single variable that summarizes as much of the variability in the set of three as possible. We can this use this as a predictor (note this is really just for visualization purposes; just pretend we ran the logistic regression with a single variable called <code>predictor</code>).</p>
<pre class="r"><code>pca1&lt;-princomp(data[c(&#39;clump_thickness&#39;,&#39;marg_adhesion&#39;,&#39;bland_chromatin&#39;)])
data$predictor&lt;-pca1$scores[,1]

fit&lt;-glm(y~predictor,data=data,family=&quot;binomial&quot;)
data$prob_pred&lt;-predict(fit,type=&quot;response&quot;)

ggplot(data, aes(predictor,prob_pred))+geom_point(aes(color=outcome),alpha=.5,size=3)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-21-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If we give anyone above <span class="math inline">\(p(y=1|x)=.5\)</span> a prediction of malignancy, and anyone below a prediction of benign, we can calculate the true positive rate (Sensitivity), the true negative rate (Specificity), and the corresponding false positive/negative rates.</p>
<pre class="r"><code>ggplot(data, aes(predictor,prob_pred))+geom_point(aes(color=outcome),alpha=.5,size=3)+geom_rug(aes(color=outcome),sides=&quot;right&quot;)+geom_hline(yintercept=.5)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Any blue dots above the line are false positives: any reds below the line are false negatives. For example, the Sensitivity (or true positive rate, TPR) is the number of red dots above the line (which are predicted positive, since <span class="math inline">\(p(y=1)&gt;.5\)</span>) out of the total number of red dots.</p>
<pre class="r"><code>sens&lt;-sum(data[data$y==1,]$prob_pred&gt;=.5)/sum(data$y)
sens</code></pre>
<pre><code>## [1] 0.9419087</code></pre>
<p>Think of it as the probability of detecting the disease, given that it exists (i.e., probability of detection). It is the percentage of <em>actual positives</em> (e.g., people with the disease) that are correctly flagged as positive. Thus, Sensitivity is the degree to which actual positives are not overlooked: A test that is highly sensitive (e.g., <span class="math inline">\(&gt;.90\)</span>) rarely overlooks a thing it is looking for. The higher the Sensitivity, the lower the false negative rate (type II error).</p>
<p>Compare this to the Specificity (true negative rate, TNR), the proportion of actual negatives (e.g., healthy people) that are correctly flagged as negative. It is the probability of detecting the absence of the disease, given that it does not exist! Thus, it indexes the avoidance of false positives: the higher the Specificity, the lower the false positive rate (1-Specificity, or type I error). A test that is highly specific rarely mistakes something else for the thing it is looking for.</p>
<p>Here, Specificity is the percentage of blue dots that fall below the horizontal cut-off (i.e., of those with <span class="math inline">\(p(y=1)&lt;.5\)</span>, the percentage that are actually <span class="math inline">\(y=0\)</span>).</p>
<pre class="r"><code>spec&lt;-sum(data[data$y==0,]$prob_pred&lt;.5)/sum(data$y==0)
spec</code></pre>
<pre><code>## [1] 0.9694323</code></pre>
<p>You can put confidence intervals on these estimates (e.g., using large-sample approximations for binomial)</p>
<pre class="r"><code>#sensitivity
sens+c(-1.96,1.96)*sqrt(sens*(1-sens)/sum(data$y))</code></pre>
<pre><code>## [1] 0.9123757 0.9714417</code></pre>
<pre class="r"><code>#specificity
spec+c(-1.96,1.96)*sqrt(spec*(1-spec)/sum(data$y==0))</code></pre>
<pre><code>## [1] 0.9536666 0.9851980</code></pre>
<p>This quickly gets confusing, so it is a good idea to make a table like this (indeed, it is referred to as a confusion matrix):</p>
<pre class="r"><code>data$test&lt;-as.factor(ifelse(predict(fit)&gt;0,&quot;positive&quot;,&quot;negative&quot;))
data$disease&lt;-data$outcome

with(data, table(test,disease))%&gt;%addmargins</code></pre>
<pre><code>##           disease
## test       malignant benign Sum
##   negative        14    444 458
##   positive       227     14 241
##   Sum            241    458 699</code></pre>
<p>As before, Sensitivity is the proportion of those with the desease who test positive, <span class="math inline">\(p(+|malignant)=\frac{227}{241}=.942\)</span>, while Specificity is the proportion of those who do <em>not</em> have the disease who test negative, <span class="math inline">\(p(-|benign)=\frac{444}{458}=.969\)</span>.</p>
<p>Here is a helpful plot of the proportions testing positive/negative in each disease category, to help keep things straight visually:</p>
<pre class="r"><code>library(magrittr)

plotdat&lt;-data.frame(rbind(
  cbind(density(data$logit_pred[data$outcome==&quot;malignant&quot;])%$%data.frame(x=x,y=y),disease=&quot;malignant&quot;),
  cbind(density(data$logit_pred[data$outcome==&quot;benign&quot;])%$%data.frame(x=x,y=y),disease=&quot;benign&quot;)))%&gt;%
  mutate(test=ifelse(x&gt;0,&#39;positive&#39;,&#39;negative&#39;),`disease.test`=interaction(disease,test))

plotdat%&gt;%ggplot(aes(x,y,fill=`disease.test`))+geom_area(alpha=.6,color=1)+geom_vline(xintercept=0,size=1)+
    theme(axis.title.y=element_blank(),legend.position=c(.87,.8))+xlab(&quot;Predicted logit&quot;)+
    geom_text(x=-3,y=.1,label=&quot;TN&quot;)+
    geom_text(x=-1,y=.015,label=&quot;FN&quot;)+
    geom_text(x=.5,y=.015,label=&quot;FP&quot;)+
    geom_text(x=3,y=.05,label=&quot;TP&quot;)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-27-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This shows the density of benign (hump on left side: green + purple) and the density of malignant (right side: blue + red): both humps add up to 100%.</p>
<ul>
<li>green represents the proportion of benign that got a negative test result (True Negative rate: Specificity)</li>
<li>purple represents the proportion of benign that got a positive test result (False Positive rate)</li>
<li>blue represents the proportion of malignant that got a positive test results (True Positive rate: Sensitivity)</li>
<li>red represents the proportion of malignant that got a negative test result (False Negative rate)</li>
</ul>
<p>Just like in traditional null hypothesis testing, there is a trade-off between type I and type II error (and so there is a trade-off between sensitivity and specificity). The stricter your test, the fewer false positives it will make (and thus the higher the Specificity), but the more false negatives. The laxer your test, the fewer false negatives it will make (and thus the higher the Sensitivity or Power), but it will allow in more false positives.</p>
<p>Notice that a lot depends on the cut-off! For example, if we set the cut-off at 10% (thus making it a laxer criterion), so that a predicted probability of 10% or more means we predict malignant (i.e., 1), otherwise benign (0), we get</p>
<pre class="r"><code>ggplot(data, aes(predictor,prob_pred))+geom_point(aes(color=outcome),alpha=.5,size=3)+geom_rug(aes(color=outcome),sides=&quot;right&quot;)+geom_hline(yintercept=.1)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-28-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>#sensitivity
sum(data[data$y==1,]$prob_pred&gt;.1)/sum(data$y)</code></pre>
<pre><code>## [1] 0.9875519</code></pre>
<pre class="r"><code>#specificity
sum(data[data$y==0,]$prob_pred&lt;.1)/sum(data$y==0)</code></pre>
<pre><code>## [1] 0.8973799</code></pre>
<p>By lowering our cut-off we have improved sensitivity (and lowered the false negative rate: very few red dots are below the line), but we have worsened our specificity (we have increased the false positive rate: we are more likely to say a patient doesn’t have cancer when they do; there is a greater proportion of blue dots above the line). This tradeoff always occurs:</p>
<pre class="r"><code>true_pos&lt;-function(x,data=data) sum(data[data$y==1,]$prob_pred&gt;x)/sum(data$y)
false_pos&lt;-function(x,data=data) sum(data[data$y==0,]$prob_pred&gt;x)/sum(data$y==0)

TPR&lt;-sapply(seq(0,1,.01),true_pos,data)
FPR&lt;-sapply(seq(0,1,.01),false_pos,data)

ROC1&lt;-data.frame(TPR,FPR,cutoff=seq(0,1,.01))

library(tidyr)
ROC1%&gt;%gather(key,value,-cutoff)%&gt;%ggplot(aes(cutoff,value,color=key))+geom_path()</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>What happens if we plot the true positive rate (Sensitivity) versus the false positive rate (1-Specificity) for all possible cut-offs?</p>
<pre class="r"><code>ROC1%&gt;%ggplot(aes(FPR,TPR))+geom_path(size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1),lty=2)+scale_x_continuous(limits = c(0,1))</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This is an ROC (Receiver Operating Characteristic) curve. If we were predicting perfectly with no mistakes, the TPR wouldbe 1 while the FPR would be 0 regardless of the cutoff and thus the curve would extend all the way to the upper right corner (100% sensitivity and 100% specificity). The dotted line shows the worst-case scenario (the FPR equal to the TPR, so our test is just as good as randomly assigning people positive and negative results), while the thick curve is our emprical ROC (pretty good!).</p>
<p>You can use this plot to find the optimal cutoff. Usually, this is defined as <span class="math inline">\(max(TPR-FPR)\)</span> but there are lots of alternatives. We can find out what this is quickly</p>
<pre class="r"><code>ROC1[which.max(ROC1$TPR-ROC1$FPR),]</code></pre>
<pre><code>##          TPR        FPR cutoff
## 18 0.9792531 0.05021834   0.17</code></pre>
<div id="auc" class="section level3">
<h3>AUC</h3>
<p>The area under the curve (AUC) quantifies how well we are predicting overall and it is easy to calculate: just connect every two adjacent points with a line and then drop a vertical line from each point to the x-axis to form a trapezoid. Then just calculate the area of each trapezoid, <span class="math inline">\((x_1-x_0)(y_0+y_1)/2\)</span> and add them all together.</p>
<pre class="r"><code>#order dataset from least to greatest
ROC1&lt;-ROC1[order(-ROC1$cutoff),]

widths&lt;-diff(ROC1$FPR)
heights&lt;-vector()

for(i in 1:100) heights[i]&lt;-ROC1$TPR[i]+ROC1$TPR[i+1]

AUC&lt;-sum(heights*widths/2)
AUC%&gt;%round(4)</code></pre>
<pre><code>## [1] 0.9903</code></pre>
<p>You can interpret the AUC as the probability that a randomly selected person <em>with cancer</em> has a higher predicted probability of having cancer than a randomly selected person <em>without cancer</em>.</p>
<p>By way of an important, interesting aside, you might be thinking that this sounds like the usual interpretation for a Wilcoxon test statistic W (the non-parametric alternative to Student’s t test, sometimes called Mann-Whitney U). To refresh, this is a nonparametric statistic used to test whether the level of some numeric variable in one population tends to be greater/less than in another population without making any assumptions about how the variable is distributed, with the null hypothesis that the numeric variable is a useful discriminator: that a value of the numeric variable is just as likely to be from an individual in the first population as from an individual in the second. To calculate this statistic, you compare every observation in your first sample to every observation in your second sample, recording 1 if the first is bigger, 0 if the second is bigger, and 0.5 if they are equal. Then you average these across all possible combinations to get your U statistic.</p>
<p>Let’s calculate this statistic for a binary prediction situation: We will compute how well the scores (i.e., the predicted probabilities of cancer) discriminate between those with cancer (sample 1) and those without (sample 2). Our malignant sample consists of 241 individuals and our benign sample consists of 458 individuals. Comparing each individual’s predicted outcome in one sample to each individual’s predicted outcome in the other means we make <span class="math inline">\(241\times 458=110378\)</span> comparisons. Using the base R function <code>expand.grid()</code> we generate a row for every possible comparison. Then we check if predicted probability of disease is higher for malignant than for benign for every row and sum up the yesses.</p>
<pre class="r"><code>pos&lt;-data[which(data$disease==&quot;malignant&quot;),]$prob_pred
neg&lt;-data[which(data$disease==&quot;benign&quot;),]$prob_pred

n1&lt;-length(pos)
n2&lt;-length(neg)

expand.grid(pos=pos,neg=neg)%&gt;%summarize(W=sum(ifelse(pos&gt;neg,1,ifelse(pos==neg,.5,0))),n=n(),W/n)</code></pre>
<pre><code>##          W      n       W/n
## 1 109309.5 110378 0.9903196</code></pre>
<pre class="r"><code>wilcox.test(pos,neg)</code></pre>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  pos and neg
## W = 109310, p-value &lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<pre class="r"><code>wilcox.test(pos,neg)$stat/(n1*n2)</code></pre>
<pre><code>##         W 
## 0.9903196</code></pre>
<p>The W (or U) statistic itself counts the number of times that the predicted probability for malignant individuals is higher than for benign individuals. Here we can see that the proportion of times that malignant individuals have a higher predicted probability than beningn individuals is .9903 (for all possible comparisons). This quantity is the same as the AUC!</p>
<p>The nice thing is, we can calculate a standard error for the AUC using the large-sample normal aproximation</p>
<pre class="r"><code>#see Fogarty, Baker, and Hudson 2005 for details
Dp&lt;-(n1-1)*((AUC/(2-AUC))-AUC^2)
Dn&lt;-(n2-1)*((2*AUC^2)/(1+AUC)-AUC^2)

sqrt((AUC*(1-AUC)+Dp+Dn)/(n1*n2))</code></pre>
<pre><code>## [1] 0.00448658</code></pre>
<p>Several packages have out-of-the-box ROC curve plotters and AUC calculators. For example, here’s the package <code>plotROC</code>’s <code>geom_roc()</code> function that plays nicely with ggplot2, and it’s <code>calc_auc()</code> function, which you can use on a plot object that uses <code>geom_roc()</code></p>
<pre class="r"><code>library(plotROC)
ROCplot&lt;-ggplot(data,aes(d=disease,m=predictor))+geom_roc(n.cuts=0)+coord_fixed()
ROCplot</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.9903196</code></pre>
<p>Just for comparison, here is the ROC curve from our logistic regression using only clump thickness as a predictor</p>
<pre class="r"><code>library(plotROC)
ROCplot&lt;-ggplot(data,aes(d=disease,m=clump_thickness))+geom_roc(n.cuts=0)+coord_fixed()
ROCplot</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-36-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>calc_auc(ROCplot)</code></pre>
<pre><code>##   PANEL group       AUC
## 1     1    -1 0.9098416</code></pre>
<p>Here is another package called <code>auctestr</code> which gives you standard errors as well (using the large-sample approximation above)</p>
<pre class="r"><code>#install.packages(&quot;auctestr&quot;)
library(auctestr)
AUC&lt;-.9903
se_auc(AUC,n1,n2)</code></pre>
<pre><code>## [1] 0.004480684</code></pre>
<pre class="r"><code>sqrt((AUC*(1-AUC)+(n1-1)*((AUC/(2-AUC))-AUC^2)+(n2-1)*((2*AUC^2)/(1+AUC)-AUC^2))/(n1*n2))</code></pre>
<pre><code>## [1] 0.004480684</code></pre>
<p>The package `pROC gives bootstrap SE estimates too</p>
<pre class="r"><code>#install.packages(&quot;pROC&quot;)
library(pROC)

data$test_num&lt;-data$test%&gt;%as.numeric()-1

roc(response=data$y,predictor=data$prob_pred,print.auc=T,ci=T,plot=T)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## 
## Call:
## roc.default(response = data$y, predictor = data$prob_pred, ci = T,     plot = T, print.auc = T)
## 
## Data: data$prob_pred in 458 controls (data$y 0) &lt; 241 cases (data$y 1).
## Area under the curve: 0.9903
## 95% CI: 0.9848-0.9959 (DeLong)</code></pre>
<pre class="r"><code>ci.auc(response=data$y,predictor=data$prob_pred,method=&quot;bootstrap&quot;)</code></pre>
<pre><code>## 95% CI: 0.9842-0.9953 (2000 stratified bootstrap replicates)</code></pre>
<p>I’m curious if we can replicate this bootstrap ourselves by resampling cases with replacement. Let’s try it!</p>
<pre class="r"><code>AUCboots&lt;-replicate(1000, {
  samp&lt;-data[sample(1:dim(data)[1],replace=T),]
  fit&lt;-glm(y~predictor,data=samp,family=binomial)
  samp$prob_pred&lt;-predict(fit,type=&quot;response&quot;)
  
  TPR&lt;-sapply(seq(0,1,.01),true_pos,samp)
  FPR&lt;-sapply(seq(0,1,.01),false_pos,samp)
  ROC1&lt;-data.frame(TPR,FPR,cutoff=seq(0,1,.01)) 

  #compute AUC
  ROC1&lt;-ROC1[order(-ROC1$cutoff),]
  widths&lt;-diff(ROC1$FPR)
  heights&lt;-vector()
  for(i in 1:100) heights[i]&lt;-ROC1$TPR[i]+ROC1$TPR[i+1]
  sum(heights*widths/2)%&gt;%round(4)
})

mean(AUCboots)</code></pre>
<pre><code>## [1] 0.9894112</code></pre>
<pre class="r"><code>sd(AUCboots)</code></pre>
<pre><code>## [1] 0.002633121</code></pre>
<pre class="r"><code>mean(AUCboots)+c(-1,1)*sd(AUCboots)</code></pre>
<pre><code>## [1] 0.9867781 0.9920443</code></pre>
</div>
<div id="some-other-metrics" class="section level3">
<h3>Some other metrics</h3>
<div id="positive-predictive-value-ppv-or-pv-aka-precision" class="section level4">
<h4>Positive predictive value (PPV or PV+, aka Precision)</h4>
<p>Another metric that is commonly calculated is the PPV, or positive predictive value. This is just what it sounds like: the predictive value of a positive test result, or the probability of actually having the disease given a positive test result. Therefore, it is simply <span class="math inline">\(p(disease \mid positive)=\frac{p(disease\ \&amp;\ positive)}{p(positive)}=\frac{TP}{TP+FP}\)</span>. In this case (from the confusion matrix), there are 227 TPs and 14 FPs, for a PPV of 94.19. Thus, if you get a positive test result, there is a 94% chance that you actually have the disease. In this case, the Precision is the same as the Sesitivity because FP=FN.</p>
<pre class="r"><code>with(data, table(test,disease))%&gt;%addmargins</code></pre>
<pre><code>##           disease
## test       malignant benign Sum
##   negative        14    444 458
##   positive       227     14 241
##   Sum            241    458 699</code></pre>
</div>
<div id="positivenegative-likelihood-ratios" class="section level4">
<h4>Positive/negative likelihood ratios</h4>
<p>These are simply TP/FP for the positive LR and FN/TN for negative LR. So in this case, <span class="math inline">\(LR_+=\frac{227}{14}=16.214\)</span> and $LR_-==.0315. Thus, true positives are 16.2 times as likely as false positives and false negatives are .0315 times as likely as true negatives.</p>
<p>The ratio of LR+ and LR- is the <strong>diagnostic odds ratio</strong>: it is the ratio of the odds of testing positive if you have the disease relative to the odds of testing positive if you do not have the disease</p>
<p><span class="math display">\[
DOR=\frac{LR_+}{LR_-}=\frac{TP/FP}{FN/TN}=\frac{TP/FN}{FP/TN}=\frac{odds(+|disease)}{odds(+|no\ disease)}=\frac{227/14}{14/444}=514.225
\]</span> So the odds of testing positive are 514 times greater if you have the disease than if you do not!</p>
<p>It can be shown that the logarithm of an odds ratio is approximately normal with a standard error of <span class="math inline">\(SE(ln OR)=\sqrt{\frac{1}{TP}+\frac{1}{FP}+\frac{1}{TN}+\frac{1}{FN}}\)</span>.</p>
<p>So we have <span class="math inline">\(\sqrt{\frac{1}{227}+\frac{1}{14}+\frac{1}{14}+\frac{1}{444}}=.3867\)</span>, so our 95% CI for <span class="math inline">\(lnOR\)</span> is</p>
<pre class="r"><code>log(514.225)+(c(-1.96,1.96)*(.3867))</code></pre>
<pre><code>## [1] 5.484729 7.000593</code></pre>
<p>And exponentiating to get back to the original scale,</p>
<pre class="r"><code>exp(c(5.485,7.001))</code></pre>
<pre><code>## [1]  241.0489 1097.7303</code></pre>
</div>
</div>
</div>
<div id="poisson-regression" class="section level2">
<h2>Poisson Regression</h2>
<p>In my Biostatistics course, students have to conduct their own research project, which requires data. Many students choose to collect their own data and often their response variable is count-based. For example, they might ask how many times per month a student eats at a restaurant. However, in this course we only cover up to multiple regression and we do not broach the topic of link functions at all. Just like the binary cases above, we can’t very well model count data using a linear model, since a line has negative y-values for certain x values. Counts have a minimum value of 0!</p>
<p>Just like a linear regression models the average outcome as <span class="math inline">\(\mu_i=\beta_0+\beta_1x_i\)</span>, we could try to model the average of the count data in the same way, <span class="math inline">\(\lambda_i=\beta_0+\beta_1x_i\)</span>, where <span class="math inline">\(\lambda\)</span> is the average (and variance) of a Poisson distribution, often called the “rate”. But this is linear, so to avoid the problem of negative values, we use a link function that maps a domain of <span class="math inline">\((-\infty,\infty)\)</span> to a range of <span class="math inline">\([0,\infty)\)</span>, the natural log.</p>
<p><span class="math display">\[
log(\lambda_i)=\beta_0+\beta_1x_i
\]</span></p>
<p>We are also assuming that our observed counts <span class="math inline">\(Y_i\)</span> come from a Poisson distribution with <span class="math inline">\(\lambda=\lambda_i\)</span> for a given <span class="math inline">\(x_i\)</span>. Note that there is no separate error term: this is because <span class="math inline">\(\lambda\)</span> determines both the mean and the variance of our response variable.</p>
<p>Below we have the famous Prussian horse kicks dataset, compiled by Ladislaus Bortkiewicz, the father of Poisson applications (see his book Das Gezets der kleinen Zahlen, or <em>the Law of Small Numbers</em>). This dataset gives the number of personnel in the Prussian army killed by horse kicks per year from 1875 to 1895 from 10 different corps. Each corps is given its own column, so we will need to rearrange a bit.</p>
<pre class="r"><code>prussian&lt;-read.table(&quot;http://www.randomservices.org/random/data/HorseKicks.txt&quot;,header = T)

prussian&lt;-gather(prussian,Corps,Kicks,-Year)
prussian$Corps&lt;-as.factor(prussian$Corps)
prussian%&gt;%head()</code></pre>
<pre><code>##   Year Corps Kicks
## 1 1875    GC     0
## 2 1876    GC     2
## 3 1877    GC     2
## 4 1878    GC     1
## 5 1879    GC     0
## 6 1880    GC     0</code></pre>
<p>Good! Notice that most of the Kicks per year are close to zero. For a Poisson distribution, recall that the pmf is given as</p>
<p><span class="math display">\[
P(y|\lambda)=\frac{\lambda^ye^{-\lambda}}{y!}
\]</span> And <span class="math inline">\(E(y)=\lambda\)</span> and <span class="math inline">\(Var(Y)=\lambda\)</span>. Thus, using <code>mean(y)</code> is a good estimate for <span class="math inline">\(\lambda\)</span> (indeed, it can be shown that it is the maximum likelihood estimate). Let’s quickly do this. There are 280 observations, and assuming they arise from a poisson distribution, we have the following likelihood function, which we take the log of for convenience, then take the deriviative with respect to <span class="math inline">\(\lambda\)</span> so we can solve for the critical point (in this case, the maximum of the likelihood function).</p>
<p><span class="math display">\[
\begin{aligned}
L(\lambda,y)&amp;=\prod_{i=1}^{280}\frac{\lambda^{y_i}e^{-\lambda}}{y_i!}\\
log(L(\lambda,y))&amp;=-280\lambda+log(\lambda)\sum_{i=1}^{280} y-\sum log(y!)\\
\frac{d}{d\lambda}log(L(\lambda,y))&amp;=-280+\frac1\lambda\sum_{i=1}^{280}y \phantom{xxxxx}\text{set equal to zero to find maximum}\\
0&amp;:=-280+\frac1\lambda\sum_{i=1}^{280}y\\
\rightarrow \lambda&amp;=\sum_{i=1}^{280}y/280\\
\end{aligned}
\]</span> Thus, the maximum probability occurs when we set <span class="math inline">\(\lambda\)</span> to the mean of the data <span class="math inline">\(y\)</span>, which is exactly what we see.</p>
<pre class="r"><code>loglik&lt;-function(lambda){sum(log(lambda^prussian$Kicks*exp(-lambda)/factorial(prussian$Kicks)))}
loglik&lt;-sapply(seq(.1,5,.1),FUN=loglik)

plot(seq(.1,5,.1),loglik,type=&quot;l&quot;)
abline(v=.7)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-44-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>mean(prussian$Kicks)</code></pre>
<pre><code>## [1] 0.7</code></pre>
<pre class="r"><code>prussian%&gt;%ggplot(aes(Kicks))+geom_histogram(aes(y=..density..),bins=5)+stat_function(geom=&quot;point&quot;,fun=dpois,n=5,args=list(lambda=.7),color=&quot;red&quot;,size=3)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-45-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>These data clearly follow a Poisson distribution (as do most low-count data). Let’s test the null hypothesis that the observed counts are not different from the counts expected under a poisson distribution with a <span class="math inline">\(\lambda=.7\)</span>. We can get our observed counts by simply tabling our response variable. The expected couns of 0, 1, 2, 3, and 4 in a sample of 280 can be calculated using the poisson pmf <span class="math inline">\(p(y|\lambda=.7)=\frac{.7^ye^{-.7}}{y!}\)</span> and the total sample size.</p>
<pre class="r"><code>obs&lt;-table(prussian$Kicks)
exp&lt;-dpois(0:4,.7)*280
cbind(obs,exp)</code></pre>
<pre><code>##   obs        exp
## 0 144 139.043885
## 1  91  97.330720
## 2  32  34.065752
## 3  11   7.948675
## 4   2   1.391018</code></pre>
<p>Now we just have to calculate a chi-squared test statistic quantifying how far these counts are from each other and see if it is large enough to warrant rejecting the null hypothesis that they were generated with this probabilities.</p>
<pre class="r"><code>chisq&lt;-sum((obs-exp)^2/exp)
pchisq(chisq,df = 3,lower.tail = F)</code></pre>
<pre><code>## [1] 0.5415359</code></pre>
<p>Fail to reject: we don’t have evidence that the data differ from this expected distribution.</p>
<div id="poisson-regression-1" class="section level4">
<h4>Poisson Regression</h4>
<p>For Poisson random variables, the variance is equal to the mean. Let’s look within groups of years and see if this holds</p>
<pre class="r"><code>prussian$Group&lt;-cut(prussian$Year,breaks = seq(1875,1895,5),include.lowest = T)
prussian%&gt;%ggplot(aes(Kicks))+geom_histogram(bins=5)+facet_wrap(~Group)</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>prussian%&gt;%group_by(Group)%&gt;%summarize(mean(Kicks),var(Kicks),n())</code></pre>
<pre><code>## # A tibble: 4 x 4
##   Group       `mean(Kicks)` `var(Kicks)` `n()`
##   &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;
## 1 [1875,1880]         0.619        0.769    84
## 2 (1880,1885]         0.643        0.784    70
## 3 (1885,1890]         0.857        0.675    70
## 4 (1890,1895]         0.696        0.833    56</code></pre>
<p>This looks OK. Another assumption is the <span class="math inline">\(log(\lambda_i)\)</span> is a linear function of Year, since we are modeling it as such. Let’s look at the average number of kicks per year and see if it is a linear function of year.</p>
<pre class="r"><code>prussian%&gt;%group_by(Year)%&gt;%summarize(Kicks=mean(Kicks))%&gt;%ggplot(aes(Year,log(Kicks),group))+geom_point()+geom_smooth()</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-49-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hmm, it doesn’t look especially linear. Perhaps including a quadratic term is appropriate. Still, the assumption is about the true means, not the empirical means. Let’s go ahead and run our regressions.</p>
<p>Now, a Poisson regression differs from a regular regression in two ways: the errors should follow a Poisson distribution, and the link function is logarithmic. Let’s examine the effect of Year just to use a continuous predictor.</p>
<pre class="r"><code>fit3&lt;-glm(Kicks~Year,data=prussian,family=&quot;poisson&quot;)
summary(fit3)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year, family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2897  -1.1742  -1.0792   0.3997   2.8187  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -35.72380   23.43349  -1.524    0.127
## Year          0.01876    0.01243   1.510    0.131
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 320.94  on 278  degrees of freedom
## AIC: 630.02
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Because of the (log) link function, the coefficient estimate will need to be transformed via exponentiation. Here, <span class="math inline">\(e^{.019}=1.02\)</span>, so every additional year increases the expected number of horse-kick deaths by a <em>factor of 1.02</em>, or a yearly increase of 2%, which is not significant. Note that the increase is multiplicative rather than additive as in linear regression.</p>
<p>To get our 95% CI, we take our estimate <span class="math inline">\(\pm 1.96 SE\)</span> and then exponentiate. So here we have <span class="math inline">\(.01875 \pm 1.96*.1243=[-.0055,.0432]\)</span>, and we exponentiate each limit, <span class="math inline">\([.9945, 1.0441]\)</span>. It contains 1, so no effect!</p>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-51-1.png" width="672" style="display: block; margin: auto;" /><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-51-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can see the same thing by comparing this model to the null model and seeing if we significantly decrease deviance (we get the same p-value).</p>
<pre class="r"><code>nullfit&lt;-glm(Kicks~1, data=prussian, family=&quot;poisson&quot;)
anova(nullfit,fit3,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Kicks ~ 1
## Model 2: Kicks ~ Year
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1       279     323.23                     
## 2       278     320.94  1   2.2866   0.1305</code></pre>
<p>Because of our quadratic looking plot above, let’s add a term that let’s Year affect Kicks quadratically</p>
<pre class="r"><code>fit3quad&lt;-glm(Kicks~Year+I(Year^2),data=prussian,family=&quot;poisson&quot;)
summary(fit3quad)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year + I(Year^2), family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3175  -1.2046  -0.8679   0.3841   2.7584  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.376e+04  9.246e+03  -2.570   0.0102 *
## Year         2.520e+01  9.812e+00   2.568   0.0102 *
## I(Year^2)   -6.679e-03  2.603e-03  -2.566   0.0103 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 314.00  on 277  degrees of freedom
## AIC: 625.08
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>anova(fit3,fit3quad,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Kicks ~ Year
## Model 2: Kicks ~ Year + I(Year^2)
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)   
## 1       278     320.94                        
## 2       277     314.00  1   6.9469 0.008397 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Looks like it does: the quadratic model definitely fits better. Our model is <span class="math inline">\(log(Kicks)=-2376+25.2Year-.0067Year^2\)</span>, and maximizing this function shows that the year with the most deaths was <span class="math inline">\(\frac{25.2}{.0135]=1866.67\)</span></p>
<p>We could also examine the differences among corps:</p>
<pre class="r"><code>fit4&lt;-glm(Kicks~Corps,data=prussian,family=&quot;poisson&quot;)
summary(fit4)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Corps, family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5811  -1.0955  -0.8367   0.5438   2.0079  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -2.231e-01  2.500e-01  -0.893   0.3721  
## CorpsC10    -6.454e-02  3.594e-01  -0.180   0.8575  
## CorpsC11     4.463e-01  3.201e-01   1.394   0.1633  
## CorpsC14     4.055e-01  3.227e-01   1.256   0.2090  
## CorpsC15    -6.931e-01  4.330e-01  -1.601   0.1094  
## CorpsC2     -2.877e-01  3.819e-01  -0.753   0.4512  
## CorpsC3     -2.877e-01  3.819e-01  -0.753   0.4512  
## CorpsC4     -6.931e-01  4.330e-01  -1.601   0.1094  
## CorpsC5     -3.747e-01  3.917e-01  -0.957   0.3387  
## CorpsC6      6.062e-02  3.483e-01   0.174   0.8618  
## CorpsC7     -2.877e-01  3.819e-01  -0.753   0.4512  
## CorpsC8     -8.267e-01  4.532e-01  -1.824   0.0681 .
## CorpsC9     -2.076e-01  3.734e-01  -0.556   0.5781  
## CorpsGC     -4.072e-09  3.535e-01   0.000   1.0000  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 297.09  on 266  degrees of freedom
## AIC: 630.17
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>No corps differ significantly from C1, but there do appear to be some differences. For example, a person in corps C11 is <span class="math inline">\(e^{4.46}=1.56\)</span>, or 56% more likely to die from horse kicks than C1, but corps C8 is <span class="math inline">\(e^{-.827}=.44\)</span>, or about 66% <em>less</em> likely to die from horse kicks than C1. We should be mindful of multiple comparisons, but let’s see if C11 differs from C8 by releveling</p>
<pre class="r"><code>prussian$Corps&lt;-relevel(prussian$Corps,ref=&quot;C8&quot;)

fit4&lt;-glm(Kicks~Corps,data=prussian,family=&quot;poisson&quot;)
summary(fit4)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Corps, family = &quot;poisson&quot;, data = prussian)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.5811  -1.0955  -0.8367   0.5438   2.0079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  -1.0498     0.3780  -2.778  0.00548 **
## CorpsC1       0.8267     0.4532   1.824  0.06811 . 
## CorpsC10      0.7621     0.4577   1.665  0.09590 . 
## CorpsC11      1.2730     0.4276   2.977  0.00291 **
## CorpsC14      1.2321     0.4296   2.868  0.00413 **
## CorpsC15      0.1335     0.5175   0.258  0.79639   
## CorpsC2       0.5390     0.4756   1.133  0.25707   
## CorpsC3       0.5390     0.4756   1.133  0.25707   
## CorpsC4       0.1335     0.5175   0.258  0.79640   
## CorpsC5       0.4520     0.4835   0.935  0.34987   
## CorpsC6       0.8873     0.4491   1.976  0.04818 * 
## CorpsC7       0.5390     0.4756   1.133  0.25707   
## CorpsC9       0.6190     0.4688   1.320  0.18668   
## CorpsGC       0.8267     0.4532   1.824  0.06811 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 323.23  on 279  degrees of freedom
## Residual deviance: 297.09  on 266  degrees of freedom
## AIC: 630.17
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Wow, it looks like lots of Corps differ from C8: C8 had surprisingly few horse deaths! Since we have a factor variable, to test if the overall factor is significant, let’s use deviances to compare this model to the null model and test the overall effect of Corps.</p>
<pre class="r"><code>anova(nullfit,fit4,test=&quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Kicks ~ 1
## Model 2: Kicks ~ Corps
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  
## 1       279     323.23                       
## 2       266     297.09 13   26.137   0.0163 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="offsets-and-overdispersion" class="section level3">
<h3>Offsets and Overdispersion</h3>
<p>If you are dealing with counts across groups whose sizes are different, you must explicitly take this into account. For example, assume that corps sizes vary. I am just going to make up some corps sizes for the purposes of illustration. If <span class="math inline">\(\lambda\)</span> is the mean number of kicks per year, we can adjust for corps size by including its log as an offset.</p>
<pre class="r"><code>prussian&lt;-prussian%&gt;%mutate(Size=round(runif(n=280,min = 5000, max=10000)))
prussian%&gt;%dplyr::select(Year,Corps,Kicks,Size)%&gt;%head()</code></pre>
<pre><code>##   Year Corps Kicks Size
## 1 1875    GC     0 9196
## 2 1876    GC     2 5659
## 3 1877    GC     2 8478
## 4 1878    GC     1 5761
## 5 1879    GC     0 5181
## 6 1880    GC     0 9432</code></pre>
<pre class="r"><code>fitoffset&lt;-glm(Kicks~Year,data=prussian,family=&quot;poisson&quot;, offset=log(Size))
summary(fitoffset)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year, family = &quot;poisson&quot;, data = prussian, 
##     offset = log(Size))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4954  -1.1617  -0.9272   0.4772   3.1685  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -45.73589   23.49987  -1.946   0.0516 .
## Year          0.01936    0.01247   1.553   0.1205  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 328.09  on 279  degrees of freedom
## Residual deviance: 325.67  on 278  degrees of freedom
## AIC: 634.75
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Now we have estimates that are correctly adjusted for corps size.</p>
<div id="overdispersion" class="section level4">
<h4>Overdispersion</h4>
<p>Sometimes it will happen that there will be more variation in the response variable than is expected from the Poisson distribution. Recall that we expect <span class="math inline">\(E(Y)=Var(Y)\)</span>. Let <span class="math inline">\(\phi\)</span> be an overdispersion parameter such that <span class="math inline">\(E(Y)=\phi Var(Y)\)</span>. If <span class="math inline">\(\phi=1\)</span>, there is no overdispersion, but if it is greater than 1, there is overdispersion. Overdispersion is problematic in that if it exists, the standard errors will be too small, leading to a greater type I error rate. We can get an estimate of the overdispersion by dividing the model deviance (sum of squared pearson residuals) by its degrees of freedom,</p>
<pre class="r"><code>sum(residuals(fitoffset,&quot;pearson&quot;)^2)/df.residual(fitoffset)</code></pre>
<pre><code>## [1] 1.153363</code></pre>
<pre class="r"><code>sqrt(1.1628)</code></pre>
<pre><code>## [1] 1.078332</code></pre>
<p>We would need to multiply our standard errors by <span class="math inline">\(\sqrt{\hat \phi}\)</span> to correct for overdispersion, so <span class="math inline">\(.01247\times \sqrt{1.1628}=.01247*1.0783=.01345\)</span>. We can get this directly by setting <code>family=&quot;quasipoisson&quot;</code>, as below. Note that tests are now based on the <em>t</em> distribution.</p>
<pre class="r"><code>fitquasi&lt;-glm(Kicks~Year,data=prussian,family=&quot;quasipoisson&quot;, offset=log(Size))
summary(fitquasi)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Kicks ~ Year, family = &quot;quasipoisson&quot;, data = prussian, 
##     offset = log(Size))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4954  -1.1617  -0.9272   0.4772   3.1685  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) -45.73589   25.23824  -1.812    0.071 .
## Year          0.01936    0.01339   1.446    0.149  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 1.153419)
## 
##     Null deviance: 328.09  on 279  degrees of freedom
## Residual deviance: 325.67  on 278  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Perhaps a simpler technique is to use robust standard errors.</p>
<p>Another common way of handling overdispersion is to use a negative binomial model, NegBinom(. You might know this distribution as modeling the number of (Bernoulli) trials before a certain number of failures occurs, but mathematically it is a Poisson model where <span class="math inline">\(\lamda\)</span> is itself random, following a gamma distribution, <span class="math inline">\(\lambda\sim Gamma(r, \frac{1-p}{p})\)</span>. So instead of <span class="math inline">\(Y\sim Poisson(\lambda)\)</span>, we model our responses as <span class="math inline">\(Y\sim NegBinom(r,p)\)</span> where <span class="math inline">\(E(Y)=pr/(1-p)=\mu\)</span> and <span class="math inline">\(Var(Y)=\mu+\frac{\mu^2}{r}\)</span>, where <span class="math inline">\(\frac{\mu^2}{r}\)</span> is our overdispersion parameter.</p>
<pre class="r"><code>fitnb&lt;-glm.nb(Kicks~Year,data=prussian, weights=offset(log(Size)))
summary(fitnb)</code></pre>
<pre><code>## 
## Call:
## glm.nb(formula = Kicks ~ Year, data = prussian, weights = offset(log(Size)), 
##     init.theta = 8.355198954, link = log)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -3.808  -3.429  -3.139   1.124   7.906  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -35.460285   8.188981  -4.330 1.49e-05 ***
## Year          0.018625   0.004344   4.287 1.81e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Negative Binomial(8.3552) family taken to be 1)
## 
##     Null deviance: 2665.8  on 279  degrees of freedom
## Residual deviance: 2647.6  on 278  degrees of freedom
## AIC: 5555.5
## 
## Number of Fisher Scoring iterations: 1
## 
## 
##               Theta:  8.36 
##           Std. Err.:  3.29 
## 
##  2 x log-likelihood:  -5549.525</code></pre>
<p>This generates incredibly small standard errors and makes me skeptical… Because the results are now so much better than our unadjusted model, I really don’t trust it.</p>
</div>
<div id="zero-inflated-poisson" class="section level4">
<h4>Zero-Inflated Poisson</h4>
<p>Sometimes you get count data with loads of zeros. For example, a former student of mine was looking at how social life (hours per week spent socializing) and stress (credit hours + work hours per week) predicted number of alcoholic beverages consumed each week. But many students do not consume any alcohol at all!</p>
<pre class="r"><code>drinkdat&lt;-read.csv(&quot;~/Downloads/drink_data_total.csv&quot;)
drinkdat&lt;-drinkdat%&gt;%filter(Drinks&lt;20)
drinkdat%&gt;%ggplot(aes(Drinks))+geom_bar()</code></pre>
<p><img src="/post/2019-07-24-logistic-and-poisson-regression_files/figure-html/unnamed-chunk-61-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>What are we to do? Well, we get creative! Think of trying to emulate the data-generating process: there are two overlapping populations here, drinkers and non-drinkers. Among drinkers, we may see a poisson distribution for drinks consumed in a week, but not among non-drinkers. However, non-drinkers are contributing to the zeros in our data.</p>
<pre class="r"><code>poisfit&lt;-glm(Drinks~Social*Stress,data=drinkdat,family=&quot;poisson&quot;)
summary(poisfit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Drinks ~ Social * Stress, family = &quot;poisson&quot;, data = drinkdat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.4563  -1.8425  -1.5876   0.5853   5.3410  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -1.740315   0.548228  -3.174 0.001501 ** 
## Social         0.447490   0.081164   5.513 3.52e-08 ***
## Stress         0.062919   0.020656   3.046 0.002319 ** 
## Social:Stress -0.013059   0.003422  -3.816 0.000136 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 485.99  on 102  degrees of freedom
## Residual deviance: 440.71  on  99  degrees of freedom
## AIC: 586.33
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code>pchisq(440.71,99,lower.tail = F)</code></pre>
<pre><code>## [1] 1.293779e-44</code></pre>
<pre class="r"><code>exp&lt;-dpois(0:14,mean(drinkdat$Drinks))*length(drinkdat$Drinks)
obs&lt;-table(factor(drinkdat$Drinks,levels=0:14))
chisq&lt;-sum((obs-exp)^2/exp)
pchisq(chisq,df = 13,lower.tail = F)</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>library(pscl)
zip_fit&lt;-zeroinfl(Drinks~Social*Stress|1,data=drinkdat)
summary(zip_fit)</code></pre>
<pre><code>## 
## Call:
## zeroinfl(formula = Drinks ~ Social * Stress | 1, data = drinkdat)
## 
## Pearson residuals:
##     Min      1Q  Median      3Q     Max 
## -0.7407 -0.7081 -0.6985  0.3800  4.1894 
## 
## Count model coefficients (poisson with log link):
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    0.426825   0.479516   0.890  0.37340   
## Social         0.209253   0.072870   2.872  0.00408 **
## Stress         0.044174   0.015595   2.833  0.00462 **
## Social:Stress -0.007978   0.002728  -2.925  0.00345 **
## 
## Zero-inflation model coefficients (binomial with logit link):
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   0.4004     0.2024   1.978    0.048 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 
## 
## Number of iterations in BFGS optimization: 18 
## Log-likelihood: -174.8 on 5 Df</code></pre>
<p>This model fits much better than the non-zero-inflated poisson,</p>
<pre class="r"><code>vuong(zip_fit,poisfit)</code></pre>
<pre><code>## Vuong Non-Nested Hypothesis Test-Statistic: 
## (test-statistic is asymptotically distributed N(0,1) under the
##  null that the models are indistinguishible)
## -------------------------------------------------------------
##               Vuong z-statistic             H_A    p-value
## Raw                    5.891903 model1 &gt; model2 1.9089e-09
## AIC-corrected          5.840380 model1 &gt; model2 2.6041e-09
## BIC-corrected          5.772504 model1 &gt; model2 3.9051e-09</code></pre>
<p>Here the test statistic is significant across the board indicating that the zero-inflated Poisson model fits better than the standard Poisson model.</p>
</div>
<div id="zero-truncated-poisson" class="section level4">
<h4>Zero-Truncated Poisson</h4>
<p>Sometimes, instead of an abundance of true zeros, you have strictly non-zero data (e.g., count data where the minimum value is 1). An ordinary Poisson regression will try to predict zero counts, which would be inappropriate for this data.</p>
<p>What if you were interested in the affect of socializing and stress on number of drinks consumed <em>among students who drink</em>.</p>
<pre class="r"><code>drinkdat_nozero&lt;-drinkdat%&gt;%filter(Drinks&gt;0)

#install.packages(&quot;VGAM&quot;)
library(VGAM)

trunc_fit&lt;-vglm(Drinks~Social*Stress,family=pospoisson(),data=drinkdat_nozero)
summary(trunc_fit)</code></pre>
<pre><code>## 
## Call:
## vglm(formula = Drinks ~ Social * Stress, family = pospoisson(), 
##     data = drinkdat_nozero)
## 
## Pearson residuals:
##                   Min     1Q  Median    3Q   Max
## loglink(lambda) -1.85 -1.172 -0.2817 1.054 4.026
## 
## Coefficients: 
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    0.435122   0.477608   0.911  0.36227   
## Social         0.209062   0.073625   2.840  0.00452 **
## Stress         0.044434   0.015720   2.827  0.00470 **
## Social:Stress -0.008062   0.002818  -2.861  0.00423 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Name of linear predictor: loglink(lambda) 
## 
## Log-likelihood: -105.574 on 37 degrees of freedom
## 
## Number of Fisher scoring iterations: 4 
## 
## No Hauck-Donner effect found in any of the estimates</code></pre>
<pre class="r"><code>#AIC
-2*-105.574+2*3</code></pre>
<pre><code>## [1] 217.148</code></pre>
<pre class="r"><code>summary(glm(Drinks~Social*Stress,family=poisson,data=drinkdat_nozero))</code></pre>
<pre><code>## 
## Call:
## glm(formula = Drinks ~ Social * Stress, family = poisson, data = drinkdat_nozero)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2303  -1.2847  -0.2845   0.9985   3.2402  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)    0.471948   0.469777   1.005  0.31508   
## Social         0.203073   0.072302   2.809  0.00497 **
## Stress         0.043329   0.015533   2.789  0.00528 **
## Social:Stress -0.007817   0.002751  -2.842  0.00449 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 82.526  on 40  degrees of freedom
## Residual deviance: 74.156  on 37  degrees of freedom
## AIC: 219.78
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Slightly lower AIC, slightly better fit compared to using a poisson regression. Results are extremely comparable.</p>
</div>
</div>
</div>

  </div>
  


<div class="navigation navigation-single">
    
    <a href="/posts/repeated-measures-anova-in-r/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Repeated Measures ANOVA in R</span>
    </a>
    
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
