<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.60.1" />

    
    
    

<title>Enemy Tank Problem • Nathaniel Woodward</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Enemy Tank Problem"/>
<meta name="twitter:description" content="Enemy Tank Problem: Estimating Population Size  In wartime, it is advantageous to know how well-equipped your adversary is. Ideally, you would like to have full knowledge of their armory: exactly how many tanks, planes, and ships they have that are currently battle-ready.
Unfortunately, there is no good way to get this information. Imagine, however, that you have managed to capture \(k\) tanks from the enemy. Furthermore, each enemy tank has a serial number stamped on it."/>

<meta property="og:title" content="Enemy Tank Problem" />
<meta property="og:description" content="Enemy Tank Problem: Estimating Population Size  In wartime, it is advantageous to know how well-equipped your adversary is. Ideally, you would like to have full knowledge of their armory: exactly how many tanks, planes, and ships they have that are currently battle-ready.
Unfortunately, there is no good way to get this information. Imagine, however, that you have managed to capture \(k\) tanks from the enemy. Furthermore, each enemy tank has a serial number stamped on it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/enemy-tank-problem/" />
<meta property="article:published_time" content="2019-01-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-01-07T00:00:00+00:00" /><meta property="og:site_name" content=" " />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.43109490338b28cdd7a74845262efe59bf66fdc3530f5e818a66c9a579a79080.css" integrity="sha256-QxCUkDOLKM3Xp0hFJi7&#43;Wb9m/cNTD16BimbJpXmnkIA=">


<link rel="stylesheet" href="/scss/print.8c61428a4ae8b36c028e9198876312a2de5a2b3c80bbbdcceb98d738691c4f6b.css" integrity="sha256-jGFCikros2wCjpGYh2MSot5aKzyAu73M65jXOGkcT2s=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    <script type="text/javascript"
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script type="text/javascript" src="/js/bigfoot.min.js"></script>
<script type="text/javascript">
    $.bigfoot();
</script>

</head>


    <body class="theme-base-09 ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
    
      
        
        
        
        
        <div class="author-image">
           <a href="/"><img src="/img/headshot_cropped_f2019.png" alt="Nathaniel Woodward" class="img--circle img--headshot element--center"></a>
        </div>
        
      
      <p class="site__description">
      <a href="mailto:nathanielraley@gmail.com"> Nathaniel Woodward 
      </a></p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Nathaniel Woodward</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Portfolio</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/teaching/">
						<span>Teaching</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/resume.html">
						<span>Vitae</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <br>
        <section class="social">
	
	<a href="https://twitter.com/Distributino" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/nathanielwoodward" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/nathanielraley" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://last.fm/user/ramimba" rel="me"><i class="fab fa-lastfm fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2022  
  
    <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  
</div>



  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Enemy Tank Problem</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Jan 7, 2019
    
    
    
    
    
    <br/>
    <i class="fas fa-clock"></i> 11 min read
</div>


  </header>
  
  
  <div class="post">
    


<div id="enemy-tank-problem-estimating-population-size" class="section level2">
<h2>Enemy Tank Problem: Estimating Population Size</h2>
<div style="text-align: center">
<p><img src="https://www.army-technology.com/wp-content/uploads/sites/3/2017/09/M1A1-Abrams-US-Army.jpg" width="650" height="485"></p>
</div>
<p>In wartime, it is advantageous to know how well-equipped your adversary is. Ideally, you would like to have full knowledge of their armory: exactly how many tanks, planes, and ships they have that are currently battle-ready.</p>
<p>Unfortunately, there is no good way to get this information. Imagine, however, that you have managed to capture <span class="math inline">\(k\)</span> tanks from the enemy. Furthermore, each enemy tank has a serial number stamped on it. Represent the sample of serial numbers as <span class="math inline">\(\{x_1, x_2,\dots,x_k\}\)</span>. For now, we will assume that the first tank produced had the serial number <span class="math inline">\(1\)</span>, while the last tank produced has the serial number <span class="math inline">\(N\)</span> (they don’t have to start at one, but let’s assume they do for now). It is this <span class="math inline">\(N\)</span> that we would like to know, since that would tell us how many tanks have been produced so far. In our set of <span class="math inline">\(k\)</span> captured enemy tanks, the largest serial number is <span class="math inline">\(m\)</span>.</p>
<p>So the goal is to estimate <span class="math inline">\(N\)</span> given that we know <span class="math inline">\(k\)</span> and <span class="math inline">\(m\)</span>. How could we use these two pieces of information (really, sample statistics) to come up with an estimator for the population parameter (i.e., the true maximum)? It seems unlikely that we would end up with a scenario where the largest serial number in our sample happens to be the largest serial number produced (where <span class="math inline">\(m=N\)</span>), so we should guess something bigger than <span class="math inline">\(m\)</span>. Also, it seems like as the number of captured tanks <span class="math inline">\(k\)</span> grows, <span class="math inline">\(m\)</span> should get closer to <span class="math inline">\(N\)</span>: in the limit, the sample size would equal the population size, and so their maxima would be the same. Let’s make up an estimator for <span class="math inline">\(N\)</span> and see how it fares (we will derive the best unbiased estimator below). Let’s use this:</p>
<p><span class="math display">\[
\widehat N = \frac{2m}{k}
\]</span></p>
<p>Our estimate is always larger than <span class="math inline">\(m\)</span>, and as <span class="math inline">\(k\)</span> grows, our estimator <span class="math inline">\(\widehat N\)</span> shrinks. Let’s conduct a simulation to see how well this works. Let’s say the true number of tanks produced, <span class="math inline">\(N\)</span>, is <span class="math inline">\(1000\)</span> (we don’t know this). From this population, let’s say we captured 10 tanks:</p>
<pre class="r"><code>tanks=1:1000
n=max(tanks)

samp=sample(tanks,size=10, replace = F)
m=max(samp)
k=length(samp)

nhat=2*m/k</code></pre>
<p>The maximum value of our sample is 1000, and so our estimator <span class="math inline">\(\widehat N\)</span> is 198.2. Let’s do this 5000 times, taking a new sample of tanks each time.</p>
<pre class="r"><code>nhats&lt;-replicate(5000,{samp=sample(tanks,size=10, replace = F)
           m=max(samp); k=length(samp); nhat=2*m/k})

mean(nhats)</code></pre>
<pre><code>## [1] 182.2741</code></pre>
<p>This is not very good! Under replication, our made-up estimator is <em>way</em> underestimating the true <span class="math inline">\(N=1000\)</span>.</p>
<p>Let’s try to derive a better <span class="math inline">\(\widehat N\)</span>. We could start with the assumption that captured tank serial numbers follow a uniform distribution. This means that the serial numbers are equally spaced and that each tank serial number has an equal probability of being chosen. To take a simple example, if there were 1000 tanks in all, each tank would have a <span class="math inline">\(1/1000\)</span> chance of being captured (assuming tanks appear randomly, which is not necessarily very likely). But we don’t know how many tanks there are in all: this is what we are trying to estimate!</p>
<p>One way to think about this is, what is the probability of getting a sample of serial numbers with a maximum value less than or equal to <span class="math inline">\(m\)</span> (i.e., <span class="math inline">\(max(x)\le m\)</span>) when the population maximum (i.e., the true total of tanks produced) is <span class="math inline">\(N\)</span>?</p>
<p>In other words, how many ways are there to construct a sample of size <span class="math inline">\(k\)</span> with a maximum no greater than the one we observed, <span class="math inline">\(m\)</span>? Well, there are <span class="math inline">\(m\)</span> possibilities for the first choice, <span class="math inline">\(m-1\)</span> possibilities for the second choice (since we already used up one value: we are sampling <em>without</em> replacement), and so on, giving</p>
<p><span class="math display">\[
\frac{m!}{(m-k)!}=m(m-1)(m-2)\dots(m-k+1)
\]</span></p>
<p>Similarly, how many ways are there to construct a sample of size <span class="math inline">\(k\)</span> with a maximum no greater than <span class="math inline">\(N\)</span>, the number we are trying to estimate?</p>
<p>Well, there are <span class="math inline">\(N\)</span> possibilities for the first captured tank <span class="math inline">\(x_1\)</span>, <span class="math inline">\(n-1\)</span> possibilities for the second captured tank, <span class="math inline">\(n-2\)</span> possibilities for the third tank, and so on through our last captured tank <span class="math inline">\(k\)</span>. Thus, we have</p>
<p><span class="math display">\[
\frac{N!}{(N-k)!}=N(N-1)(N-2)...(N-k+1)
\]</span></p>
<p>The probability of obtaining sample <span class="math inline">\(\{x_1,\dots,x_k\}\)</span> with <span class="math inline">\(max(x)\le m\)</span> is thus</p>
<p><span class="math display">\[
P(max(x)\le m)=\frac{m!/(m-k)!}{N!/(N-k)!}=\frac{m!(N-k)!}{n!(m-k)!}
\]</span></p>
<p>Now, the smallest possible estimate for the total number of tanks is equal to the size of our sample <span class="math inline">\(k\)</span> (since if we captured all of the tanks, the size of our sample would be equal to the largest serial number). The largest possible estimate for the total number is equal to <span class="math inline">\(N\)</span>, the true total number of tanks produced. Thus, this function is bounded by <span class="math inline">\(k\)</span> and <span class="math inline">\(N\)</span>.</p>
<p>This is the distribution function of the MLE of <span class="math inline">\(N\)</span>, i.e., <span class="math inline">\(max(x)\)</span>. This tells us, for a given <span class="math inline">\(N\)</span> and <span class="math inline">\(k\)</span>, the probability of observing a <span class="math inline">\(m=max(x)\)</span> less than or equal to the one we observed. However, we never observe <span class="math inline">\(N\)</span>, so we treat it as a function of <span class="math inline">\(N\)</span> with given <span class="math inline">\(m\)</span> and <span class="math inline">\(k\)</span>.</p>
<p>The distribution function is plotted below as a function of <span class="math inline">\(N\)</span> given fixed <span class="math inline">\(m\)</span> and <span class="math inline">\(k\)</span>: We can see that 95% of the time, the population total (<span class="math inline">\(N\)</span>) will be less than 182 given a sample max <span class="math inline">\(m\)</span> of 100 and a sample size <span class="math inline">\(k\)</span> of 5 tanks.</p>
<pre class="r"><code>tank.fun1&lt;-function(x,m,k){ifelse(x==m,1,prod(m:(m-k+1))/prod(x:(x-k+1)))}

tanky&lt;-vector()
for(i in 100:350)tanky[i-99]&lt;-tank.fun1(x=i,m=100,k=5)

ggplot(data.frame(y=tanky,x=100:350),aes(x,y))+geom_point()+theme_minimal()+ylab(&quot;Prob( max &lt;= m)&quot;)+xlab(&quot;N&quot;)+geom_hline(aes(yintercept=.05),lty=3)</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Let’s say we discover 5 more tanks. Holding everything else constant, how does this change our distribution function?</p>
<pre class="r"><code>tanky1&lt;-vector()
for(i in 100:350)tanky1[i-99]&lt;-tank.fun1(x=i,m=100,k=10)

ggplot(data.frame(y=tanky1,y1=tanky,x=100:350),aes(x,y))+geom_point()+geom_point(aes(x,y1),color=&quot;gray&quot;)+theme_minimal()+ylab(&quot;Prob( max &lt;= m)&quot;)+xlab(&quot;N&quot;)+geom_hline(aes(yintercept=.05),lty=3)</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Now 95% of the time we expect the total population to be less than or equal to 135 (where black curve crosses dotted line; gray curve is from the previous plot where <span class="math inline">\(k=5\)</span>). This makes sense: increasing the sample size should result in a more precise estimate of N, thus bringing <span class="math inline">\(m\)</span> closer.</p>
<p>But what is our best guess about <span class="math inline">\(N\)</span>? We could just take the median (fiftieth percentile) from the distribution functin above: for the sample size of 5, we get an estimate of approximately 116. But is that the best estimate we can produce?</p>
<p>Assuming we draw a sample of <span class="math inline">\(k\)</span> tanks from a total of <span class="math inline">\(N\)</span> tanks, we can compute the probability that the largest serial number in the sample will be equal to <span class="math inline">\(m\)</span>.</p>
<p>First, we need the total number of possible samples of size <span class="math inline">\(k\)</span> from the total <span class="math inline">\(N\)</span>, which is just <span class="math inline">\({n \choose k}=\frac{n!}{(N-k)!k!}\)</span>.</p>
<p>Now, each of these samples has a sample maximum: we want to know how many of these samples have a maximum of <span class="math inline">\(m\)</span>. If <span class="math inline">\(m\)</span> has to be the sample maximum, then one of our <span class="math inline">\(k\)</span> tanks is fixed at <span class="math inline">\(m\)</span> and all of the other <span class="math inline">\(k-1\)</span> tanks must have smaller serial numbers (<span class="math inline">\(m-1\)</span> or less). Thus, the number of samples of size <span class="math inline">\(k\)</span> where the maximum is <span class="math inline">\(m\)</span> is <span class="math inline">\({m -1 \choose k-1}=\frac{m-1!}{(m-k-2)!(k-1)!}\)</span>.</p>
<p>So we can say, out of all possible samples of size <span class="math inline">\(k\)</span>, the proportion of those with a maximum of <span class="math inline">\(m\)</span> is <span class="math inline">\(p(m)={m -1 \choose k-1}/{N \choose k}\)</span>. Not surprsingly, this is the derivative of the distribution function above with respect to m (but I won’t show this here)!</p>
<pre class="r"><code>tank.fun2&lt;-function(x,m,k){choose(m-1,k-1)/choose(x,k)}

tanky2&lt;-vector()
for(i in 100:350)tanky2[i-99]&lt;-tank.fun2(i,100,5)
qplot(y=tanky2,x=100:350)+geom_line()+theme_minimal()+ylab(&quot;Prob( max = m)&quot;)+xlab(&quot;N&quot;)</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>#tanky2</code></pre>
<p>Notice that the value of <span class="math inline">\(N\)</span> that maximizes the probability is just our sample maximum <span class="math inline">\(m\)</span>. This seems odd: 100 seems almost certain to underestimate the true <span class="math inline">\(N\)</span>. We will see below that it is biased. Recall that the bias of an estimator is how much it deviates from the true value on average: We will show that <span class="math inline">\(E[m]\ne N\)</span></p>
<p>Since we have the probability distribution of the sample max <span class="math inline">\(m\)</span>, let’s find the expected value of <span class="math inline">\(m\)</span>. Recall that the expected value of a variable is just the sum of the values the variable takes on, weighted by their probability. That is, <span class="math inline">\(E(x)=\sum_i x_i\cdot p(x_i)\)</span>. The smallest possible value of <span class="math inline">\(m\)</span> is <span class="math inline">\(k\)</span> (the number of tanks in the sample), and the largest possible value is <span class="math inline">\(N\)</span> (the number we are trying to estimate), so the expected value is</p>
<p><span class="math display">\[
\begin{aligned}
E[m]&amp;=\sum_{m=k}^N m\cdot p(m)\\
&amp;=\sum_{m=k}^N m\cdot{m -1 \choose k-1}/{N \choose k}\\
&amp;=\frac{1}{N\choose k}\sum_{m=k}^N m\cdot \frac{(m-1)!}{(m-k)!(k-1)!}\\
&amp;=\frac{1}{{N\choose k}(k-1)! }\sum_{m=k}^N \frac{m!}{(m-k)!}\\
&amp;=\frac{k!}{(k-1)!}\frac{1}{{N\choose k}}\sum_{m=k}^N {m \choose k}\\
&amp;=k\frac{{N+1 \choose k+1}}{{N\choose k}}\phantom{xxxxxxxxxxxxxxxxx} ^* since \sum_{m=k}^N {N\choose k} = {N+1 \choose k+1} \\
&amp;=k\frac{N+1}{k+1}
\end{aligned}
\]</span>
<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p>Now, so we have <span class="math inline">\(E[m]=\frac{k(N+1)}{k+1}\)</span>. This estimator is biased, since <span class="math inline">\(E[m]-N=\frac{k-N}{k+1}\)</span>. Solving for N, we get</p>
<p><span class="math display">\[
\begin{aligned}
E[m]&amp;=\frac{k(N+1)}{k+1}\\
N+1&amp;=\frac{E[m](k+1)}{k}\\
N&amp;=E[m](1+\frac{1}{k})-1\\
\end{aligned}
\]</span></p>
<p>If we use <span class="math inline">\(\widehat{N}=m(1+\frac{1}{k})-1\)</span> instead, we have an unbiased estimator:
<span class="math display">\[
\begin{aligned}
E[\widehat N]&amp;=E[m(1+\frac{1}{k})-1]\\
&amp;=(1+\frac{1}{k})E[m]-1\\
&amp;=(1+\frac{1}{k})\frac{k(N+1)}{k+1}-1\\
&amp;=(\frac{k+1}{k})\frac{k(N+1)}{k+1}-1\\
&amp;=N
\end{aligned}
\]</span>
<span class="math inline">\(\widehat{N}\)</span> is actually the minimum-variance unbiased estimator, but I won’t show this now!</p>
<p>Using our unbiased estimator, we get</p>
<p><span class="math display">\[
\begin{aligned}
\widehat N&amp;=m(1+\frac{1}{k})-1\\
&amp;= 100(1+\frac1 5)-1\\
&amp;=100+20-1\\
&amp;=119
\end{aligned}
\]</span>
Notice what is going on here with the <span class="math inline">\(100+20-1\)</span>. We are taking the sample maximum 100 and adding to the average gap between the numbers in the sample. The average gap between numbers in the sample is <span class="math inline">\(\frac{m-k}{k}=\frac{100-5}{5}=19\)</span> (we subtract <span class="math inline">\(k\)</span>) from the top because the numbers themselves should not be included in the gap.</p>
</div>
<div id="comparison-simulation" class="section level2">
<h2>Comparison Simulation</h2>
<p>Let’s do another simulation to see all of this in action.</p>
<p>Say the enemy has actually produced <span class="math inline">\(N=250\)</span> tanks (we obviously don’t know this) and we capture <span class="math inline">\(k=5\)</span> of them.</p>
<pre class="r"><code>tanks=1:250
n=max(tanks)

samp=sample(tanks,size=5, replace = F)
m=max(samp)
k=length(samp)

nhat=m+(m/k)-1</code></pre>
<p>The maximum value of our sample is 250, and so our estimator <span class="math inline">\(\widehat N\)</span> is 285.8. Let’s do this 5000 times, taking a new sample of tanks each time.</p>
<pre class="r"><code>nhats&lt;-replicate(5000,{samp=sample(tanks,size=5, replace = F)
           m=max(samp); k=length(samp); nhat=m+(m/k)-1})

mean(nhats)</code></pre>
<pre><code>## [1] 250.0882</code></pre>
<pre class="r"><code>dists&lt;-replicate(5000,{mean(abs(diff(sort(sample(1:100,5, replace = T)))))})</code></pre>
<p>The average of these 5000 estimates is pretty close to 250. But look at the distribution:</p>
<pre class="r"><code>qplot(nhats)+theme_minimal()</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>runavg&lt;-vector()
for(i in 1:5000) runavg[i]&lt;-mean(nhats[1:i])
qplot(1:5000,runavg)+geom_line()+geom_hline(yintercept=250,color=&quot;red&quot;)</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>How does this compare to our median estimator?</p>
<pre class="r"><code>tank.fun2&lt;-function(x,m,k){choose(m-1,k-1)/choose(x,k)}

nhats2&lt;-vector()
for(j in 1:5000){
samp=sample(tanks,size=5, replace = F); tanky&lt;-vector(); m=max(samp); for(i in m:350)tanky[i-m-1]&lt;-tank.fun1(i,m,5); nhats2[j]&lt;-m+sum(tanky&gt;.5)+1}

mean(nhats2)</code></pre>
<pre><code>## [1] 239.3914</code></pre>
<p>Here’s the distribution. Notice that the running average converges somewhere between 239 and 240 rather than 250, indicating that this estimator is biased (though not too terribly).</p>
<pre class="r"><code>qplot(nhats)+theme_minimal()</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>runavg&lt;-vector()
for(i in 1:5000) runavg[i]&lt;-mean(nhats2[1:i])
qplot(1:5000,runavg)+geom_line()+geom_hline(yintercept=runavg[5000],color=&quot;red&quot;)</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
</div>
<div id="not-just-tanks" class="section level2">
<h2>Not just tanks!</h2>
<p>For all of our sakes, let’s hope we won’t have to actually estimate enemy tank production in real life. But there are many other applications of this! Imagine you are a manufacturer and you want to estimate the number of products produced by your competitors: all you need is a serial number (and the more, the better)! Back in 2008, a London investor solicited iPhone serial numbers from people online and was able to estimate that Apple had sold 9,190,680 iPhones to the end of September. This comes to just over 1,000,000 per month, making it very likely that Apple would sell more than 10,000,000 by year-end (indeed, they officially reported surpassing the mark in October, just as predicted by the estimate).</p>
</div>
<div id="takehome" class="section level2">
<h2>Takehome</h2>
<p>Here’s a handy table for getting a rough point estimate and confidence interval (leaving out the minus one)</p>
<table>
<thead>
<tr class="header">
<th>k</th>
<th>point estimate</th>
<th>confidence interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2m</td>
<td>[m, 20m]</td>
</tr>
<tr class="even">
<td>2</td>
<td>1.5m</td>
<td>[m, 4.5m]</td>
</tr>
<tr class="odd">
<td>5</td>
<td>1.2m</td>
<td>[m, 1.82m]</td>
</tr>
<tr class="even">
<td>10</td>
<td>1.1m</td>
<td>[m, 1.35m]</td>
</tr>
<tr class="odd">
<td>20</td>
<td>1.05m</td>
<td>[m, 1.16m]</td>
</tr>
</tbody>
</table>
<p>Where does this come from?</p>
<p>Imagine we have a continuous interval from 0 to 1 inclusive. The probability that a sample of size <span class="math inline">\(k\)</span> will have a maximum less than or equal to <span class="math inline">\(m\)</span>, where <span class="math inline">\(m\)</span> is some number <span class="math inline">\(0 \le m \le 1\)</span>, is <span class="math inline">\(m^k\)</span>. For example, if <span class="math inline">\(m=0.5\)</span>, the probability that all the <span class="math inline">\(k\)</span> numbers in your sample are less than or equal to <span class="math inline">\(0.5\)</span> is just
<span class="math display">\[\underbrace{0.5*0.5...*0.5}_{k\ times}=0.5^k=m^k=p(M\le m)=F_M(m)\]</span></p>
<pre class="r"><code>ggplot(data.frame(m=c(0, 1)), aes(m)) + 
  stat_function(fun=function(x) x^3)+ylab(&#39;p(M &lt;= m)&#39;)</code></pre>
<p><img src="/posts/2019-01-07-enemy-tank-problem_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Here, <span class="math inline">\(F_M(m)=p(M\le m)=m^k\)</span> is the CDF of the sample max <span class="math inline">\(M\)</span>, the function that returns the probability that the sample max is less than or equal to some value <span class="math inline">\(m\)</span>. The quantile function is given by the <em>inverse</em> of this CDF, i.e., <span class="math inline">\(F^{-1}_M(m)\)</span>. This tells you what sample max <span class="math inline">\(m\)</span> would make <span class="math inline">\(F_M(m)\)</span> return a certain probability <span class="math inline">\(p\)</span>. (This is just like when you look up <span class="math inline">\(p=.5\)</span> z-table and it gives you <span class="math inline">\(z=0\)</span>: <span class="math inline">\(InvNorm(p=.5)=0\)</span>.)</p>
<p>Now, in our case <span class="math inline">\(F_M(m)=m^k=p, so\ \ F^{-1}_M(m)=p^{1/k}\)</span>. So for a population maximum of <span class="math inline">\(N\)</span>, we are <span class="math inline">\(95%\)</span> confident that the sample maximum is covered by the interval</p>
<p><span class="math display">\[
.025^{1/k}N\le m \le .975^{1/k}N
\]</span>
(We multiply by <span class="math inline">\(N\)</span> here to take the interval from <span class="math inline">\([0,1]\)</span> to <span class="math inline">\([O,N]\)</span>.) Solving for N in the middle gives</p>
<p><span class="math display">\[
\begin{aligned}
.025^{1/k}\frac 1 m &amp;\le \frac 1 N \le .975^{1/k}\frac 1 m\\
\frac{m}{.025^{1/k}} &amp;\ge N \ge \frac{m}{.975^{1/k}}\\ 
\frac{m}{.975^{1/k}} &amp;\le N \le  \frac{m}{.025^{1/k}}\\ 
\end{aligned}
\]</span></p>
<p>Giving the population max <span class="math inline">\(N\)</span> a <span class="math inline">\(95%\)</span> CI of <span class="math inline">\([\frac{m}{.975^{1/k}}, \frac{m}{.025^{1/k}}]\)</span>. Thus, if you just have a single observation <span class="math inline">\(k\)</span>, your <span class="math inline">\(95%\)</span> CI is <span class="math inline">\([\frac{m}{.975}, \frac{m}{.025}]\)</span> or <span class="math inline">\([1.026 m, 40m]\)</span>. Since the sampling distribution is asymmetrical, we are better off reporting the asymmetric <span class="math inline">\(95%\)</span> CI by doing <span class="math inline">\([\frac{m}{1}, \frac{m}{.05^{1/k}}]\)</span>. This yields <span class="math inline">\([m, 20m]\)</span> for a single sample (these values are reported above).</p>
<div id="references" class="section level3">
<h3>References:</h3>
<p>en.wikipedia.org/wiki/German_tank_problem
mathsection.com/german-tank-problem</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is the <a href="https://en.wikipedia.org/wiki/Hockey-stick_identity">hockey-stick identity</a> from pascal’s triangle, but we can derive it easily here from <span class="math inline">\(p(m)\)</span>. Since <span class="math inline">\(p(m)={m -1 \choose k-1}/{N \choose k}\)</span> must sum to 1 (it is a probability mass function),<span class="math inline">\(1=\sum_{m=k}^N{m -1 \choose k-1}/{N \choose k} \implies \sum_{m=k}^N{m -1 \choose k-1}= {N \choose k}\)</span>}<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>

  </div>
  


<div class="navigation navigation-single">
    
    <a href="/posts/basic-stats-u-need-4-correlation-and-regression/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Basic Stats U Need #4: Correlation &amp; Regression</span>
    </a>
    
    
    <a href="/posts/sample-means-and-shrinkage-estimators/" class="navigation-next">
      <span class="navigation-title">Sample Means and Shrinkage Estimators</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
